{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "import random as rd\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "with open(\"padded_features.p\", \"rb\") as pickled_file:\n",
    "    padded_features = pk.load(pickled_file)\n",
    "\n",
    "UK_OST = [mp3 for mp3 in os.listdir(os.path.join(current_dir, \"Audio_Files\\\\UK_OST\"))]\n",
    "SMG_OST = [mp3 for mp3 in os.listdir(os.path.join(current_dir, \"Audio_Files\\\\SMG_OST\"))]\n",
    "all_OST = UK_OST + SMG_OST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('04Sunshine(Mirage).mp3', 1.0), ('01Intro.mp3', 1.0), ('(Alt)04Cerberus.mp3', 1.0), ('04DancerintheDarkness.mp3', 1.0), ('07TakeCare.mp3', 1.0), ('ClairdeLune.mp3', 1.0), ('16Guts.mp3', 1.0), ('1-22 Final Battle with Bowser.mp3', 0.0), ('04LakesideSongbook.mp3', 1.0), ('14PanicBetrayer.mp3', 1.0), ('06SufferingLeavesSufferingLeaves.mp3', 1.0), ('1-13 Rosalina in the Observatory 2.mp3', 0.0), ('10TheAbyssandtheSerpent.mp3', 1.0), ('2-29 Buoy Base Galaxy - Undersea.mp3', 0.0), (\"2-23 Bowser's Stronghold Appears.mp3\", 0.0), ('2-34 Pipe Interior.mp3', 0.0), ('03DeepBlue(EarlyVersionCombatLoop).mp3', 1.0), ('2-44 Heavy Metal Mecha-Bowser.mp3', 0.0), ('2-09 A Chance to Grab a Star!.mp3', 0.0), ('01TheSpinalStaircase.mp3', 1.0), ('1-18 Rosalina in the Observatory 3.mp3', 0.0), ('(Alt)05AThousandGreetings.mp3', 1.0), ('02DeepBlue(EarlyVersionCalmLoop).mp3', 1.0), ('1-28 Super Mario 2007.mp3', 0.0), ('1-26 Purple Comet.mp3', 0.0), ('2-15 Aquatic Race.mp3', 0.0), ('03BullofHell.mp3', 1.0), ('2-14 Airship Armada.mp3', 0.0), ('1-17 Gusty Garden Galaxy.mp3', 0.0), ('Allegretto(AnExcerptforPiano).mp3', 1.0), ('2-39 Ice Mountain.mp3', 0.0), ('2-12 King Kaliente.mp3', 0.0), ('04Cerberus.mp3', 1.0), ('2-52 A Wish.mp3', 0.0), ('05Hear!TheSirenSongCallofDeath.mp3', 1.0), ('2-21 Beach Bowl Galaxy - Undersea.mp3', 0.0), ('1-23 Daybreak - A New Dawn.mp3', 0.0), ('02DuneEternal.mp3', 1.0), ('21TakeCare.mp3', 1.0), ('03WAR.mp3', 1.0), ('15InthePresenceofaKing.mp3', 1.0), ('12AltarsofApostasy(incl.-HallofSacreligiousRemains-).mp3', 1.0), ('01DeepBlue(EarlyVersion).mp3', 1.0), ('2-19 Space Athletic.mp3', 0.0), ('2-45 A-wa-wa-wa!.mp3', 0.0), ('1-25 Super Mario Galaxy.mp3', 0.0), ('2-03 Gateway Galaxy.mp3', 0.0), ('11Versus.mp3', 1.0), ('08SanctuaryintheGardenoftheMind.mp3', 1.0), ('05Thesongthatplaysinthelevelcolloquiallyknownas4-S.mp3', 1.0), ('1-20 Melty Molten Galaxy.mp3', 0.0), ('1-11 Battlerock Galaxy.mp3', 0.0), ('1-12 Beach Bowl Galaxy.mp3', 0.0), ('1-15 Waltz of the Boos.mp3', 0.0), ('1-06 Enter the Galaxy.mp3', 0.0), ('1-07 Egg Planet.mp3', 0.0), ('07-HeIstheLightinMyDarkness-.mp3', 1.0), ('2-26 Bowser Appears.mp3', 0.0), ('2-49 Sad Girl.mp3', 0.0), ('2-53 Family.mp3', 0.0), ('2-42 Fire Mario.mp3', 0.0), ('23TheFireIsGone(forMusicBox).mp3', 1.0), ('02IntotheFire.mp3', 1.0), ('1-08 Rosalina in the Observatory 1.mp3', 0.0), ('2-05 To the Observatory Grounds 1.mp3', 0.0), ('2-41 Lava Path.mp3', 0.0), ('08DeathOdyssey.mp3', 1.0), ('2-43 Dusty Dune Galaxy.mp3', 0.0), ('2-10 A Tense Moment.mp3', 0.0), ('2-46 Deep Dark Galaxy.mp3', 0.0), ('05AThousandGreetings.mp3', 1.0), ('(Alt)03UnstoppableForce.mp3', 1.0), ('03ORDER.mp3', 1.0), ('2-37 Kingfin.mp3', 0.0), ('1-27 Blue Sky Athletic.mp3', 0.0), ('1-01 Overture.mp3', 0.0), ('2-33 Major Burrows.mp3', 0.0), ('15Silence.Introspection..mp3', 1.0), ('2-22 Interlude.mp3', 0.0), ('(Alt)06AShatteredIllusion.mp3', 1.0), ('2-30 Rainbow Mario.mp3', 0.0), ('1-24 Birth.mp3', 0.0), ('2-50 Flying Mario.mp3', 0.0), ('2-25 The Big Staircase.mp3', 0.0), ('2-27 Star Ball.mp3', 0.0), ('2-02 Luma.mp3', 0.0), ('03UnstoppableForce.mp3', 1.0), (\"1-05 Peach's Castle Stolen.mp3\", 0.0), ('11ChordoftheCrookedSaints.mp3', 1.0), ('01TheFireIsGone(forPiano,SaxophoneandTrumpet).mp3', 1.0), ('1-19 King Bowser.mp3', 0.0), ('2-13 The Toad Brigade.mp3', 0.0), ('TheCyberGrind.mp3', 1.0), ('2-48 Star Ball 2.mp3', 0.0), ('09CastleVein.mp3', 1.0), ('2-16 Space Fantasy.mp3', 0.0), ('01Intro(WeihnachtenAmKlavier).mp3', 1.0), ('1-09 The Honeyhive.mp3', 0.0), ('2-04 Stolen Grand Star.mp3', 0.0), ('2-24 The Fiery Stronghold.mp3', 0.0), ('20Disgrace.Humiliation..mp3', 1.0), ('1-02 The Star Festival.mp3', 0.0), ('2-18 To the Observatory Grounds 2.mp3', 0.0), ('02PANDEMONIUM.mp3', 1.0), ('2-01 File Select.mp3', 0.0), ('2-35 Cosmic Comet.mp3', 0.0), ('1-14 Enter Bowser Jr.!.mp3', 0.0), ('2-17 Megaleg.mp3', 0.0), ('2-36 Drip Drop Galaxy.mp3', 0.0), ('07ACompleteandUtterDestructionoftheSenses.mp3', 1.0), ('13Requiem.mp3', 1.0), ('1-04 Catastrophe.mp3', 0.0), ('04DoRobotsDreamofEternalSleep-.mp3', 1.0), ('02CHAOS.mp3', 1.0), ('1-10 Space Junk Road.mp3', 0.0), ('2-38 Boo Race.mp3', 0.0), ('2-32 Help!.mp3', 0.0), ('09DeathOdysseyAftermath.mp3', 1.0), (\"14TheDeathofGod'sWill(incl.-HornsofInsurrection-).mp3\", 1.0), ('01TheWorldLooksWhite.mp3', 1.0), ('19DivineIntervention.mp3', 1.0), ('08WarWithoutReason.mp3', 1.0), ('13FallenAngel(AnAdaptationofBach-BWV639).mp3', 1.0), ('2-20 Speedy Comet.mp3', 0.0), ('2-47 Kamella.mp3', 0.0), ('17Glory.mp3', 1.0), ('1-16 Buoy Base Galaxy.mp3', 0.0), ('(Alt)02IntotheFire.mp3', 1.0), ('06AShatteredIllusion.mp3', 1.0), ('1-03 Attack of the Airships.mp3', 0.0), ('06DeepBlue.mp3', 1.0), ('TenebreRossoSangue.mp3', 1.0), ('2-28 The Library.mp3', 0.0), ('2-51 Star Child.mp3', 0.0), ('12ColdWinds.mp3', 1.0), ('2-11 Big Bad Bugaboom.mp3', 0.0), ('05Duel(VersusReprise).mp3', 1.0), ('2-31 Chase the Bunnies!.mp3', 0.0), ('02TheWorldLooksRed.mp3', 1.0), ('2-40 Ice Mario.mp3', 0.0), ('03SandsofTide.mp3', 1.0), ('07DanseMacabre.mp3', 1.0), ('1-21 The Galaxy Reactor.mp3', 0.0), ('2-06 Observation Dome.mp3', 0.0), ('2-07 Course Select.mp3', 0.0), ('2-08 Dino Piranha.mp3', 0.0))\n"
     ]
    }
   ],
   "source": [
    "labels = np.concatenate([np.ones(65), np.zeros(81)])\n",
    "labels_with_titles = list(zip(all_OST, labels))\n",
    "all_data = list(zip(padded_features, labels_with_titles))\n",
    "rd.shuffle(all_data)\n",
    "songs, labels_with_titles = zip(*all_data)\n",
    "titles, labels = zip(*labels_with_titles)\n",
    "print(labels_with_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(len(songs) * 0.75)\n",
    "train_songs = [songs[i] for i in range(train_split)]\n",
    "train_labels = [labels[i] for i in range(train_split)]\n",
    "train_titles = [titles[i] for i in range(train_split)]\n",
    "\n",
    "test_split = range(train_split, len(songs))\n",
    "test_songs = [songs[i] for i in test_split]\n",
    "test_labels = [labels[i] for i in test_split]\n",
    "test_titles = [titles[i] for i in test_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69887536\n",
      "Iteration 2, loss = 0.67990287\n",
      "Iteration 3, loss = 0.67954614\n",
      "Iteration 4, loss = 0.67940987\n",
      "Iteration 5, loss = 0.67925829\n",
      "Iteration 6, loss = 0.67910172\n",
      "Iteration 7, loss = 0.67894644\n",
      "Iteration 8, loss = 0.67879662\n",
      "Iteration 9, loss = 0.67865518\n",
      "Iteration 10, loss = 0.67852415\n",
      "Iteration 11, loss = 0.67840489\n",
      "Iteration 12, loss = 0.67829828\n",
      "Iteration 13, loss = 0.67820476\n",
      "Iteration 14, loss = 0.67812442\n",
      "Iteration 15, loss = 0.67805701\n",
      "Iteration 16, loss = 0.67800203\n",
      "Iteration 17, loss = 0.67795872\n",
      "Iteration 18, loss = 0.67792613\n",
      "Iteration 19, loss = 0.67790317\n",
      "Iteration 20, loss = 0.67788859\n",
      "Iteration 21, loss = 0.67788110\n",
      "Iteration 22, loss = 0.67787936\n",
      "Iteration 23, loss = 0.67788205\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76377790\n",
      "Iteration 2, loss = 0.74642104\n",
      "Iteration 3, loss = 0.72527975\n",
      "Iteration 4, loss = 0.71447874\n",
      "Iteration 5, loss = 0.70267381\n",
      "Iteration 6, loss = 0.70111457\n",
      "Iteration 7, loss = 0.72060752\n",
      "Iteration 8, loss = 0.73330978\n",
      "Iteration 9, loss = 0.73356199\n",
      "Iteration 10, loss = 0.72158545\n",
      "Iteration 11, loss = 0.72002157\n",
      "Iteration 12, loss = 0.69616294\n",
      "Iteration 13, loss = 0.68171839\n",
      "Iteration 14, loss = 0.67608828\n",
      "Iteration 15, loss = 0.67349880\n",
      "Iteration 16, loss = 0.67097796\n",
      "Iteration 17, loss = 0.67095371\n",
      "Iteration 18, loss = 0.67367350\n",
      "Iteration 19, loss = 0.67925343\n",
      "Iteration 20, loss = 0.67705517\n",
      "Iteration 21, loss = 0.67750943\n",
      "Iteration 22, loss = 0.67550217\n",
      "Iteration 23, loss = 0.67342783\n",
      "Iteration 24, loss = 0.67139478\n",
      "Iteration 25, loss = 0.66940175\n",
      "Iteration 26, loss = 0.66744760\n",
      "Iteration 27, loss = 0.66553130\n",
      "Iteration 28, loss = 0.66365192\n",
      "Iteration 29, loss = 0.66180864\n",
      "Iteration 30, loss = 0.66000067\n",
      "Iteration 31, loss = 0.65822731\n",
      "Iteration 32, loss = 0.65648790\n",
      "Iteration 33, loss = 0.65478229\n",
      "Iteration 34, loss = 0.65315494\n",
      "Iteration 35, loss = 0.65169168\n",
      "Iteration 36, loss = 0.64985840\n",
      "Iteration 37, loss = 0.64828050\n",
      "Iteration 38, loss = 0.64673335\n",
      "Iteration 39, loss = 0.64521652\n",
      "Iteration 40, loss = 0.64372958\n",
      "Iteration 41, loss = 0.64227211\n",
      "Iteration 42, loss = 0.64084371\n",
      "Iteration 43, loss = 0.63944398\n",
      "Iteration 44, loss = 0.63807254\n",
      "Iteration 45, loss = 0.63672899\n",
      "Iteration 46, loss = 0.63541295\n",
      "Iteration 47, loss = 0.63412406\n",
      "Iteration 48, loss = 0.63286192\n",
      "Iteration 49, loss = 0.63162618\n",
      "Iteration 50, loss = 0.63041645\n",
      "Iteration 51, loss = 0.62923237\n",
      "Iteration 52, loss = 0.62807356\n",
      "Iteration 53, loss = 0.62693965\n",
      "Iteration 54, loss = 0.62583028\n",
      "Iteration 55, loss = 0.62474506\n",
      "Iteration 56, loss = 0.62368363\n",
      "Iteration 57, loss = 0.62264562\n",
      "Iteration 58, loss = 0.62163065\n",
      "Iteration 59, loss = 0.62063836\n",
      "Iteration 60, loss = 0.61966836\n",
      "Iteration 61, loss = 0.61872029\n",
      "Iteration 62, loss = 0.61779377\n",
      "Iteration 63, loss = 0.61688844\n",
      "Iteration 64, loss = 0.61600392\n",
      "Iteration 65, loss = 0.61513983\n",
      "Iteration 66, loss = 0.61429581\n",
      "Iteration 67, loss = 0.61347149\n",
      "Iteration 68, loss = 0.61266649\n",
      "Iteration 69, loss = 0.61188045\n",
      "Iteration 70, loss = 0.61111300\n",
      "Iteration 71, loss = 0.61036378\n",
      "Iteration 72, loss = 0.60963241\n",
      "Iteration 73, loss = 0.60891854\n",
      "Iteration 74, loss = 0.60822181\n",
      "Iteration 75, loss = 0.60754185\n",
      "Iteration 76, loss = 0.60687832\n",
      "Iteration 77, loss = 0.60623084\n",
      "Iteration 78, loss = 0.60559909\n",
      "Iteration 79, loss = 0.60498270\n",
      "Iteration 80, loss = 0.60438133\n",
      "Iteration 81, loss = 0.60379463\n",
      "Iteration 82, loss = 0.60322228\n",
      "Iteration 83, loss = 0.60266392\n",
      "Iteration 84, loss = 0.60211923\n",
      "Iteration 85, loss = 0.60158789\n",
      "Iteration 86, loss = 0.60106956\n",
      "Iteration 87, loss = 0.60056392\n",
      "Iteration 88, loss = 0.60007066\n",
      "Iteration 89, loss = 0.59958946\n",
      "Iteration 90, loss = 0.59912002\n",
      "Iteration 91, loss = 0.59866203\n",
      "Iteration 92, loss = 0.59821520\n",
      "Iteration 93, loss = 0.59777922\n",
      "Iteration 94, loss = 0.59735381\n",
      "Iteration 95, loss = 0.59693868\n",
      "Iteration 96, loss = 0.59653355\n",
      "Iteration 97, loss = 0.59613815\n",
      "Iteration 98, loss = 0.59575221\n",
      "Iteration 99, loss = 0.59537545\n",
      "Iteration 100, loss = 0.59500763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.74548545\n",
      "Iteration 2, loss = 0.74291244\n",
      "Iteration 3, loss = 0.74041612\n",
      "Iteration 4, loss = 0.73859098\n",
      "Iteration 5, loss = 0.73679171\n",
      "Iteration 6, loss = 0.73501879\n",
      "Iteration 7, loss = 0.73327277\n",
      "Iteration 8, loss = 0.73155417\n",
      "Iteration 9, loss = 0.72986350\n",
      "Iteration 10, loss = 0.72820121\n",
      "Iteration 11, loss = 0.72656777\n",
      "Iteration 12, loss = 0.72496356\n",
      "Iteration 13, loss = 0.72338897\n",
      "Iteration 14, loss = 0.72184434\n",
      "Iteration 15, loss = 0.72032997\n",
      "Iteration 16, loss = 0.71884615\n",
      "Iteration 17, loss = 0.71739312\n",
      "Iteration 18, loss = 0.71597109\n",
      "Iteration 19, loss = 0.71458022\n",
      "Iteration 20, loss = 0.71322067\n",
      "Iteration 21, loss = 0.71189252\n",
      "Iteration 22, loss = 0.71059587\n",
      "Iteration 23, loss = 0.70933073\n",
      "Iteration 24, loss = 0.70809712\n",
      "Iteration 25, loss = 0.70689500\n",
      "Iteration 26, loss = 0.70572431\n",
      "Iteration 27, loss = 0.70458496\n",
      "Iteration 28, loss = 0.70347682\n",
      "Iteration 29, loss = 0.70239972\n",
      "Iteration 30, loss = 0.70135348\n",
      "Iteration 31, loss = 0.70033789\n",
      "Iteration 32, loss = 0.69935269\n",
      "Iteration 33, loss = 0.69839761\n",
      "Iteration 34, loss = 0.69747236\n",
      "Iteration 35, loss = 0.69657662\n",
      "Iteration 36, loss = 0.69571002\n",
      "Iteration 37, loss = 0.69487221\n",
      "Iteration 38, loss = 0.69406278\n",
      "Iteration 39, loss = 0.69328134\n",
      "Iteration 40, loss = 0.69252744\n",
      "Iteration 41, loss = 0.69180063\n",
      "Iteration 42, loss = 0.69110045\n",
      "Iteration 43, loss = 0.69042642\n",
      "Iteration 44, loss = 0.68977804\n",
      "Iteration 45, loss = 0.68915479\n",
      "Iteration 46, loss = 0.68855616\n",
      "Iteration 47, loss = 0.68798161\n",
      "Iteration 48, loss = 0.68743061\n",
      "Iteration 49, loss = 0.68690259\n",
      "Iteration 50, loss = 0.68639700\n",
      "Iteration 51, loss = 0.68591328\n",
      "Iteration 52, loss = 0.68545084\n",
      "Iteration 53, loss = 0.68500913\n",
      "Iteration 54, loss = 0.68458756\n",
      "Iteration 55, loss = 0.68418555\n",
      "Iteration 56, loss = 0.68380252\n",
      "Iteration 57, loss = 0.68343788\n",
      "Iteration 58, loss = 0.68309107\n",
      "Iteration 59, loss = 0.68276149\n",
      "Iteration 60, loss = 0.68244858\n",
      "Iteration 61, loss = 0.68215176\n",
      "Iteration 62, loss = 0.68187046\n",
      "Iteration 63, loss = 0.68160412\n",
      "Iteration 64, loss = 0.68135219\n",
      "Iteration 65, loss = 0.68111410\n",
      "Iteration 66, loss = 0.68088933\n",
      "Iteration 67, loss = 0.68067733\n",
      "Iteration 68, loss = 0.68047758\n",
      "Iteration 69, loss = 0.68028956\n",
      "Iteration 70, loss = 0.68011277\n",
      "Iteration 71, loss = 0.67994670\n",
      "Iteration 72, loss = 0.67979088\n",
      "Iteration 73, loss = 0.67964482\n",
      "Iteration 74, loss = 0.67950806\n",
      "Iteration 75, loss = 0.67938016\n",
      "Iteration 76, loss = 0.67926067\n",
      "Iteration 77, loss = 0.67914917\n",
      "Iteration 78, loss = 0.67904524\n",
      "Iteration 79, loss = 0.67894848\n",
      "Iteration 80, loss = 0.67885851\n",
      "Iteration 81, loss = 0.67877495\n",
      "Iteration 82, loss = 0.67869744\n",
      "Iteration 83, loss = 0.67862563\n",
      "Iteration 84, loss = 0.67855918\n",
      "Iteration 85, loss = 0.67849778\n",
      "Iteration 86, loss = 0.67844112\n",
      "Iteration 87, loss = 0.67838890\n",
      "Iteration 88, loss = 0.67834083\n",
      "Iteration 89, loss = 0.67829666\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.89194091\n",
      "Iteration 2, loss = 0.83453392\n",
      "Iteration 3, loss = 0.82160916\n",
      "Iteration 4, loss = 0.81935793\n",
      "Iteration 5, loss = 0.81658005\n",
      "Iteration 6, loss = 0.81412886\n",
      "Iteration 7, loss = 0.81161406\n",
      "Iteration 8, loss = 0.80905946\n",
      "Iteration 9, loss = 0.80648000\n",
      "Iteration 10, loss = 0.80388576\n",
      "Iteration 11, loss = 0.80128385\n",
      "Iteration 12, loss = 0.79867956\n",
      "Iteration 13, loss = 0.79607677\n",
      "Iteration 14, loss = 0.79346542\n",
      "Iteration 15, loss = 0.79007391\n",
      "Iteration 16, loss = 0.78746636\n",
      "Iteration 17, loss = 0.78398206\n",
      "Iteration 18, loss = 0.78139099\n",
      "Iteration 19, loss = 0.77878804\n",
      "Iteration 20, loss = 0.77619722\n",
      "Iteration 21, loss = 0.77361977\n",
      "Iteration 22, loss = 0.77105676\n",
      "Iteration 23, loss = 0.76850909\n",
      "Iteration 24, loss = 0.76597755\n",
      "Iteration 25, loss = 0.76346283\n",
      "Iteration 26, loss = 0.76096551\n",
      "Iteration 27, loss = 0.75848609\n",
      "Iteration 28, loss = 0.75602498\n",
      "Iteration 29, loss = 0.75358238\n",
      "Iteration 30, loss = 0.75115801\n",
      "Iteration 31, loss = 0.74874885\n",
      "Iteration 32, loss = 0.74629741\n",
      "Iteration 33, loss = 0.74270409\n",
      "Iteration 34, loss = 0.74032826\n",
      "Iteration 35, loss = 0.73796913\n",
      "Iteration 36, loss = 0.73423983\n",
      "Iteration 37, loss = 0.73188623\n",
      "Iteration 38, loss = 0.72955038\n",
      "Iteration 39, loss = 0.72723165\n",
      "Iteration 40, loss = 0.72492366\n",
      "Iteration 41, loss = 0.72207246\n",
      "Iteration 42, loss = 0.71250814\n",
      "Iteration 43, loss = 0.71154777\n",
      "Iteration 44, loss = 0.70915516\n",
      "Iteration 45, loss = 0.70993393\n",
      "Iteration 46, loss = 0.70908424\n",
      "Iteration 47, loss = 0.70834026\n",
      "Iteration 48, loss = 0.70937999\n",
      "Iteration 49, loss = 0.70717955\n",
      "Iteration 50, loss = 0.70499695\n",
      "Iteration 51, loss = 0.70168217\n",
      "Iteration 52, loss = 0.70069185\n",
      "Iteration 53, loss = 0.69856594\n",
      "Iteration 54, loss = 0.69288785\n",
      "Iteration 55, loss = 0.68357312\n",
      "Iteration 56, loss = 0.68264256\n",
      "Iteration 57, loss = 0.68000603\n",
      "Iteration 58, loss = 0.67998051\n",
      "Iteration 59, loss = 0.68004371\n",
      "Iteration 60, loss = 0.67802020\n",
      "Iteration 61, loss = 0.68044710\n",
      "Iteration 62, loss = 0.68304355\n",
      "Iteration 63, loss = 0.68124334\n",
      "Iteration 64, loss = 0.68178638\n",
      "Iteration 65, loss = 0.68009051\n",
      "Iteration 66, loss = 0.67841834\n",
      "Iteration 67, loss = 0.67677539\n",
      "Iteration 68, loss = 0.67516050\n",
      "Iteration 69, loss = 0.67364807\n",
      "Iteration 70, loss = 0.67256088\n",
      "Iteration 71, loss = 0.67047449\n",
      "Iteration 72, loss = 0.66896250\n",
      "Iteration 73, loss = 0.66747389\n",
      "Iteration 74, loss = 0.66600799\n",
      "Iteration 75, loss = 0.66456416\n",
      "Iteration 76, loss = 0.66314185\n",
      "Iteration 77, loss = 0.66174052\n",
      "Iteration 78, loss = 0.66035971\n",
      "Iteration 79, loss = 0.65899863\n",
      "Iteration 80, loss = 0.65764708\n",
      "Iteration 81, loss = 0.65597808\n",
      "Iteration 82, loss = 0.65209550\n",
      "Iteration 83, loss = 0.65077385\n",
      "Iteration 84, loss = 0.64946789\n",
      "Iteration 85, loss = 0.64817764\n",
      "Iteration 86, loss = 0.64690310\n",
      "Iteration 87, loss = 0.64564427\n",
      "Iteration 88, loss = 0.64440113\n",
      "Iteration 89, loss = 0.64317364\n",
      "Iteration 90, loss = 0.64196178\n",
      "Iteration 91, loss = 0.64076550\n",
      "Iteration 92, loss = 0.63958474\n",
      "Iteration 93, loss = 0.63841945\n",
      "Iteration 94, loss = 0.63726956\n",
      "Iteration 95, loss = 0.63613500\n",
      "Iteration 96, loss = 0.63501568\n",
      "Iteration 97, loss = 0.63391154\n",
      "Iteration 98, loss = 0.63282246\n",
      "Iteration 99, loss = 0.63174838\n",
      "Iteration 100, loss = 0.63068918\n",
      "Iteration 1, loss = 1.20426271\n",
      "Iteration 2, loss = 1.08632921\n",
      "Iteration 3, loss = 1.07932601\n",
      "Iteration 4, loss = 1.07410999\n",
      "Iteration 5, loss = 1.06984855\n",
      "Iteration 6, loss = 1.06443134\n",
      "Iteration 7, loss = 1.05922158\n",
      "Iteration 8, loss = 1.05400961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 1.04880145\n",
      "Iteration 10, loss = 1.04341873\n",
      "Iteration 11, loss = 1.03756332\n",
      "Iteration 12, loss = 1.03237122\n",
      "Iteration 13, loss = 1.02630685\n",
      "Iteration 14, loss = 1.02112813\n",
      "Iteration 15, loss = 1.01596707\n",
      "Iteration 16, loss = 1.01082512\n",
      "Iteration 17, loss = 1.00570353\n",
      "Iteration 18, loss = 1.00575171\n",
      "Iteration 19, loss = 0.99617156\n",
      "Iteration 20, loss = 0.99047786\n",
      "Iteration 21, loss = 0.98544988\n",
      "Iteration 22, loss = 0.98044604\n",
      "Iteration 23, loss = 0.97546705\n",
      "Iteration 24, loss = 0.97161813\n",
      "Iteration 25, loss = 0.96671336\n",
      "Iteration 26, loss = 0.96183633\n",
      "Iteration 27, loss = 0.95698762\n",
      "Iteration 28, loss = 0.95216754\n",
      "Iteration 29, loss = 0.94737638\n",
      "Iteration 30, loss = 0.94384885\n",
      "Iteration 31, loss = 0.93950430\n",
      "Iteration 32, loss = 0.93446326\n",
      "Iteration 33, loss = 0.92981767\n",
      "Iteration 34, loss = 0.92520341\n",
      "Iteration 35, loss = 0.92062056\n",
      "Iteration 36, loss = 0.91606922\n",
      "Iteration 37, loss = 0.91154945\n",
      "Iteration 38, loss = 0.90706135\n",
      "Iteration 39, loss = 0.90260500\n",
      "Iteration 40, loss = 0.89818047\n",
      "Iteration 41, loss = 0.89378785\n",
      "Iteration 42, loss = 0.88942720\n",
      "Iteration 43, loss = 0.88509860\n",
      "Iteration 44, loss = 0.88080210\n",
      "Iteration 45, loss = 0.87653778\n",
      "Iteration 46, loss = 0.87230567\n",
      "Iteration 47, loss = 0.86810585\n",
      "Iteration 48, loss = 0.86393834\n",
      "Iteration 49, loss = 0.85980319\n",
      "Iteration 50, loss = 0.85570043\n",
      "Iteration 51, loss = 0.85163009\n",
      "Iteration 52, loss = 0.84759220\n",
      "Iteration 53, loss = 0.84358677\n",
      "Iteration 54, loss = 0.83961381\n",
      "Iteration 55, loss = 0.83567334\n",
      "Iteration 56, loss = 0.83176534\n",
      "Iteration 57, loss = 0.82788983\n",
      "Iteration 58, loss = 0.82404678\n",
      "Iteration 59, loss = 0.82023619\n",
      "Iteration 60, loss = 0.81645804\n",
      "Iteration 61, loss = 0.81271229\n",
      "Iteration 62, loss = 0.80899893\n",
      "Iteration 63, loss = 0.80531791\n",
      "Iteration 64, loss = 0.80166920\n",
      "Iteration 65, loss = 0.79805276\n",
      "Iteration 66, loss = 0.79446852\n",
      "Iteration 67, loss = 0.79091645\n",
      "Iteration 68, loss = 0.78739648\n",
      "Iteration 69, loss = 0.78390855\n",
      "Iteration 70, loss = 0.78045259\n",
      "Iteration 71, loss = 0.77702854\n",
      "Iteration 72, loss = 0.77363632\n",
      "Iteration 73, loss = 0.77027584\n",
      "Iteration 74, loss = 0.76694703\n",
      "Iteration 75, loss = 0.76364980\n",
      "Iteration 76, loss = 0.76038406\n",
      "Iteration 77, loss = 0.75714972\n",
      "Iteration 78, loss = 0.75394667\n",
      "Iteration 79, loss = 0.75077482\n",
      "Iteration 80, loss = 0.74763406\n",
      "Iteration 81, loss = 0.74452429\n",
      "Iteration 82, loss = 0.74144539\n",
      "Iteration 83, loss = 0.73839725\n",
      "Iteration 84, loss = 0.73537975\n",
      "Iteration 85, loss = 0.73239277\n",
      "Iteration 86, loss = 0.72943620\n",
      "Iteration 87, loss = 0.72650989\n",
      "Iteration 88, loss = 0.72361374\n",
      "Iteration 89, loss = 0.72074759\n",
      "Iteration 90, loss = 0.71791133\n",
      "Iteration 91, loss = 0.71510481\n",
      "Iteration 92, loss = 0.71232790\n",
      "Iteration 93, loss = 0.70958045\n",
      "Iteration 94, loss = 0.70686233\n",
      "Iteration 95, loss = 0.70417338\n",
      "Iteration 96, loss = 0.70151346\n",
      "Iteration 97, loss = 0.69888243\n",
      "Iteration 98, loss = 0.69628012\n",
      "Iteration 99, loss = 0.69370638\n",
      "Iteration 100, loss = 0.69116107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.82170390\n",
      "Iteration 2, loss = 0.61577448\n",
      "Iteration 3, loss = 0.59443812\n",
      "Iteration 4, loss = 0.57974135\n",
      "Iteration 5, loss = 0.57778815\n",
      "Iteration 6, loss = 0.56110112\n",
      "Iteration 7, loss = 0.55431724\n",
      "Iteration 8, loss = 0.55149195\n",
      "Iteration 9, loss = 0.54602281\n",
      "Iteration 10, loss = 0.54421239\n",
      "Iteration 11, loss = 0.54660012\n",
      "Iteration 12, loss = 0.53692618\n",
      "Iteration 13, loss = 0.53420326\n",
      "Iteration 14, loss = 0.52943228\n",
      "Iteration 15, loss = 0.52509937\n",
      "Iteration 16, loss = 0.52651679\n",
      "Iteration 17, loss = 0.52701203\n",
      "Iteration 18, loss = 0.52391272\n",
      "Iteration 19, loss = 0.52694076\n",
      "Iteration 20, loss = 0.52207563\n",
      "Iteration 21, loss = 0.51554996\n",
      "Iteration 22, loss = 0.50863279\n",
      "Iteration 23, loss = 0.50516637\n",
      "Iteration 24, loss = 0.50497299\n",
      "Iteration 25, loss = 0.50747147\n",
      "Iteration 26, loss = 0.50315510\n",
      "Iteration 27, loss = 0.50152450\n",
      "Iteration 28, loss = 0.49649059\n",
      "Iteration 29, loss = 0.49531841\n",
      "Iteration 30, loss = 0.49081802\n",
      "Iteration 31, loss = 0.48756495\n",
      "Iteration 32, loss = 0.48132832\n",
      "Iteration 33, loss = 0.47362268\n",
      "Iteration 34, loss = 0.46411083\n",
      "Iteration 35, loss = 0.46572613\n",
      "Iteration 36, loss = 0.45538080\n",
      "Iteration 37, loss = 0.45224496\n",
      "Iteration 38, loss = 0.44777525\n",
      "Iteration 39, loss = 0.44651691\n",
      "Iteration 40, loss = 0.43839930\n",
      "Iteration 41, loss = 0.43619549\n",
      "Iteration 42, loss = 0.43987990\n",
      "Iteration 43, loss = 0.43058726\n",
      "Iteration 44, loss = 0.43076184\n",
      "Iteration 45, loss = 0.42932077\n",
      "Iteration 46, loss = 0.43580722\n",
      "Iteration 47, loss = 0.44192549\n",
      "Iteration 48, loss = 0.45043605\n",
      "Iteration 49, loss = 0.45360191\n",
      "Iteration 50, loss = 0.45681651\n",
      "Iteration 51, loss = 0.45624887\n",
      "Iteration 52, loss = 0.45201609\n",
      "Iteration 53, loss = 0.45210247\n",
      "Iteration 54, loss = 0.45089670\n",
      "Iteration 55, loss = 0.44736208\n",
      "Iteration 56, loss = 0.44663558\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71203847\n",
      "Iteration 2, loss = 0.59558246\n",
      "Iteration 3, loss = 0.55073072\n",
      "Iteration 4, loss = 0.50738614\n",
      "Iteration 5, loss = 0.53081370\n",
      "Iteration 6, loss = 0.55245060\n",
      "Iteration 7, loss = 0.54334307\n",
      "Iteration 8, loss = 0.55455532\n",
      "Iteration 9, loss = 0.57297684\n",
      "Iteration 10, loss = 0.57208590\n",
      "Iteration 11, loss = 0.55828880\n",
      "Iteration 12, loss = 0.56206246\n",
      "Iteration 13, loss = 0.55905146\n",
      "Iteration 14, loss = 0.55381600\n",
      "Iteration 15, loss = 0.54816768\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66257234\n",
      "Iteration 2, loss = 0.54170561\n",
      "Iteration 3, loss = 0.51974135\n",
      "Iteration 4, loss = 0.52418228\n",
      "Iteration 5, loss = 0.51679674\n",
      "Iteration 6, loss = 0.51955911\n",
      "Iteration 7, loss = 0.51081917\n",
      "Iteration 8, loss = 0.50769778\n",
      "Iteration 9, loss = 0.50027435\n",
      "Iteration 10, loss = 0.51023873\n",
      "Iteration 11, loss = 0.50588661\n",
      "Iteration 12, loss = 0.51416844\n",
      "Iteration 13, loss = 0.50531229\n",
      "Iteration 14, loss = 0.51638874\n",
      "Iteration 15, loss = 0.52425849\n",
      "Iteration 16, loss = 0.50990565\n",
      "Iteration 17, loss = 0.50881648\n",
      "Iteration 18, loss = 0.50686686\n",
      "Iteration 19, loss = 0.49734340\n",
      "Iteration 20, loss = 0.49097567\n",
      "Iteration 21, loss = 0.48144017\n",
      "Iteration 22, loss = 0.49101343\n",
      "Iteration 23, loss = 0.49832235\n",
      "Iteration 24, loss = 0.51443232\n",
      "Iteration 25, loss = 0.50929419\n",
      "Iteration 26, loss = 0.50174943\n",
      "Iteration 27, loss = 0.50093369\n",
      "Iteration 28, loss = 0.49665559\n",
      "Iteration 29, loss = 0.49381489\n",
      "Iteration 30, loss = 0.49280814\n",
      "Iteration 31, loss = 0.48952787\n",
      "Iteration 32, loss = 0.48551346\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77150447\n",
      "Iteration 2, loss = 0.56298242\n",
      "Iteration 3, loss = 0.53235817\n",
      "Iteration 4, loss = 0.51357985\n",
      "Iteration 5, loss = 0.49835946\n",
      "Iteration 6, loss = 0.48824786\n",
      "Iteration 7, loss = 0.48894138\n",
      "Iteration 8, loss = 0.47447033\n",
      "Iteration 9, loss = 0.46389015\n",
      "Iteration 10, loss = 0.45526081\n",
      "Iteration 11, loss = 0.45683140\n",
      "Iteration 12, loss = 0.45952160\n",
      "Iteration 13, loss = 0.45769899\n",
      "Iteration 14, loss = 0.46448900\n",
      "Iteration 15, loss = 0.45786394\n",
      "Iteration 16, loss = 0.45615107\n",
      "Iteration 17, loss = 0.45502479\n",
      "Iteration 18, loss = 0.45573205\n",
      "Iteration 19, loss = 0.44844991\n",
      "Iteration 20, loss = 0.45262993\n",
      "Iteration 21, loss = 0.45139373\n",
      "Iteration 22, loss = 0.45168679\n",
      "Iteration 23, loss = 0.44200248\n",
      "Iteration 24, loss = 0.44337645\n",
      "Iteration 25, loss = 0.43693985\n",
      "Iteration 26, loss = 0.43188522\n",
      "Iteration 27, loss = 0.43491055\n",
      "Iteration 28, loss = 0.43476726\n",
      "Iteration 29, loss = 0.44448422\n",
      "Iteration 30, loss = 0.44414239\n",
      "Iteration 31, loss = 0.44600318\n",
      "Iteration 32, loss = 0.43907229\n",
      "Iteration 33, loss = 0.43452483\n",
      "Iteration 34, loss = 0.42685672\n",
      "Iteration 35, loss = 0.42421525\n",
      "Iteration 36, loss = 0.42799214\n",
      "Iteration 37, loss = 0.43477143\n",
      "Iteration 38, loss = 0.43327425\n",
      "Iteration 39, loss = 0.43214668\n",
      "Iteration 40, loss = 0.43142892\n",
      "Iteration 41, loss = 0.43269390\n",
      "Iteration 42, loss = 0.43368543\n",
      "Iteration 43, loss = 0.42985204\n",
      "Iteration 44, loss = 0.42572775\n",
      "Iteration 45, loss = 0.42922760\n",
      "Iteration 46, loss = 0.42776738\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66043175\n",
      "Iteration 2, loss = 0.69739046\n",
      "Iteration 3, loss = 0.64891820\n",
      "Iteration 4, loss = 0.64718951\n",
      "Iteration 5, loss = 0.61509257\n",
      "Iteration 6, loss = 0.59149399\n",
      "Iteration 7, loss = 0.57879881\n",
      "Iteration 8, loss = 0.57267604\n",
      "Iteration 9, loss = 0.56427018\n",
      "Iteration 10, loss = 0.55620457\n",
      "Iteration 11, loss = 0.54604931\n",
      "Iteration 12, loss = 0.54248407\n",
      "Iteration 13, loss = 0.54230303\n",
      "Iteration 14, loss = 0.54096668\n",
      "Iteration 15, loss = 0.53800244\n",
      "Iteration 16, loss = 0.53033659\n",
      "Iteration 17, loss = 0.52702264\n",
      "Iteration 18, loss = 0.53101794\n",
      "Iteration 19, loss = 0.52444303\n",
      "Iteration 20, loss = 0.52692871\n",
      "Iteration 21, loss = 0.52214464\n",
      "Iteration 22, loss = 0.52023375\n",
      "Iteration 23, loss = 0.51921636\n",
      "Iteration 24, loss = 0.51947854\n",
      "Iteration 25, loss = 0.51700432\n",
      "Iteration 26, loss = 0.50983308\n",
      "Iteration 27, loss = 0.50451755\n",
      "Iteration 28, loss = 0.49874009\n",
      "Iteration 29, loss = 0.49501445\n",
      "Iteration 30, loss = 0.49242197\n",
      "Iteration 31, loss = 0.48984415\n",
      "Iteration 32, loss = 0.48697018\n",
      "Iteration 33, loss = 0.48738888\n",
      "Iteration 34, loss = 0.48760144\n",
      "Iteration 35, loss = 0.48812565\n",
      "Iteration 36, loss = 0.48544286\n",
      "Iteration 37, loss = 0.48251314\n",
      "Iteration 38, loss = 0.48006570\n",
      "Iteration 39, loss = 0.47848906\n",
      "Iteration 40, loss = 0.47644322\n",
      "Iteration 41, loss = 0.48168996\n",
      "Iteration 42, loss = 0.48354895\n",
      "Iteration 43, loss = 0.47355317\n",
      "Iteration 44, loss = 0.47088488\n",
      "Iteration 45, loss = 0.46492859\n",
      "Iteration 46, loss = 0.47468322\n",
      "Iteration 47, loss = 0.47291924\n",
      "Iteration 48, loss = 0.48538525\n",
      "Iteration 49, loss = 0.48769205\n",
      "Iteration 50, loss = 0.47876560\n",
      "Iteration 51, loss = 0.47325426\n",
      "Iteration 52, loss = 0.46670844\n",
      "Iteration 53, loss = 0.46647817\n",
      "Iteration 54, loss = 0.46992430\n",
      "Iteration 55, loss = 0.47303661\n",
      "Iteration 56, loss = 0.47323866\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.02067159\n",
      "Iteration 2, loss = 0.55477346\n",
      "Iteration 3, loss = 0.52994684\n",
      "Iteration 4, loss = 0.49975544\n",
      "Iteration 5, loss = 0.46999987\n",
      "Iteration 6, loss = 0.45340750\n",
      "Iteration 7, loss = 0.44544086\n",
      "Iteration 8, loss = 0.44374640\n",
      "Iteration 9, loss = 0.44583216\n",
      "Iteration 10, loss = 0.44686366\n",
      "Iteration 11, loss = 0.43714277\n",
      "Iteration 12, loss = 0.42434229\n",
      "Iteration 13, loss = 0.41315596\n",
      "Iteration 14, loss = 0.41055107\n",
      "Iteration 15, loss = 0.40375184\n",
      "Iteration 16, loss = 0.39980908\n",
      "Iteration 17, loss = 0.39731795\n",
      "Iteration 18, loss = 0.39237919\n",
      "Iteration 19, loss = 0.39795454\n",
      "Iteration 20, loss = 0.39725399\n",
      "Iteration 21, loss = 0.39737563\n",
      "Iteration 22, loss = 0.40278058\n",
      "Iteration 23, loss = 0.40276006\n",
      "Iteration 24, loss = 0.40554443\n",
      "Iteration 25, loss = 0.39599386\n",
      "Iteration 26, loss = 0.37953076\n",
      "Iteration 27, loss = 0.37901290\n",
      "Iteration 28, loss = 0.37923198\n",
      "Iteration 29, loss = 0.37246644\n",
      "Iteration 30, loss = 0.37214994\n",
      "Iteration 31, loss = 0.37819897\n",
      "Iteration 32, loss = 0.37164460\n",
      "Iteration 33, loss = 0.36956973\n",
      "Iteration 34, loss = 0.37555497\n",
      "Iteration 35, loss = 0.37058090\n",
      "Iteration 36, loss = 0.37160155\n",
      "Iteration 37, loss = 0.37412483\n",
      "Iteration 38, loss = 0.36925537\n",
      "Iteration 39, loss = 0.36782251\n",
      "Iteration 40, loss = 0.36619749\n",
      "Iteration 41, loss = 0.36461956\n",
      "Iteration 42, loss = 0.36479776\n",
      "Iteration 43, loss = 0.36594609\n",
      "Iteration 44, loss = 0.35887332\n",
      "Iteration 45, loss = 0.35526221\n",
      "Iteration 46, loss = 0.35341469\n",
      "Iteration 47, loss = 0.35013563\n",
      "Iteration 48, loss = 0.34457525\n",
      "Iteration 49, loss = 0.34253712\n",
      "Iteration 50, loss = 0.34610360\n",
      "Iteration 51, loss = 0.34421978\n",
      "Iteration 52, loss = 0.34085198\n",
      "Iteration 53, loss = 0.34014564\n",
      "Iteration 54, loss = 0.33985148\n",
      "Iteration 55, loss = 0.34002765\n",
      "Iteration 56, loss = 0.33496070\n",
      "Iteration 57, loss = 0.33300735\n",
      "Iteration 58, loss = 0.33025816\n",
      "Iteration 59, loss = 0.33044630\n",
      "Iteration 60, loss = 0.33024675\n",
      "Iteration 61, loss = 0.32874448\n",
      "Iteration 62, loss = 0.32610233\n",
      "Iteration 63, loss = 0.32366716\n",
      "Iteration 64, loss = 0.32206335\n",
      "Iteration 65, loss = 0.32207319\n",
      "Iteration 66, loss = 0.32102743\n",
      "Iteration 67, loss = 0.32059680\n",
      "Iteration 68, loss = 0.32054448\n",
      "Iteration 69, loss = 0.31619121\n",
      "Iteration 70, loss = 0.31210028\n",
      "Iteration 71, loss = 0.30901790\n",
      "Iteration 72, loss = 0.30466924\n",
      "Iteration 73, loss = 0.30163951\n",
      "Iteration 74, loss = 0.29675516\n",
      "Iteration 75, loss = 0.29489885\n",
      "Iteration 76, loss = 0.30242385\n",
      "Iteration 77, loss = 0.30760081\n",
      "Iteration 78, loss = 0.30240408\n",
      "Iteration 79, loss = 0.30147687\n",
      "Iteration 80, loss = 0.30325356\n",
      "Iteration 81, loss = 0.29807458\n",
      "Iteration 82, loss = 0.29337841\n",
      "Iteration 83, loss = 0.30371685\n",
      "Iteration 84, loss = 0.30100304\n",
      "Iteration 85, loss = 0.29877443\n",
      "Iteration 86, loss = 0.28888984\n",
      "Iteration 87, loss = 0.28893701\n",
      "Iteration 88, loss = 0.28780574\n",
      "Iteration 89, loss = 0.28607154\n",
      "Iteration 90, loss = 0.28550015\n",
      "Iteration 91, loss = 0.28585668\n",
      "Iteration 92, loss = 0.28508007\n",
      "Iteration 93, loss = 0.28186666\n",
      "Iteration 94, loss = 0.28532770\n",
      "Iteration 95, loss = 0.27735927\n",
      "Iteration 96, loss = 0.27461828\n",
      "Iteration 97, loss = 0.27888704\n",
      "Iteration 98, loss = 0.27738619\n",
      "Iteration 99, loss = 0.27780267\n",
      "Iteration 100, loss = 0.27534607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.91470556\n",
      "Iteration 2, loss = 0.57434597\n",
      "Iteration 3, loss = 0.46988471\n",
      "Iteration 4, loss = 0.42692996\n",
      "Iteration 5, loss = 0.41911762\n",
      "Iteration 6, loss = 0.42436819\n",
      "Iteration 7, loss = 0.40640203\n",
      "Iteration 8, loss = 0.40726736\n",
      "Iteration 9, loss = 0.40378157\n",
      "Iteration 10, loss = 0.40293460\n",
      "Iteration 11, loss = 0.40658976\n",
      "Iteration 12, loss = 0.40187683\n",
      "Iteration 13, loss = 0.39825993\n",
      "Iteration 14, loss = 0.39851787\n",
      "Iteration 15, loss = 0.39974456\n",
      "Iteration 16, loss = 0.39495927\n",
      "Iteration 17, loss = 0.39074159\n",
      "Iteration 18, loss = 0.39142647\n",
      "Iteration 19, loss = 0.38521788\n",
      "Iteration 20, loss = 0.38222268\n",
      "Iteration 21, loss = 0.37688149\n",
      "Iteration 22, loss = 0.37338313\n",
      "Iteration 23, loss = 0.37778107\n",
      "Iteration 24, loss = 0.37706104\n",
      "Iteration 25, loss = 0.37430750\n",
      "Iteration 26, loss = 0.37335185\n",
      "Iteration 27, loss = 0.37298493\n",
      "Iteration 28, loss = 0.38500694\n",
      "Iteration 29, loss = 0.37076145\n",
      "Iteration 30, loss = 0.36964683\n",
      "Iteration 31, loss = 0.37209929\n",
      "Iteration 32, loss = 0.37581855\n",
      "Iteration 33, loss = 0.37252138\n",
      "Iteration 34, loss = 0.37265923\n",
      "Iteration 35, loss = 0.37464659\n",
      "Iteration 36, loss = 0.37890720\n",
      "Iteration 37, loss = 0.36868474\n",
      "Iteration 38, loss = 0.36707361\n",
      "Iteration 39, loss = 0.36263722\n",
      "Iteration 40, loss = 0.36206867\n",
      "Iteration 41, loss = 0.35617893\n",
      "Iteration 42, loss = 0.36108868\n",
      "Iteration 43, loss = 0.35569959\n",
      "Iteration 44, loss = 0.35649223\n",
      "Iteration 45, loss = 0.35342802\n",
      "Iteration 46, loss = 0.35227099\n",
      "Iteration 47, loss = 0.35425631\n",
      "Iteration 48, loss = 0.34851236\n",
      "Iteration 49, loss = 0.34198089\n",
      "Iteration 50, loss = 0.34809609\n",
      "Iteration 51, loss = 0.35239162\n",
      "Iteration 52, loss = 0.35178945\n",
      "Iteration 53, loss = 0.35280990\n",
      "Iteration 54, loss = 0.34901340\n",
      "Iteration 55, loss = 0.34473449\n",
      "Iteration 56, loss = 0.34398515\n",
      "Iteration 57, loss = 0.34221619\n",
      "Iteration 58, loss = 0.34313314\n",
      "Iteration 59, loss = 0.34155729\n",
      "Iteration 60, loss = 0.33703710\n",
      "Iteration 61, loss = 0.33546260\n",
      "Iteration 62, loss = 0.33590270\n",
      "Iteration 63, loss = 0.33283728\n",
      "Iteration 64, loss = 0.32873685\n",
      "Iteration 65, loss = 0.33363526\n",
      "Iteration 66, loss = 0.32903409\n",
      "Iteration 67, loss = 0.32402559\n",
      "Iteration 68, loss = 0.32508040\n",
      "Iteration 69, loss = 0.32530800\n",
      "Iteration 70, loss = 0.32180400\n",
      "Iteration 71, loss = 0.31881505\n",
      "Iteration 72, loss = 0.31153032\n",
      "Iteration 73, loss = 0.30528683\n",
      "Iteration 74, loss = 0.30768422\n",
      "Iteration 75, loss = 0.30787512\n",
      "Iteration 76, loss = 0.31136029\n",
      "Iteration 77, loss = 0.30818922\n",
      "Iteration 78, loss = 0.31005030\n",
      "Iteration 79, loss = 0.30664192\n",
      "Iteration 80, loss = 0.30429798\n",
      "Iteration 81, loss = 0.30582444\n",
      "Iteration 82, loss = 0.30398855\n",
      "Iteration 83, loss = 0.30577820\n",
      "Iteration 84, loss = 0.30124673\n",
      "Iteration 85, loss = 0.29938047\n",
      "Iteration 86, loss = 0.29817245\n",
      "Iteration 87, loss = 0.29271023\n",
      "Iteration 88, loss = 0.29187977\n",
      "Iteration 89, loss = 0.29244548\n",
      "Iteration 90, loss = 0.29367447\n",
      "Iteration 91, loss = 0.29385772\n",
      "Iteration 92, loss = 0.29324233\n",
      "Iteration 93, loss = 0.29040954\n",
      "Iteration 94, loss = 0.28729935\n",
      "Iteration 95, loss = 0.28462165\n",
      "Iteration 96, loss = 0.28098822\n",
      "Iteration 97, loss = 0.27933566\n",
      "Iteration 98, loss = 0.27822506\n",
      "Iteration 99, loss = 0.27731819\n",
      "Iteration 100, loss = 0.27215183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70697940\n",
      "Iteration 2, loss = 0.82820168\n",
      "Iteration 3, loss = 0.53656049\n",
      "Iteration 4, loss = 0.49466732\n",
      "Iteration 5, loss = 0.45872428\n",
      "Iteration 6, loss = 0.44552813\n",
      "Iteration 7, loss = 0.44310686\n",
      "Iteration 8, loss = 0.43786307\n",
      "Iteration 9, loss = 0.42940552\n",
      "Iteration 10, loss = 0.42228813\n",
      "Iteration 11, loss = 0.41914407\n",
      "Iteration 12, loss = 0.42818552\n",
      "Iteration 13, loss = 0.42162693\n",
      "Iteration 14, loss = 0.42853006\n",
      "Iteration 15, loss = 0.42992210\n",
      "Iteration 16, loss = 0.42426246\n",
      "Iteration 17, loss = 0.41959795\n",
      "Iteration 18, loss = 0.41261960\n",
      "Iteration 19, loss = 0.41277626\n",
      "Iteration 20, loss = 0.40220774\n",
      "Iteration 21, loss = 0.39392826\n",
      "Iteration 22, loss = 0.39351640\n",
      "Iteration 23, loss = 0.39312679\n",
      "Iteration 24, loss = 0.39067089\n",
      "Iteration 25, loss = 0.39209174\n",
      "Iteration 26, loss = 0.39562149\n",
      "Iteration 27, loss = 0.38920803\n",
      "Iteration 28, loss = 0.38283976\n",
      "Iteration 29, loss = 0.38351563\n",
      "Iteration 30, loss = 0.38397892\n",
      "Iteration 31, loss = 0.37601247\n",
      "Iteration 32, loss = 0.37240814\n",
      "Iteration 33, loss = 0.37126291\n",
      "Iteration 34, loss = 0.37109512\n",
      "Iteration 35, loss = 0.37267745\n",
      "Iteration 36, loss = 0.37084777\n",
      "Iteration 37, loss = 0.36526404\n",
      "Iteration 38, loss = 0.37007526\n",
      "Iteration 39, loss = 0.36531073\n",
      "Iteration 40, loss = 0.37053094\n",
      "Iteration 41, loss = 0.37392993\n",
      "Iteration 42, loss = 0.37836137\n",
      "Iteration 43, loss = 0.37561766\n",
      "Iteration 44, loss = 0.37343653\n",
      "Iteration 45, loss = 0.37990446\n",
      "Iteration 46, loss = 0.37937859\n",
      "Iteration 47, loss = 0.37434884\n",
      "Iteration 48, loss = 0.37387012\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.92425535\n",
      "Iteration 2, loss = 0.47359146\n",
      "Iteration 3, loss = 0.44001383\n",
      "Iteration 4, loss = 0.40571023\n",
      "Iteration 5, loss = 0.38499097\n",
      "Iteration 6, loss = 0.37415285\n",
      "Iteration 7, loss = 0.37490799\n",
      "Iteration 8, loss = 0.38266488\n",
      "Iteration 9, loss = 0.37435259\n",
      "Iteration 10, loss = 0.39412234\n",
      "Iteration 11, loss = 0.38744837\n",
      "Iteration 12, loss = 0.39612214\n",
      "Iteration 13, loss = 0.39320400\n",
      "Iteration 14, loss = 0.38131664\n",
      "Iteration 15, loss = 0.37501984\n",
      "Iteration 16, loss = 0.37316107\n",
      "Iteration 17, loss = 0.37205974\n",
      "Iteration 18, loss = 0.37057622\n",
      "Iteration 19, loss = 0.36684738\n",
      "Iteration 20, loss = 0.36914900\n",
      "Iteration 21, loss = 0.36497361\n",
      "Iteration 22, loss = 0.36189914\n",
      "Iteration 23, loss = 0.35941073\n",
      "Iteration 24, loss = 0.35671502\n",
      "Iteration 25, loss = 0.35792889\n",
      "Iteration 26, loss = 0.35177717\n",
      "Iteration 27, loss = 0.35232540\n",
      "Iteration 28, loss = 0.34902892\n",
      "Iteration 29, loss = 0.34606424\n",
      "Iteration 30, loss = 0.33866058\n",
      "Iteration 31, loss = 0.33448318\n",
      "Iteration 32, loss = 0.33872550\n",
      "Iteration 33, loss = 0.33296220\n",
      "Iteration 34, loss = 0.32897295\n",
      "Iteration 35, loss = 0.32551543\n",
      "Iteration 36, loss = 0.32371225\n",
      "Iteration 37, loss = 0.32453665\n",
      "Iteration 38, loss = 0.31925573\n",
      "Iteration 39, loss = 0.31671340\n",
      "Iteration 40, loss = 0.31742994\n",
      "Iteration 41, loss = 0.31466453\n",
      "Iteration 42, loss = 0.31638524\n",
      "Iteration 43, loss = 0.31450158\n",
      "Iteration 44, loss = 0.30993091\n",
      "Iteration 45, loss = 0.30794970\n",
      "Iteration 46, loss = 0.30754455\n",
      "Iteration 47, loss = 0.30888181\n",
      "Iteration 48, loss = 0.30710803\n",
      "Iteration 49, loss = 0.30351946\n",
      "Iteration 50, loss = 0.30288562\n",
      "Iteration 51, loss = 0.30026236\n",
      "Iteration 52, loss = 0.29833833\n",
      "Iteration 53, loss = 0.29754053\n",
      "Iteration 54, loss = 0.29394860\n",
      "Iteration 55, loss = 0.29826806\n",
      "Iteration 56, loss = 0.29464948\n",
      "Iteration 57, loss = 0.29570563\n",
      "Iteration 58, loss = 0.29402504\n",
      "Iteration 59, loss = 0.29746903\n",
      "Iteration 60, loss = 0.30110561\n",
      "Iteration 61, loss = 0.29611787\n",
      "Iteration 62, loss = 0.29639863\n",
      "Iteration 63, loss = 0.29624968\n",
      "Iteration 64, loss = 0.29819974\n",
      "Iteration 65, loss = 0.29872833\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79562292\n",
      "Iteration 2, loss = 0.49131945\n",
      "Iteration 3, loss = 0.46458329\n",
      "Iteration 4, loss = 0.44137610\n",
      "Iteration 5, loss = 0.42929572\n",
      "Iteration 6, loss = 0.40534427\n",
      "Iteration 7, loss = 0.40909334\n",
      "Iteration 8, loss = 0.39409687\n",
      "Iteration 9, loss = 0.38716075\n",
      "Iteration 10, loss = 0.39970529\n",
      "Iteration 11, loss = 0.39750391\n",
      "Iteration 12, loss = 0.39672647\n",
      "Iteration 13, loss = 0.39574463\n",
      "Iteration 14, loss = 0.39183345\n",
      "Iteration 15, loss = 0.38706993\n",
      "Iteration 16, loss = 0.37925989\n",
      "Iteration 17, loss = 0.39107252\n",
      "Iteration 18, loss = 0.38885590\n",
      "Iteration 19, loss = 0.39236028\n",
      "Iteration 20, loss = 0.38380975\n",
      "Iteration 21, loss = 0.37423700\n",
      "Iteration 22, loss = 0.37736369\n",
      "Iteration 23, loss = 0.37561631\n",
      "Iteration 24, loss = 0.37374957\n",
      "Iteration 25, loss = 0.36535669\n",
      "Iteration 26, loss = 0.35911554\n",
      "Iteration 27, loss = 0.35921267\n",
      "Iteration 28, loss = 0.36571169\n",
      "Iteration 29, loss = 0.36185673\n",
      "Iteration 30, loss = 0.36437491\n",
      "Iteration 31, loss = 0.36194097\n",
      "Iteration 32, loss = 0.36378373\n",
      "Iteration 33, loss = 0.35999842\n",
      "Iteration 34, loss = 0.35314123\n",
      "Iteration 35, loss = 0.35251066\n",
      "Iteration 36, loss = 0.35007438\n",
      "Iteration 37, loss = 0.34690445\n",
      "Iteration 38, loss = 0.34737353\n",
      "Iteration 39, loss = 0.34898055\n",
      "Iteration 40, loss = 0.34941153\n",
      "Iteration 41, loss = 0.34393532\n",
      "Iteration 42, loss = 0.34069959\n",
      "Iteration 43, loss = 0.34194951\n",
      "Iteration 44, loss = 0.33597942\n",
      "Iteration 45, loss = 0.33723710\n",
      "Iteration 46, loss = 0.33539682\n",
      "Iteration 47, loss = 0.32690853\n",
      "Iteration 48, loss = 0.32687715\n",
      "Iteration 49, loss = 0.32451039\n",
      "Iteration 50, loss = 0.32191253\n",
      "Iteration 51, loss = 0.32323140\n",
      "Iteration 52, loss = 0.31826198\n",
      "Iteration 53, loss = 0.31438168\n",
      "Iteration 54, loss = 0.31226205\n",
      "Iteration 55, loss = 0.31133221\n",
      "Iteration 56, loss = 0.30965926\n",
      "Iteration 57, loss = 0.30663827\n",
      "Iteration 58, loss = 0.31034847\n",
      "Iteration 59, loss = 0.31683642\n",
      "Iteration 60, loss = 0.32097869\n",
      "Iteration 61, loss = 0.31956494\n",
      "Iteration 62, loss = 0.31540816\n",
      "Iteration 63, loss = 0.31488775\n",
      "Iteration 64, loss = 0.31415134\n",
      "Iteration 65, loss = 0.31226998\n",
      "Iteration 66, loss = 0.30504154\n",
      "Iteration 67, loss = 0.30429694\n",
      "Iteration 68, loss = 0.30521401\n",
      "Iteration 69, loss = 0.30166160\n",
      "Iteration 70, loss = 0.29937274\n",
      "Iteration 71, loss = 0.29930104\n",
      "Iteration 72, loss = 0.29326969\n",
      "Iteration 73, loss = 0.28867854\n",
      "Iteration 74, loss = 0.28164361\n",
      "Iteration 75, loss = 0.28293766\n",
      "Iteration 76, loss = 0.29700641\n",
      "Iteration 77, loss = 0.29765648\n",
      "Iteration 78, loss = 0.29095911\n",
      "Iteration 79, loss = 0.28161748\n",
      "Iteration 80, loss = 0.27388610\n",
      "Iteration 81, loss = 0.27591730\n",
      "Iteration 82, loss = 0.27388361\n",
      "Iteration 83, loss = 0.26952728\n",
      "Iteration 84, loss = 0.27205522\n",
      "Iteration 85, loss = 0.26803321\n",
      "Iteration 86, loss = 0.26775812\n",
      "Iteration 87, loss = 0.27000276\n",
      "Iteration 88, loss = 0.27174265\n",
      "Iteration 89, loss = 0.27492266\n",
      "Iteration 90, loss = 0.27331915\n",
      "Iteration 91, loss = 0.27425945\n",
      "Iteration 92, loss = 0.27596455\n",
      "Iteration 93, loss = 0.27151707\n",
      "Iteration 94, loss = 0.26709069\n",
      "Iteration 95, loss = 0.26210303\n",
      "Iteration 96, loss = 0.25902487\n",
      "Iteration 97, loss = 0.26257113\n",
      "Iteration 98, loss = 0.25625135\n",
      "Iteration 99, loss = 0.25972513\n",
      "Iteration 100, loss = 0.25325370\n",
      "Iteration 1, loss = 0.73729240\n",
      "Iteration 2, loss = 0.72118638\n",
      "Iteration 3, loss = 0.71543101\n",
      "Iteration 4, loss = 0.71011869\n",
      "Iteration 5, loss = 0.70524516\n",
      "Iteration 6, loss = 0.70081061\n",
      "Iteration 7, loss = 0.69681368\n",
      "Iteration 8, loss = 0.69325016\n",
      "Iteration 9, loss = 0.69011245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.68738937\n",
      "Iteration 11, loss = 0.68506592\n",
      "Iteration 12, loss = 0.68312327\n",
      "Iteration 13, loss = 0.68153878\n",
      "Iteration 14, loss = 0.68028618\n",
      "Iteration 15, loss = 0.67933587\n",
      "Iteration 16, loss = 0.67865542\n",
      "Iteration 17, loss = 0.67821018\n",
      "Iteration 18, loss = 0.67796412\n",
      "Iteration 19, loss = 0.67788065\n",
      "Iteration 20, loss = 0.67792369\n",
      "Iteration 21, loss = 0.67805864\n",
      "Iteration 22, loss = 0.67825335\n",
      "Iteration 23, loss = 0.67847900\n",
      "Iteration 24, loss = 0.67871081\n",
      "Iteration 25, loss = 0.67892847\n",
      "Iteration 26, loss = 0.67911640\n",
      "Iteration 27, loss = 0.67926374\n",
      "Iteration 28, loss = 0.67936406\n",
      "Iteration 29, loss = 0.67941500\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73600118\n",
      "Iteration 2, loss = 0.61132805\n",
      "Iteration 3, loss = 0.61637420\n",
      "Iteration 4, loss = 0.64791779\n",
      "Iteration 5, loss = 0.64599225\n",
      "Iteration 6, loss = 0.63871383\n",
      "Iteration 7, loss = 0.63925387\n",
      "Iteration 8, loss = 0.63527625\n",
      "Iteration 9, loss = 0.63138649\n",
      "Iteration 10, loss = 0.62759652\n",
      "Iteration 11, loss = 0.62362595\n",
      "Iteration 12, loss = 0.61667103\n",
      "Iteration 13, loss = 0.61181161\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67767012\n",
      "Iteration 2, loss = 0.67035375\n",
      "Iteration 3, loss = 0.66134418\n",
      "Iteration 4, loss = 0.65549717\n",
      "Iteration 5, loss = 0.65068055\n",
      "Iteration 6, loss = 0.64758661\n",
      "Iteration 7, loss = 0.64457126\n",
      "Iteration 8, loss = 0.64161286\n",
      "Iteration 9, loss = 0.63870250\n",
      "Iteration 10, loss = 0.63582899\n",
      "Iteration 11, loss = 0.62904518\n",
      "Iteration 12, loss = 0.62643195\n",
      "Iteration 13, loss = 0.62308007\n",
      "Iteration 14, loss = 0.61690651\n",
      "Iteration 15, loss = 0.61806104\n",
      "Iteration 16, loss = 0.63732065\n",
      "Iteration 17, loss = 0.64493140\n",
      "Iteration 18, loss = 0.64847642\n",
      "Iteration 19, loss = 0.64741984\n",
      "Iteration 20, loss = 0.64636611\n",
      "Iteration 21, loss = 0.64531977\n",
      "Iteration 22, loss = 0.64430369\n",
      "Iteration 23, loss = 0.64327156\n",
      "Iteration 24, loss = 0.64228286\n",
      "Iteration 25, loss = 0.64132654\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74925780\n",
      "Iteration 2, loss = 0.68710546\n",
      "Iteration 3, loss = 0.67482957\n",
      "Iteration 4, loss = 0.67067597\n",
      "Iteration 5, loss = 0.66678637\n",
      "Iteration 6, loss = 0.66342132\n",
      "Iteration 7, loss = 0.65975475\n",
      "Iteration 8, loss = 0.65500171\n",
      "Iteration 9, loss = 0.65048954\n",
      "Iteration 10, loss = 0.64658746\n",
      "Iteration 11, loss = 0.64442836\n",
      "Iteration 12, loss = 0.63911671\n",
      "Iteration 13, loss = 0.63560943\n",
      "Iteration 14, loss = 0.63227119\n",
      "Iteration 15, loss = 0.62923872\n",
      "Iteration 16, loss = 0.62612341\n",
      "Iteration 17, loss = 0.62330966\n",
      "Iteration 18, loss = 0.62065900\n",
      "Iteration 19, loss = 0.61815976\n",
      "Iteration 20, loss = 0.61579807\n",
      "Iteration 21, loss = 0.61355876\n",
      "Iteration 22, loss = 0.61142616\n",
      "Iteration 23, loss = 0.60938480\n",
      "Iteration 24, loss = 0.60742009\n",
      "Iteration 25, loss = 0.60551871\n",
      "Iteration 26, loss = 0.60366903\n",
      "Iteration 27, loss = 0.60186119\n",
      "Iteration 28, loss = 0.60008723\n",
      "Iteration 29, loss = 0.59834096\n",
      "Iteration 30, loss = 0.59661789\n",
      "Iteration 31, loss = 0.59491498\n",
      "Iteration 32, loss = 0.59323049\n",
      "Iteration 33, loss = 0.59156374\n",
      "Iteration 34, loss = 0.58991488\n",
      "Iteration 35, loss = 0.58828472\n",
      "Iteration 36, loss = 0.58667454\n",
      "Iteration 37, loss = 0.58508594\n",
      "Iteration 38, loss = 0.58352069\n",
      "Iteration 39, loss = 0.58198064\n",
      "Iteration 40, loss = 0.58046762\n",
      "Iteration 41, loss = 0.57898333\n",
      "Iteration 42, loss = 0.57752935\n",
      "Iteration 43, loss = 0.57610705\n",
      "Iteration 44, loss = 0.57471757\n",
      "Iteration 45, loss = 0.57336180\n",
      "Iteration 46, loss = 0.57204041\n",
      "Iteration 47, loss = 0.57075378\n",
      "Iteration 48, loss = 0.56950208\n",
      "Iteration 49, loss = 0.56828523\n",
      "Iteration 50, loss = 0.56710297\n",
      "Iteration 51, loss = 0.56595484\n",
      "Iteration 52, loss = 0.56484021\n",
      "Iteration 53, loss = 0.56375833\n",
      "Iteration 54, loss = 0.56270836\n",
      "Iteration 55, loss = 0.56168935\n",
      "Iteration 56, loss = 0.56070031\n",
      "Iteration 57, loss = 0.55974023\n",
      "Iteration 58, loss = 0.55880807\n",
      "Iteration 59, loss = 0.55790280\n",
      "Iteration 60, loss = 0.55702344\n",
      "Iteration 61, loss = 0.55616900\n",
      "Iteration 62, loss = 0.55533859\n",
      "Iteration 63, loss = 0.55453134\n",
      "Iteration 64, loss = 0.55374642\n",
      "Iteration 65, loss = 0.55298309\n",
      "Iteration 66, loss = 0.55224066\n",
      "Iteration 67, loss = 0.55151846\n",
      "Iteration 68, loss = 0.55081591\n",
      "Iteration 69, loss = 0.55013243\n",
      "Iteration 70, loss = 0.54946752\n",
      "Iteration 71, loss = 0.54882067\n",
      "Iteration 72, loss = 0.54819142\n",
      "Iteration 73, loss = 0.54757931\n",
      "Iteration 74, loss = 0.54698390\n",
      "Iteration 75, loss = 0.54640477\n",
      "Iteration 76, loss = 0.54584147\n",
      "Iteration 77, loss = 0.54529360\n",
      "Iteration 78, loss = 0.54476073\n",
      "Iteration 79, loss = 0.54424242\n",
      "Iteration 80, loss = 0.54373827\n",
      "Iteration 81, loss = 0.54324784\n",
      "Iteration 82, loss = 0.54277072\n",
      "Iteration 83, loss = 0.54230649\n",
      "Iteration 84, loss = 0.54185473\n",
      "Iteration 85, loss = 0.54141505\n",
      "Iteration 86, loss = 0.54098705\n",
      "Iteration 87, loss = 0.54057033\n",
      "Iteration 88, loss = 0.54016454\n",
      "Iteration 89, loss = 0.53976930\n",
      "Iteration 90, loss = 0.53938428\n",
      "Iteration 91, loss = 0.53900913\n",
      "Iteration 92, loss = 0.53864355\n",
      "Iteration 93, loss = 0.53828723\n",
      "Iteration 94, loss = 0.53793987\n",
      "Iteration 95, loss = 0.53760120\n",
      "Iteration 96, loss = 0.53727096\n",
      "Iteration 97, loss = 0.53694889\n",
      "Iteration 98, loss = 0.53663475\n",
      "Iteration 99, loss = 0.53632831\n",
      "Iteration 100, loss = 0.53602933\n",
      "Iteration 1, loss = 0.69313259\n",
      "Iteration 2, loss = 0.68545572\n",
      "Iteration 3, loss = 0.67829805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.67584819\n",
      "Iteration 5, loss = 0.67114309\n",
      "Iteration 6, loss = 0.66668097\n",
      "Iteration 7, loss = 0.66278612\n",
      "Iteration 8, loss = 0.66009201\n",
      "Iteration 9, loss = 0.65540569\n",
      "Iteration 10, loss = 0.65144663\n",
      "Iteration 11, loss = 0.64782860\n",
      "Iteration 12, loss = 0.64413416\n",
      "Iteration 13, loss = 0.64049038\n",
      "Iteration 14, loss = 0.63688561\n",
      "Iteration 15, loss = 0.63333583\n",
      "Iteration 16, loss = 0.62985368\n",
      "Iteration 17, loss = 0.62644782\n",
      "Iteration 18, loss = 0.62310652\n",
      "Iteration 19, loss = 0.61649215\n",
      "Iteration 20, loss = 0.61301175\n",
      "Iteration 21, loss = 0.60952592\n",
      "Iteration 22, loss = 0.60522384\n",
      "Iteration 23, loss = 0.59913771\n",
      "Iteration 24, loss = 0.59033276\n",
      "Iteration 25, loss = 0.59596103\n",
      "Iteration 26, loss = 0.58307778\n",
      "Iteration 27, loss = 0.57965083\n",
      "Iteration 28, loss = 0.56941505\n",
      "Iteration 29, loss = 0.56605915\n",
      "Iteration 30, loss = 0.56278934\n",
      "Iteration 31, loss = 0.55961566\n",
      "Iteration 32, loss = 0.55654644\n",
      "Iteration 33, loss = 0.55358839\n",
      "Iteration 34, loss = 0.55074664\n",
      "Iteration 35, loss = 0.54802478\n",
      "Iteration 36, loss = 0.54542497\n",
      "Iteration 37, loss = 0.54294802\n",
      "Iteration 38, loss = 0.54059355\n",
      "Iteration 39, loss = 0.53836006\n",
      "Iteration 40, loss = 0.53624513\n",
      "Iteration 41, loss = 0.53424558\n",
      "Iteration 42, loss = 0.53235764\n",
      "Iteration 43, loss = 0.53057711\n",
      "Iteration 44, loss = 0.52889951\n",
      "Iteration 45, loss = 0.52732023\n",
      "Iteration 46, loss = 0.52583468\n",
      "Iteration 47, loss = 0.52443832\n",
      "Iteration 48, loss = 0.52312682\n",
      "Iteration 49, loss = 0.52189601\n",
      "Iteration 50, loss = 0.52074199\n",
      "Iteration 51, loss = 0.51966107\n",
      "Iteration 52, loss = 0.51864981\n",
      "Iteration 53, loss = 0.51770496\n",
      "Iteration 54, loss = 0.51682347\n",
      "Iteration 55, loss = 0.51600243\n",
      "Iteration 56, loss = 0.51523905\n",
      "Iteration 57, loss = 0.51453065\n",
      "Iteration 58, loss = 0.51387458\n",
      "Iteration 59, loss = 0.51326826\n",
      "Iteration 60, loss = 0.51270914\n",
      "Iteration 61, loss = 0.51219470\n",
      "Iteration 62, loss = 0.51172240\n",
      "Iteration 63, loss = 0.51128977\n",
      "Iteration 64, loss = 0.51089434\n",
      "Iteration 65, loss = 0.51053367\n",
      "Iteration 66, loss = 0.51020538\n",
      "Iteration 67, loss = 0.50990715\n",
      "Iteration 68, loss = 0.50963673\n",
      "Iteration 69, loss = 0.50939198\n",
      "Iteration 70, loss = 0.50917082\n",
      "Iteration 71, loss = 0.50897131\n",
      "Iteration 72, loss = 0.50879157\n",
      "Iteration 73, loss = 0.50862982\n",
      "Iteration 74, loss = 0.50848414\n",
      "Iteration 75, loss = 0.50835137\n",
      "Iteration 76, loss = 0.50820610\n",
      "Iteration 77, loss = 0.49331071\n",
      "Iteration 78, loss = 0.50094349\n",
      "Iteration 79, loss = 0.51144453\n",
      "Iteration 80, loss = 0.50890636\n",
      "Iteration 81, loss = 0.50839835\n",
      "Iteration 82, loss = 0.49256278\n",
      "Iteration 83, loss = 0.49125271\n",
      "Iteration 84, loss = 0.50205471\n",
      "Iteration 85, loss = 0.51384307\n",
      "Iteration 86, loss = 0.57770031\n",
      "Iteration 87, loss = 0.59368591\n",
      "Iteration 88, loss = 0.58105281\n",
      "Iteration 89, loss = 0.54742837\n",
      "Iteration 90, loss = 0.55124744\n",
      "Iteration 91, loss = 0.52583896\n",
      "Iteration 92, loss = 0.52315649\n",
      "Iteration 93, loss = 0.45252527\n",
      "Iteration 94, loss = 0.45889636\n",
      "Iteration 95, loss = 0.46846491\n",
      "Iteration 96, loss = 0.46852926\n",
      "Iteration 97, loss = 0.45367707\n",
      "Iteration 98, loss = 0.46363201\n",
      "Iteration 99, loss = 0.45544093\n",
      "Iteration 100, loss = 0.45404611\n",
      "Iteration 1, loss = 0.70710598\n",
      "Iteration 2, loss = 0.54729227\n",
      "Iteration 3, loss = 0.54180970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.51525490\n",
      "Iteration 5, loss = 0.51827962\n",
      "Iteration 6, loss = 0.47495775\n",
      "Iteration 7, loss = 0.48291046\n",
      "Iteration 8, loss = 0.50552328\n",
      "Iteration 9, loss = 0.50099898\n",
      "Iteration 10, loss = 0.47858016\n",
      "Iteration 11, loss = 0.46872735\n",
      "Iteration 12, loss = 0.44431180\n",
      "Iteration 13, loss = 0.44112435\n",
      "Iteration 14, loss = 0.41475696\n",
      "Iteration 15, loss = 0.42643846\n",
      "Iteration 16, loss = 0.43027773\n",
      "Iteration 17, loss = 0.42410873\n",
      "Iteration 18, loss = 0.41746140\n",
      "Iteration 19, loss = 0.41191730\n",
      "Iteration 20, loss = 0.40093516\n",
      "Iteration 21, loss = 0.39397865\n",
      "Iteration 22, loss = 0.39100208\n",
      "Iteration 23, loss = 0.39304255\n",
      "Iteration 24, loss = 0.38617042\n",
      "Iteration 25, loss = 0.39008589\n",
      "Iteration 26, loss = 0.38506525\n",
      "Iteration 27, loss = 0.37430329\n",
      "Iteration 28, loss = 0.36940699\n",
      "Iteration 29, loss = 0.36101054\n",
      "Iteration 30, loss = 0.35807930\n",
      "Iteration 31, loss = 0.37207065\n",
      "Iteration 32, loss = 0.38346307\n",
      "Iteration 33, loss = 0.35945668\n",
      "Iteration 34, loss = 0.36102317\n",
      "Iteration 35, loss = 0.35515797\n",
      "Iteration 36, loss = 0.36612621\n",
      "Iteration 37, loss = 0.37625543\n",
      "Iteration 38, loss = 0.37351507\n",
      "Iteration 39, loss = 0.36768539\n",
      "Iteration 40, loss = 0.37258884\n",
      "Iteration 41, loss = 0.36643283\n",
      "Iteration 42, loss = 0.36796775\n",
      "Iteration 43, loss = 0.36460903\n",
      "Iteration 44, loss = 0.36808842\n",
      "Iteration 45, loss = 0.34424713\n",
      "Iteration 46, loss = 0.35069960\n",
      "Iteration 47, loss = 0.35724003\n",
      "Iteration 48, loss = 0.36455356\n",
      "Iteration 49, loss = 0.36587804\n",
      "Iteration 50, loss = 0.35840376\n",
      "Iteration 51, loss = 0.35923833\n",
      "Iteration 52, loss = 0.35513965\n",
      "Iteration 53, loss = 0.35282676\n",
      "Iteration 54, loss = 0.34623922\n",
      "Iteration 55, loss = 0.34253220\n",
      "Iteration 56, loss = 0.35944776\n",
      "Iteration 57, loss = 0.35915156\n",
      "Iteration 58, loss = 0.34171297\n",
      "Iteration 59, loss = 0.33816139\n",
      "Iteration 60, loss = 0.33487228\n",
      "Iteration 61, loss = 0.33399730\n",
      "Iteration 62, loss = 0.33314403\n",
      "Iteration 63, loss = 0.34344034\n",
      "Iteration 64, loss = 0.33246716\n",
      "Iteration 65, loss = 0.33087710\n",
      "Iteration 66, loss = 0.33104318\n",
      "Iteration 67, loss = 0.32529806\n",
      "Iteration 68, loss = 0.33222626\n",
      "Iteration 69, loss = 0.33157424\n",
      "Iteration 70, loss = 0.33156892\n",
      "Iteration 71, loss = 0.33472587\n",
      "Iteration 72, loss = 0.32620850\n",
      "Iteration 73, loss = 0.31793536\n",
      "Iteration 74, loss = 0.31707799\n",
      "Iteration 75, loss = 0.31622008\n",
      "Iteration 76, loss = 0.31576121\n",
      "Iteration 77, loss = 0.31813167\n",
      "Iteration 78, loss = 0.31401001\n",
      "Iteration 79, loss = 0.31345534\n",
      "Iteration 80, loss = 0.31298245\n",
      "Iteration 81, loss = 0.31163976\n",
      "Iteration 82, loss = 0.30776468\n",
      "Iteration 83, loss = 0.30726343\n",
      "Iteration 84, loss = 0.30613551\n",
      "Iteration 85, loss = 0.30420376\n",
      "Iteration 86, loss = 0.30325717\n",
      "Iteration 87, loss = 0.30673380\n",
      "Iteration 88, loss = 0.30063647\n",
      "Iteration 89, loss = 0.29959770\n",
      "Iteration 90, loss = 0.30311623\n",
      "Iteration 91, loss = 0.30232983\n",
      "Iteration 92, loss = 0.30170917\n",
      "Iteration 93, loss = 0.30121691\n",
      "Iteration 94, loss = 0.30079933\n",
      "Iteration 95, loss = 0.30039973\n",
      "Iteration 96, loss = 0.29997067\n",
      "Iteration 97, loss = 0.29948331\n",
      "Iteration 98, loss = 0.30038538\n",
      "Iteration 99, loss = 0.29429984\n",
      "Iteration 100, loss = 0.29360221\n",
      "Iteration 1, loss = 0.70432004\n",
      "Iteration 2, loss = 0.61670546\n",
      "Iteration 3, loss = 0.54635797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.51334529\n",
      "Iteration 5, loss = 0.50028573\n",
      "Iteration 6, loss = 0.48018377\n",
      "Iteration 7, loss = 0.48519571\n",
      "Iteration 8, loss = 0.48708560\n",
      "Iteration 9, loss = 0.46846828\n",
      "Iteration 10, loss = 0.46113508\n",
      "Iteration 11, loss = 0.45327983\n",
      "Iteration 12, loss = 0.44248755\n",
      "Iteration 13, loss = 0.42304352\n",
      "Iteration 14, loss = 0.41781977\n",
      "Iteration 15, loss = 0.41405209\n",
      "Iteration 16, loss = 0.40627093\n",
      "Iteration 17, loss = 0.40699994\n",
      "Iteration 18, loss = 0.39564170\n",
      "Iteration 19, loss = 0.40946558\n",
      "Iteration 20, loss = 0.41682224\n",
      "Iteration 21, loss = 0.41376659\n",
      "Iteration 22, loss = 0.40808578\n",
      "Iteration 23, loss = 0.40693863\n",
      "Iteration 24, loss = 0.39539626\n",
      "Iteration 25, loss = 0.40484031\n",
      "Iteration 26, loss = 0.40510297\n",
      "Iteration 27, loss = 0.40234406\n",
      "Iteration 28, loss = 0.39495932\n",
      "Iteration 29, loss = 0.37912148\n",
      "Iteration 30, loss = 0.37654622\n",
      "Iteration 31, loss = 0.36320266\n",
      "Iteration 32, loss = 0.37308162\n",
      "Iteration 33, loss = 0.38935882\n",
      "Iteration 34, loss = 0.38874486\n",
      "Iteration 35, loss = 0.36960661\n",
      "Iteration 36, loss = 0.37113552\n",
      "Iteration 37, loss = 0.37419947\n",
      "Iteration 38, loss = 0.37949875\n",
      "Iteration 39, loss = 0.39508636\n",
      "Iteration 40, loss = 0.38432281\n",
      "Iteration 41, loss = 0.38283432\n",
      "Iteration 42, loss = 0.37565763\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76650859\n",
      "Iteration 2, loss = 0.57009465\n",
      "Iteration 3, loss = 0.55794241\n",
      "Iteration 4, loss = 0.54608162\n",
      "Iteration 5, loss = 0.53466360\n",
      "Iteration 6, loss = 0.52532424\n",
      "Iteration 7, loss = 0.51628126\n",
      "Iteration 8, loss = 0.50822219\n",
      "Iteration 9, loss = 0.50277620\n",
      "Iteration 10, loss = 0.50097190\n",
      "Iteration 11, loss = 0.48666491\n",
      "Iteration 12, loss = 0.47177999\n",
      "Iteration 13, loss = 0.45852236\n",
      "Iteration 14, loss = 0.45593829\n",
      "Iteration 15, loss = 0.45055128\n",
      "Iteration 16, loss = 0.45331759\n",
      "Iteration 17, loss = 0.45160739\n",
      "Iteration 18, loss = 0.44801504\n",
      "Iteration 19, loss = 0.43910770\n",
      "Iteration 20, loss = 0.43659679\n",
      "Iteration 21, loss = 0.44901518\n",
      "Iteration 22, loss = 0.45059296\n",
      "Iteration 23, loss = 0.43595488\n",
      "Iteration 24, loss = 0.44614692\n",
      "Iteration 25, loss = 0.45409889\n",
      "Iteration 26, loss = 0.45842828\n",
      "Iteration 27, loss = 0.45546205\n",
      "Iteration 28, loss = 0.44337259\n",
      "Iteration 29, loss = 0.42199534\n",
      "Iteration 30, loss = 0.41795952\n",
      "Iteration 31, loss = 0.39748656\n",
      "Iteration 32, loss = 0.39244758\n",
      "Iteration 33, loss = 0.39627300\n",
      "Iteration 34, loss = 0.39862420\n",
      "Iteration 35, loss = 0.41291487\n",
      "Iteration 36, loss = 0.42799605\n",
      "Iteration 37, loss = 0.42197361\n",
      "Iteration 38, loss = 0.41216724\n",
      "Iteration 39, loss = 0.40861801\n",
      "Iteration 40, loss = 0.42621077\n",
      "Iteration 41, loss = 0.42450570\n",
      "Iteration 42, loss = 0.41263573\n",
      "Iteration 43, loss = 0.40880181\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71912380\n",
      "Iteration 2, loss = 0.60075848\n",
      "Iteration 3, loss = 0.56766789\n",
      "Iteration 4, loss = 0.53933697\n",
      "Iteration 5, loss = 0.54333057\n",
      "Iteration 6, loss = 0.52256806\n",
      "Iteration 7, loss = 0.50655970\n",
      "Iteration 8, loss = 0.49854509\n",
      "Iteration 9, loss = 0.49319637\n",
      "Iteration 10, loss = 0.48352395\n",
      "Iteration 11, loss = 0.47685444\n",
      "Iteration 12, loss = 0.46968186\n",
      "Iteration 13, loss = 0.46608655\n",
      "Iteration 14, loss = 0.46342401\n",
      "Iteration 15, loss = 0.45236407\n",
      "Iteration 16, loss = 0.44001099\n",
      "Iteration 17, loss = 0.43629773\n",
      "Iteration 18, loss = 0.43155859\n",
      "Iteration 19, loss = 0.42646888\n",
      "Iteration 20, loss = 0.42827745\n",
      "Iteration 21, loss = 0.42090222\n",
      "Iteration 22, loss = 0.41804422\n",
      "Iteration 23, loss = 0.41361229\n",
      "Iteration 24, loss = 0.40330273\n",
      "Iteration 25, loss = 0.39921133\n",
      "Iteration 26, loss = 0.41337775\n",
      "Iteration 27, loss = 0.41160559\n",
      "Iteration 28, loss = 0.42228952\n",
      "Iteration 29, loss = 0.42118178\n",
      "Iteration 30, loss = 0.42178624\n",
      "Iteration 31, loss = 0.41321282\n",
      "Iteration 32, loss = 0.40866663\n",
      "Iteration 33, loss = 0.40024631\n",
      "Iteration 34, loss = 0.38976017\n",
      "Iteration 35, loss = 0.39199029\n",
      "Iteration 36, loss = 0.39383457\n",
      "Iteration 37, loss = 0.39315264\n",
      "Iteration 38, loss = 0.37942281\n",
      "Iteration 39, loss = 0.38794534\n",
      "Iteration 40, loss = 0.38541576\n",
      "Iteration 41, loss = 0.38505643\n",
      "Iteration 42, loss = 0.38991830\n",
      "Iteration 43, loss = 0.39313266\n",
      "Iteration 44, loss = 0.39505059\n",
      "Iteration 45, loss = 0.39150056\n",
      "Iteration 46, loss = 0.38422604\n",
      "Iteration 47, loss = 0.38862180\n",
      "Iteration 48, loss = 0.39834857\n",
      "Iteration 49, loss = 0.40531739\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75019064\n",
      "Iteration 2, loss = 0.51219356\n",
      "Iteration 3, loss = 0.49003577\n",
      "Iteration 4, loss = 0.49265501\n",
      "Iteration 5, loss = 0.49101251\n",
      "Iteration 6, loss = 0.47748612\n",
      "Iteration 7, loss = 0.46003917\n",
      "Iteration 8, loss = 0.45259737\n",
      "Iteration 9, loss = 0.44875356\n",
      "Iteration 10, loss = 0.45236280\n",
      "Iteration 11, loss = 0.44230392\n",
      "Iteration 12, loss = 0.43404022\n",
      "Iteration 13, loss = 0.42199166\n",
      "Iteration 14, loss = 0.41622911\n",
      "Iteration 15, loss = 0.41717085\n",
      "Iteration 16, loss = 0.41474446\n",
      "Iteration 17, loss = 0.41188403\n",
      "Iteration 18, loss = 0.40833661\n",
      "Iteration 19, loss = 0.40450591\n",
      "Iteration 20, loss = 0.40044227\n",
      "Iteration 21, loss = 0.40479735\n",
      "Iteration 22, loss = 0.39954642\n",
      "Iteration 23, loss = 0.39536602\n",
      "Iteration 24, loss = 0.39261519\n",
      "Iteration 25, loss = 0.39290824\n",
      "Iteration 26, loss = 0.39173366\n",
      "Iteration 27, loss = 0.38428031\n",
      "Iteration 28, loss = 0.38432210\n",
      "Iteration 29, loss = 0.38518481\n",
      "Iteration 30, loss = 0.38623072\n",
      "Iteration 31, loss = 0.38663160\n",
      "Iteration 32, loss = 0.38429424\n",
      "Iteration 33, loss = 0.38318526\n",
      "Iteration 34, loss = 0.38180851\n",
      "Iteration 35, loss = 0.38106031\n",
      "Iteration 36, loss = 0.37913707\n",
      "Iteration 37, loss = 0.38369970\n",
      "Iteration 38, loss = 0.38820434\n",
      "Iteration 39, loss = 0.38693457\n",
      "Iteration 40, loss = 0.37927787\n",
      "Iteration 41, loss = 0.38656776\n",
      "Iteration 42, loss = 0.37655411\n",
      "Iteration 43, loss = 0.37561763\n",
      "Iteration 44, loss = 0.38078648\n",
      "Iteration 45, loss = 0.38011840\n",
      "Iteration 46, loss = 0.37911776\n",
      "Iteration 47, loss = 0.36733910\n",
      "Iteration 48, loss = 0.36623385\n",
      "Iteration 49, loss = 0.36585506\n",
      "Iteration 50, loss = 0.36512850\n",
      "Iteration 51, loss = 0.36434703\n",
      "Iteration 52, loss = 0.36353221\n",
      "Iteration 53, loss = 0.36341676\n",
      "Iteration 54, loss = 0.36273636\n",
      "Iteration 55, loss = 0.36194551\n",
      "Iteration 56, loss = 0.36120291\n",
      "Iteration 57, loss = 0.36050230\n",
      "Iteration 58, loss = 0.35936892\n",
      "Iteration 59, loss = 0.35884691\n",
      "Iteration 60, loss = 0.35825894\n",
      "Iteration 61, loss = 0.35773676\n",
      "Iteration 62, loss = 0.35721287\n",
      "Iteration 63, loss = 0.35667805\n",
      "Iteration 64, loss = 0.35612739\n",
      "Iteration 65, loss = 0.35552369\n",
      "Iteration 66, loss = 0.35453413\n",
      "Iteration 67, loss = 0.35396505\n",
      "Iteration 68, loss = 0.35341086\n",
      "Iteration 69, loss = 0.35287189\n",
      "Iteration 70, loss = 0.35235295\n",
      "Iteration 71, loss = 0.35185558\n",
      "Iteration 72, loss = 0.35137824\n",
      "Iteration 73, loss = 0.35091702\n",
      "Iteration 74, loss = 0.35046678\n",
      "Iteration 75, loss = 0.35002225\n",
      "Iteration 76, loss = 0.34957907\n",
      "Iteration 77, loss = 0.34913407\n",
      "Iteration 78, loss = 0.34867736\n",
      "Iteration 79, loss = 0.34807510\n",
      "Iteration 80, loss = 0.34763119\n",
      "Iteration 81, loss = 0.34719078\n",
      "Iteration 82, loss = 0.34675672\n",
      "Iteration 83, loss = 0.34633094\n",
      "Iteration 84, loss = 0.34591160\n",
      "Iteration 85, loss = 0.34549573\n",
      "Iteration 86, loss = 0.34508465\n",
      "Iteration 87, loss = 0.34458697\n",
      "Iteration 88, loss = 0.34360950\n",
      "Iteration 89, loss = 0.34256146\n",
      "Iteration 90, loss = 0.34217992\n",
      "Iteration 91, loss = 0.34134636\n",
      "Iteration 92, loss = 0.34096038\n",
      "Iteration 93, loss = 0.34057526\n",
      "Iteration 94, loss = 0.34019300\n",
      "Iteration 95, loss = 0.33981601\n",
      "Iteration 96, loss = 0.33944546\n",
      "Iteration 97, loss = 0.33887695\n",
      "Iteration 98, loss = 0.33851587\n",
      "Iteration 99, loss = 0.33815828\n",
      "Iteration 100, loss = 0.33780266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.80733295\n",
      "Iteration 2, loss = 0.43103720\n",
      "Iteration 3, loss = 0.43407086\n",
      "Iteration 4, loss = 0.39741490\n",
      "Iteration 5, loss = 0.39742787\n",
      "Iteration 6, loss = 0.39731077\n",
      "Iteration 7, loss = 0.37210691\n",
      "Iteration 8, loss = 0.37059703\n",
      "Iteration 9, loss = 0.37343522\n",
      "Iteration 10, loss = 0.37015852\n",
      "Iteration 11, loss = 0.37103179\n",
      "Iteration 12, loss = 0.35855572\n",
      "Iteration 13, loss = 0.35064834\n",
      "Iteration 14, loss = 0.34912843\n",
      "Iteration 15, loss = 0.34537390\n",
      "Iteration 16, loss = 0.33845110\n",
      "Iteration 17, loss = 0.33571377\n",
      "Iteration 18, loss = 0.33023485\n",
      "Iteration 19, loss = 0.32762143\n",
      "Iteration 20, loss = 0.32254725\n",
      "Iteration 21, loss = 0.31768910\n",
      "Iteration 22, loss = 0.31550974\n",
      "Iteration 23, loss = 0.30884795\n",
      "Iteration 24, loss = 0.30844488\n",
      "Iteration 25, loss = 0.29732416\n",
      "Iteration 26, loss = 0.29202858\n",
      "Iteration 27, loss = 0.28605951\n",
      "Iteration 28, loss = 0.27541671\n",
      "Iteration 29, loss = 0.27787740\n",
      "Iteration 30, loss = 0.27511281\n",
      "Iteration 31, loss = 0.27009547\n",
      "Iteration 32, loss = 0.27385608\n",
      "Iteration 33, loss = 0.26580758\n",
      "Iteration 34, loss = 0.26605813\n",
      "Iteration 35, loss = 0.26601039\n",
      "Iteration 36, loss = 0.25340542\n",
      "Iteration 37, loss = 0.24094581\n",
      "Iteration 38, loss = 0.23939399\n",
      "Iteration 39, loss = 0.22975583\n",
      "Iteration 40, loss = 0.22605182\n",
      "Iteration 41, loss = 0.22730506\n",
      "Iteration 42, loss = 0.22353966\n",
      "Iteration 43, loss = 0.21981307\n",
      "Iteration 44, loss = 0.21794623\n",
      "Iteration 45, loss = 0.21479939\n",
      "Iteration 46, loss = 0.21452824\n",
      "Iteration 47, loss = 0.21307222\n",
      "Iteration 48, loss = 0.21389861\n",
      "Iteration 49, loss = 0.21002556\n",
      "Iteration 50, loss = 0.21367494\n",
      "Iteration 51, loss = 0.20656711\n",
      "Iteration 52, loss = 0.19923230\n",
      "Iteration 53, loss = 0.19758491\n",
      "Iteration 54, loss = 0.20032319\n",
      "Iteration 55, loss = 0.19516087\n",
      "Iteration 56, loss = 0.18566710\n",
      "Iteration 57, loss = 0.19146309\n",
      "Iteration 58, loss = 0.18953374\n",
      "Iteration 59, loss = 0.18917352\n",
      "Iteration 60, loss = 0.19221828\n",
      "Iteration 61, loss = 0.19121564\n",
      "Iteration 62, loss = 0.18779135\n",
      "Iteration 63, loss = 0.18747270\n",
      "Iteration 64, loss = 0.18426695\n",
      "Iteration 65, loss = 0.18085573\n",
      "Iteration 66, loss = 0.18461122\n",
      "Iteration 67, loss = 0.17708834\n",
      "Iteration 68, loss = 0.17460121\n",
      "Iteration 69, loss = 0.16800096\n",
      "Iteration 70, loss = 0.16301698\n",
      "Iteration 71, loss = 0.15768063\n",
      "Iteration 72, loss = 0.15227099\n",
      "Iteration 73, loss = 0.16657018\n",
      "Iteration 74, loss = 0.16564514\n",
      "Iteration 75, loss = 0.16461525\n",
      "Iteration 76, loss = 0.16214907\n",
      "Iteration 77, loss = 0.17046697\n",
      "Iteration 78, loss = 0.17039801\n",
      "Iteration 79, loss = 0.16621156\n",
      "Iteration 80, loss = 0.16256857\n",
      "Iteration 81, loss = 0.16556802\n",
      "Iteration 82, loss = 0.15585160\n",
      "Iteration 83, loss = 0.15959067\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68663926\n",
      "Iteration 2, loss = 0.54452304\n",
      "Iteration 3, loss = 0.42825328\n",
      "Iteration 4, loss = 0.46121612\n",
      "Iteration 5, loss = 0.43919247\n",
      "Iteration 6, loss = 0.43257779\n",
      "Iteration 7, loss = 0.40838179\n",
      "Iteration 8, loss = 0.39960830\n",
      "Iteration 9, loss = 0.40562910\n",
      "Iteration 10, loss = 0.40584872\n",
      "Iteration 11, loss = 0.40187585\n",
      "Iteration 12, loss = 0.39389308\n",
      "Iteration 13, loss = 0.38052873\n",
      "Iteration 14, loss = 0.37064741\n",
      "Iteration 15, loss = 0.36173208\n",
      "Iteration 16, loss = 0.35866703\n",
      "Iteration 17, loss = 0.35460518\n",
      "Iteration 18, loss = 0.35600519\n",
      "Iteration 19, loss = 0.35263903\n",
      "Iteration 20, loss = 0.35020207\n",
      "Iteration 21, loss = 0.34043504\n",
      "Iteration 22, loss = 0.33518906\n",
      "Iteration 23, loss = 0.33469966\n",
      "Iteration 24, loss = 0.33372047\n",
      "Iteration 25, loss = 0.33586342\n",
      "Iteration 26, loss = 0.33510900\n",
      "Iteration 27, loss = 0.33364573\n",
      "Iteration 28, loss = 0.33148455\n",
      "Iteration 29, loss = 0.32588055\n",
      "Iteration 30, loss = 0.31771815\n",
      "Iteration 31, loss = 0.31772054\n",
      "Iteration 32, loss = 0.30933022\n",
      "Iteration 33, loss = 0.31587343\n",
      "Iteration 34, loss = 0.30869061\n",
      "Iteration 35, loss = 0.29445273\n",
      "Iteration 36, loss = 0.28749157\n",
      "Iteration 37, loss = 0.29073214\n",
      "Iteration 38, loss = 0.29181374\n",
      "Iteration 39, loss = 0.28813242\n",
      "Iteration 40, loss = 0.30756783\n",
      "Iteration 41, loss = 0.31000430\n",
      "Iteration 42, loss = 0.29686786\n",
      "Iteration 43, loss = 0.29017843\n",
      "Iteration 44, loss = 0.28974628\n",
      "Iteration 45, loss = 0.28528199\n",
      "Iteration 46, loss = 0.27535268\n",
      "Iteration 47, loss = 0.27290074\n",
      "Iteration 48, loss = 0.27657657\n",
      "Iteration 49, loss = 0.27258647\n",
      "Iteration 50, loss = 0.27605254\n",
      "Iteration 51, loss = 0.26695440\n",
      "Iteration 52, loss = 0.25480092\n",
      "Iteration 53, loss = 0.25114352\n",
      "Iteration 54, loss = 0.24919256\n",
      "Iteration 55, loss = 0.26511509\n",
      "Iteration 56, loss = 0.26143638\n",
      "Iteration 57, loss = 0.25970384\n",
      "Iteration 58, loss = 0.25775804\n",
      "Iteration 59, loss = 0.25802339\n",
      "Iteration 60, loss = 0.26526624\n",
      "Iteration 61, loss = 0.26665850\n",
      "Iteration 62, loss = 0.26516020\n",
      "Iteration 63, loss = 0.26335599\n",
      "Iteration 64, loss = 0.25428203\n",
      "Iteration 65, loss = 0.26433064\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79133266\n",
      "Iteration 2, loss = 0.43507274\n",
      "Iteration 3, loss = 0.41298165\n",
      "Iteration 4, loss = 0.41376961\n",
      "Iteration 5, loss = 0.37620064\n",
      "Iteration 6, loss = 0.37316071\n",
      "Iteration 7, loss = 0.34603803\n",
      "Iteration 8, loss = 0.34482352\n",
      "Iteration 9, loss = 0.33401846\n",
      "Iteration 10, loss = 0.32930956\n",
      "Iteration 11, loss = 0.32626550\n",
      "Iteration 12, loss = 0.32344432\n",
      "Iteration 13, loss = 0.31098908\n",
      "Iteration 14, loss = 0.31156192\n",
      "Iteration 15, loss = 0.31256410\n",
      "Iteration 16, loss = 0.30844492\n",
      "Iteration 17, loss = 0.30732116\n",
      "Iteration 18, loss = 0.29902395\n",
      "Iteration 19, loss = 0.29988382\n",
      "Iteration 20, loss = 0.30479232\n",
      "Iteration 21, loss = 0.29477418\n",
      "Iteration 22, loss = 0.30557926\n",
      "Iteration 23, loss = 0.30111028\n",
      "Iteration 24, loss = 0.29326010\n",
      "Iteration 25, loss = 0.28802566\n",
      "Iteration 26, loss = 0.28559465\n",
      "Iteration 27, loss = 0.28520233\n",
      "Iteration 28, loss = 0.27961190\n",
      "Iteration 29, loss = 0.27609407\n",
      "Iteration 30, loss = 0.27273828\n",
      "Iteration 31, loss = 0.27462350\n",
      "Iteration 32, loss = 0.28080882\n",
      "Iteration 33, loss = 0.27823153\n",
      "Iteration 34, loss = 0.27488691\n",
      "Iteration 35, loss = 0.27410112\n",
      "Iteration 36, loss = 0.27317305\n",
      "Iteration 37, loss = 0.27342607\n",
      "Iteration 38, loss = 0.26672142\n",
      "Iteration 39, loss = 0.26915396\n",
      "Iteration 40, loss = 0.26039492\n",
      "Iteration 41, loss = 0.25434729\n",
      "Iteration 42, loss = 0.24767050\n",
      "Iteration 43, loss = 0.24737516\n",
      "Iteration 44, loss = 0.24862192\n",
      "Iteration 45, loss = 0.23898141\n",
      "Iteration 46, loss = 0.23974578\n",
      "Iteration 47, loss = 0.23880101\n",
      "Iteration 48, loss = 0.24480396\n",
      "Iteration 49, loss = 0.24168862\n",
      "Iteration 50, loss = 0.24822670\n",
      "Iteration 51, loss = 0.24957502\n",
      "Iteration 52, loss = 0.24525786\n",
      "Iteration 53, loss = 0.23737390\n",
      "Iteration 54, loss = 0.23673914\n",
      "Iteration 55, loss = 0.23223299\n",
      "Iteration 56, loss = 0.23034986\n",
      "Iteration 57, loss = 0.22950078\n",
      "Iteration 58, loss = 0.22799666\n",
      "Iteration 59, loss = 0.23130962\n",
      "Iteration 60, loss = 0.22473646\n",
      "Iteration 61, loss = 0.22046539\n",
      "Iteration 62, loss = 0.21527627\n",
      "Iteration 63, loss = 0.21777855\n",
      "Iteration 64, loss = 0.21067024\n",
      "Iteration 65, loss = 0.21078280\n",
      "Iteration 66, loss = 0.21314210\n",
      "Iteration 67, loss = 0.21196887\n",
      "Iteration 68, loss = 0.20168427\n",
      "Iteration 69, loss = 0.20277703\n",
      "Iteration 70, loss = 0.20075604\n",
      "Iteration 71, loss = 0.20795407\n",
      "Iteration 72, loss = 0.20694494\n",
      "Iteration 73, loss = 0.20912601\n",
      "Iteration 74, loss = 0.20813368\n",
      "Iteration 75, loss = 0.20890679\n",
      "Iteration 76, loss = 0.20901438\n",
      "Iteration 77, loss = 0.20793718\n",
      "Iteration 78, loss = 0.20685458\n",
      "Iteration 79, loss = 0.20575355\n",
      "Iteration 80, loss = 0.20461441\n",
      "Iteration 81, loss = 0.20320803\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.07031004\n",
      "Iteration 2, loss = 0.85242191\n",
      "Iteration 3, loss = 0.60871380\n",
      "Iteration 4, loss = 0.48342356\n",
      "Iteration 5, loss = 0.42622973\n",
      "Iteration 6, loss = 0.41188520\n",
      "Iteration 7, loss = 0.42903382\n",
      "Iteration 8, loss = 0.44079827\n",
      "Iteration 9, loss = 0.43389056\n",
      "Iteration 10, loss = 0.42252047\n",
      "Iteration 11, loss = 0.40607692\n",
      "Iteration 12, loss = 0.39246250\n",
      "Iteration 13, loss = 0.38193776\n",
      "Iteration 14, loss = 0.39129045\n",
      "Iteration 15, loss = 0.39347422\n",
      "Iteration 16, loss = 0.40196551\n",
      "Iteration 17, loss = 0.38688011\n",
      "Iteration 18, loss = 0.38340757\n",
      "Iteration 19, loss = 0.37420583\n",
      "Iteration 20, loss = 0.36735092\n",
      "Iteration 21, loss = 0.36431846\n",
      "Iteration 22, loss = 0.36543265\n",
      "Iteration 23, loss = 0.36533113\n",
      "Iteration 24, loss = 0.36517649\n",
      "Iteration 25, loss = 0.36005044\n",
      "Iteration 26, loss = 0.35157524\n",
      "Iteration 27, loss = 0.34727327\n",
      "Iteration 28, loss = 0.34553689\n",
      "Iteration 29, loss = 0.33938641\n",
      "Iteration 30, loss = 0.33880762\n",
      "Iteration 31, loss = 0.33889538\n",
      "Iteration 32, loss = 0.33309732\n",
      "Iteration 33, loss = 0.33743000\n",
      "Iteration 34, loss = 0.32706062\n",
      "Iteration 35, loss = 0.32573479\n",
      "Iteration 36, loss = 0.33165573\n",
      "Iteration 37, loss = 0.32647025\n",
      "Iteration 38, loss = 0.32985358\n",
      "Iteration 39, loss = 0.32965341\n",
      "Iteration 40, loss = 0.32396767\n",
      "Iteration 41, loss = 0.31986711\n",
      "Iteration 42, loss = 0.32488930\n",
      "Iteration 43, loss = 0.32270808\n",
      "Iteration 44, loss = 0.32045354\n",
      "Iteration 45, loss = 0.31326286\n",
      "Iteration 46, loss = 0.30996417\n",
      "Iteration 47, loss = 0.30998745\n",
      "Iteration 48, loss = 0.30625768\n",
      "Iteration 49, loss = 0.30753871\n",
      "Iteration 50, loss = 0.30590123\n",
      "Iteration 51, loss = 0.30460978\n",
      "Iteration 52, loss = 0.30284279\n",
      "Iteration 53, loss = 0.30242001\n",
      "Iteration 54, loss = 0.30076978\n",
      "Iteration 55, loss = 0.29807184\n",
      "Iteration 56, loss = 0.29560093\n",
      "Iteration 57, loss = 0.29290521\n",
      "Iteration 58, loss = 0.29045393\n",
      "Iteration 59, loss = 0.28867612\n",
      "Iteration 60, loss = 0.28859676\n",
      "Iteration 61, loss = 0.28643822\n",
      "Iteration 62, loss = 0.28595922\n",
      "Iteration 63, loss = 0.28500617\n",
      "Iteration 64, loss = 0.28303736\n",
      "Iteration 65, loss = 0.27894388\n",
      "Iteration 66, loss = 0.29693090\n",
      "Iteration 67, loss = 0.30487010\n",
      "Iteration 68, loss = 0.30168974\n",
      "Iteration 69, loss = 0.30572358\n",
      "Iteration 70, loss = 0.31266756\n",
      "Iteration 71, loss = 0.30968918\n",
      "Iteration 72, loss = 0.30483407\n",
      "Iteration 73, loss = 0.30919505\n",
      "Iteration 74, loss = 0.30712319\n",
      "Iteration 75, loss = 0.30564660\n",
      "Iteration 76, loss = 0.30285527\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63822305\n",
      "Iteration 2, loss = 0.50849332\n",
      "Iteration 3, loss = 0.41488227\n",
      "Iteration 4, loss = 0.43694691\n",
      "Iteration 5, loss = 0.43388580\n",
      "Iteration 6, loss = 0.41170586\n",
      "Iteration 7, loss = 0.38122400\n",
      "Iteration 8, loss = 0.38655608\n",
      "Iteration 9, loss = 0.39626737\n",
      "Iteration 10, loss = 0.39520025\n",
      "Iteration 11, loss = 0.37117673\n",
      "Iteration 12, loss = 0.36517845\n",
      "Iteration 13, loss = 0.34765243\n",
      "Iteration 14, loss = 0.33939786\n",
      "Iteration 15, loss = 0.34380240\n",
      "Iteration 16, loss = 0.34989034\n",
      "Iteration 17, loss = 0.34834469\n",
      "Iteration 18, loss = 0.35311286\n",
      "Iteration 19, loss = 0.34608348\n",
      "Iteration 20, loss = 0.34696378\n",
      "Iteration 21, loss = 0.34489468\n",
      "Iteration 22, loss = 0.33760675\n",
      "Iteration 23, loss = 0.33696940\n",
      "Iteration 24, loss = 0.32299175\n",
      "Iteration 25, loss = 0.31738725\n",
      "Iteration 26, loss = 0.31444988\n",
      "Iteration 27, loss = 0.30877264\n",
      "Iteration 28, loss = 0.30455654\n",
      "Iteration 29, loss = 0.29514422\n",
      "Iteration 30, loss = 0.30918308\n",
      "Iteration 31, loss = 0.29090763\n",
      "Iteration 32, loss = 0.29391597\n",
      "Iteration 33, loss = 0.28015508\n",
      "Iteration 34, loss = 0.29361814\n",
      "Iteration 35, loss = 0.29761137\n",
      "Iteration 36, loss = 0.30160004\n",
      "Iteration 37, loss = 0.29534781\n",
      "Iteration 38, loss = 0.29344354\n",
      "Iteration 39, loss = 0.28774154\n",
      "Iteration 40, loss = 0.28298632\n",
      "Iteration 41, loss = 0.27532529\n",
      "Iteration 42, loss = 0.27441237\n",
      "Iteration 43, loss = 0.27452938\n",
      "Iteration 44, loss = 0.28259976\n",
      "Iteration 45, loss = 0.28038138\n",
      "Iteration 46, loss = 0.25530467\n",
      "Iteration 47, loss = 0.24736527\n",
      "Iteration 48, loss = 0.24726112\n",
      "Iteration 49, loss = 0.23924346\n",
      "Iteration 50, loss = 0.24507612\n",
      "Iteration 51, loss = 0.23879263\n",
      "Iteration 52, loss = 0.23802774\n",
      "Iteration 53, loss = 0.23330224\n",
      "Iteration 54, loss = 0.24242455\n",
      "Iteration 55, loss = 0.25214048\n",
      "Iteration 56, loss = 0.25568530\n",
      "Iteration 57, loss = 0.24529455\n",
      "Iteration 58, loss = 0.23703427\n",
      "Iteration 59, loss = 0.23893728\n",
      "Iteration 60, loss = 0.23842816\n",
      "Iteration 61, loss = 0.22903129\n",
      "Iteration 62, loss = 0.23256385\n",
      "Iteration 63, loss = 0.22295858\n",
      "Iteration 64, loss = 0.23281971\n",
      "Iteration 65, loss = 0.24719440\n",
      "Iteration 66, loss = 0.24705839\n",
      "Iteration 67, loss = 0.24641516\n",
      "Iteration 68, loss = 0.24547273\n",
      "Iteration 69, loss = 0.23962021\n",
      "Iteration 70, loss = 0.23746494\n",
      "Iteration 71, loss = 0.24093012\n",
      "Iteration 72, loss = 0.23443580\n",
      "Iteration 73, loss = 0.23141798\n",
      "Iteration 74, loss = 0.22587646\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69720142\n",
      "Iteration 2, loss = 0.66734465\n",
      "Iteration 3, loss = 0.64937211\n",
      "Iteration 4, loss = 0.62913328\n",
      "Iteration 5, loss = 0.60836633\n",
      "Iteration 6, loss = 0.63669035\n",
      "Iteration 7, loss = 0.64327382\n",
      "Iteration 8, loss = 0.63975939\n",
      "Iteration 9, loss = 0.63389169\n",
      "Iteration 10, loss = 0.62500137\n",
      "Iteration 11, loss = 0.61150887\n",
      "Iteration 12, loss = 0.60256998\n",
      "Iteration 13, loss = 0.58893382\n",
      "Iteration 14, loss = 0.57933966\n",
      "Iteration 15, loss = 0.57224547\n",
      "Iteration 16, loss = 0.56483569\n",
      "Iteration 17, loss = 0.56487153\n",
      "Iteration 18, loss = 0.54391616\n",
      "Iteration 19, loss = 0.53767564\n",
      "Iteration 20, loss = 0.53490547\n",
      "Iteration 21, loss = 0.52650554\n",
      "Iteration 22, loss = 0.51744603\n",
      "Iteration 23, loss = 0.51327930\n",
      "Iteration 24, loss = 0.50036607\n",
      "Iteration 25, loss = 0.47157893\n",
      "Iteration 26, loss = 0.44943726\n",
      "Iteration 27, loss = 0.45321810\n",
      "Iteration 28, loss = 0.53675825\n",
      "Iteration 29, loss = 0.57825545\n",
      "Iteration 30, loss = 0.57248432\n",
      "Iteration 31, loss = 0.58474065\n",
      "Iteration 32, loss = 0.59606328\n",
      "Iteration 33, loss = 0.60622076\n",
      "Iteration 34, loss = 0.59729702\n",
      "Iteration 35, loss = 0.58805212\n",
      "Iteration 36, loss = 0.56615711\n",
      "Iteration 37, loss = 0.55699088\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68946601\n",
      "Iteration 2, loss = 0.67804577\n",
      "Iteration 3, loss = 0.67224305\n",
      "Iteration 4, loss = 0.64663689\n",
      "Iteration 5, loss = 0.62715686\n",
      "Iteration 6, loss = 0.61137361\n",
      "Iteration 7, loss = 0.58838042\n",
      "Iteration 8, loss = 0.57820093\n",
      "Iteration 9, loss = 0.56844347\n",
      "Iteration 10, loss = 0.57243799\n",
      "Iteration 11, loss = 0.55792108\n",
      "Iteration 12, loss = 0.54166550\n",
      "Iteration 13, loss = 0.51580852\n",
      "Iteration 14, loss = 0.48846487\n",
      "Iteration 15, loss = 0.48269913\n",
      "Iteration 16, loss = 0.47111196\n",
      "Iteration 17, loss = 0.47022607\n",
      "Iteration 18, loss = 0.45666512\n",
      "Iteration 19, loss = 0.46234931\n",
      "Iteration 20, loss = 0.45940797\n",
      "Iteration 21, loss = 0.44428251\n",
      "Iteration 22, loss = 0.43018732\n",
      "Iteration 23, loss = 0.43923580\n",
      "Iteration 24, loss = 0.43911415\n",
      "Iteration 25, loss = 0.41728634\n",
      "Iteration 26, loss = 0.41520939\n",
      "Iteration 27, loss = 0.42129295\n",
      "Iteration 28, loss = 0.41927985\n",
      "Iteration 29, loss = 0.41740123\n",
      "Iteration 30, loss = 0.41564054\n",
      "Iteration 31, loss = 0.41422734\n",
      "Iteration 32, loss = 0.41243448\n",
      "Iteration 33, loss = 0.41101132\n",
      "Iteration 34, loss = 0.40971979\n",
      "Iteration 35, loss = 0.40856748\n",
      "Iteration 36, loss = 0.40755680\n",
      "Iteration 37, loss = 0.40668318\n",
      "Iteration 38, loss = 0.40593461\n",
      "Iteration 39, loss = 0.40529272\n",
      "Iteration 40, loss = 0.40473509\n",
      "Iteration 41, loss = 0.40423838\n",
      "Iteration 42, loss = 0.40378165\n",
      "Iteration 43, loss = 0.40334911\n",
      "Iteration 44, loss = 0.40293166\n",
      "Iteration 45, loss = 0.40252700\n",
      "Iteration 46, loss = 0.40213833\n",
      "Iteration 47, loss = 0.40177214\n",
      "Iteration 48, loss = 0.40143551\n",
      "Iteration 49, loss = 0.40113383\n",
      "Iteration 50, loss = 0.40086915\n",
      "Iteration 51, loss = 0.40063967\n",
      "Iteration 52, loss = 0.40044017\n",
      "Iteration 53, loss = 0.40026327\n",
      "Iteration 54, loss = 0.40010118\n",
      "Iteration 55, loss = 0.39994717\n",
      "Iteration 56, loss = 0.39979676\n",
      "Iteration 57, loss = 0.39964823\n",
      "Iteration 58, loss = 0.39950235\n",
      "Iteration 59, loss = 0.39936162\n",
      "Iteration 60, loss = 0.39922915\n",
      "Iteration 61, loss = 0.39910756\n",
      "Iteration 62, loss = 0.39899817\n",
      "Iteration 63, loss = 0.39890067\n",
      "Iteration 64, loss = 0.39881321\n",
      "Iteration 65, loss = 0.39873304\n",
      "Iteration 66, loss = 0.39865725\n",
      "Iteration 67, loss = 0.39858351\n",
      "Iteration 68, loss = 0.39851054\n",
      "Iteration 69, loss = 0.39843826\n",
      "Iteration 70, loss = 0.39836752\n",
      "Iteration 71, loss = 0.39829964\n",
      "Iteration 72, loss = 0.39823582\n",
      "Iteration 73, loss = 0.39817677\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69594054\n",
      "Iteration 2, loss = 0.67821697\n",
      "Iteration 3, loss = 0.65429535\n",
      "Iteration 4, loss = 0.64168379\n",
      "Iteration 5, loss = 0.62228624\n",
      "Iteration 6, loss = 0.61487979\n",
      "Iteration 7, loss = 0.59079550\n",
      "Iteration 8, loss = 0.59223585\n",
      "Iteration 9, loss = 0.61063170\n",
      "Iteration 10, loss = 0.62985934\n",
      "Iteration 11, loss = 0.59808700\n",
      "Iteration 12, loss = 0.55717214\n",
      "Iteration 13, loss = 0.54231812\n",
      "Iteration 14, loss = 0.52386731\n",
      "Iteration 15, loss = 0.51874929\n",
      "Iteration 16, loss = 0.52600182\n",
      "Iteration 17, loss = 0.51702755\n",
      "Iteration 18, loss = 0.50295951\n",
      "Iteration 19, loss = 0.50544524\n",
      "Iteration 20, loss = 0.52854346\n",
      "Iteration 21, loss = 0.54326480\n",
      "Iteration 22, loss = 0.53561974\n",
      "Iteration 23, loss = 0.53567362\n",
      "Iteration 24, loss = 0.54722569\n",
      "Iteration 25, loss = 0.54929110\n",
      "Iteration 26, loss = 0.49919553\n",
      "Iteration 27, loss = 0.48986689\n",
      "Iteration 28, loss = 0.50045378\n",
      "Iteration 29, loss = 0.50659290\n",
      "Iteration 30, loss = 0.50161586\n",
      "Iteration 31, loss = 0.50165705\n",
      "Iteration 32, loss = 0.50131860\n",
      "Iteration 33, loss = 0.50021731\n",
      "Iteration 34, loss = 0.48004098\n",
      "Iteration 35, loss = 0.47907888\n",
      "Iteration 36, loss = 0.47816739\n",
      "Iteration 37, loss = 0.47740812\n",
      "Iteration 38, loss = 0.47832344\n",
      "Iteration 39, loss = 0.47643015\n",
      "Iteration 40, loss = 0.47624846\n",
      "Iteration 41, loss = 0.47621174\n",
      "Iteration 42, loss = 0.46951755\n",
      "Iteration 43, loss = 0.45988406\n",
      "Iteration 44, loss = 0.50169946\n",
      "Iteration 45, loss = 0.50239067\n",
      "Iteration 46, loss = 0.50238357\n",
      "Iteration 47, loss = 0.50220450\n",
      "Iteration 48, loss = 0.50191001\n",
      "Iteration 49, loss = 0.50156077\n",
      "Iteration 50, loss = 0.50121072\n",
      "Iteration 51, loss = 0.50089954\n",
      "Iteration 52, loss = 0.50064923\n",
      "Iteration 53, loss = 0.50046468\n",
      "Iteration 54, loss = 0.50111448\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69386474\n",
      "Iteration 2, loss = 0.69366247\n",
      "Iteration 3, loss = 0.68352238\n",
      "Iteration 4, loss = 0.68179801\n",
      "Iteration 5, loss = 0.68022818\n",
      "Iteration 6, loss = 0.67859104\n",
      "Iteration 7, loss = 0.67757774\n",
      "Iteration 8, loss = 0.67725732\n",
      "Iteration 9, loss = 0.67744870\n",
      "Iteration 10, loss = 0.67784713\n",
      "Iteration 11, loss = 0.67814398\n",
      "Iteration 12, loss = 0.67815952\n",
      "Iteration 13, loss = 0.67787605\n",
      "Iteration 14, loss = 0.67739751\n",
      "Iteration 15, loss = 0.67687976\n",
      "Iteration 16, loss = 0.67646564\n",
      "Iteration 17, loss = 0.67623983\n",
      "Iteration 18, loss = 0.67620891\n",
      "Iteration 19, loss = 0.67630954\n",
      "Iteration 20, loss = 0.67644152\n",
      "Iteration 21, loss = 0.67651194\n",
      "Iteration 22, loss = 0.67647053\n",
      "Iteration 23, loss = 0.67632175\n",
      "Iteration 24, loss = 0.67611308\n",
      "Iteration 25, loss = 0.67590908\n",
      "Iteration 26, loss = 0.67576361\n",
      "Iteration 27, loss = 0.67570040\n",
      "Iteration 28, loss = 0.67570773\n",
      "Iteration 29, loss = 0.67574796\n",
      "Iteration 30, loss = 0.67577674\n",
      "Iteration 31, loss = 0.67576264\n",
      "Iteration 32, loss = 0.67569858\n",
      "Iteration 33, loss = 0.67560107\n",
      "Iteration 34, loss = 0.67549939\n",
      "Iteration 35, loss = 0.67542092\n",
      "Iteration 36, loss = 0.67537923\n",
      "Iteration 37, loss = 0.67536989\n",
      "Iteration 38, loss = 0.67537488\n",
      "Iteration 39, loss = 0.67537282\n",
      "Iteration 40, loss = 0.67534955\n",
      "Iteration 41, loss = 0.67530375\n",
      "Iteration 42, loss = 0.67524563\n",
      "Iteration 43, loss = 0.67519023\n",
      "Iteration 44, loss = 0.67514930\n",
      "Iteration 45, loss = 0.67512590\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68993537\n",
      "Iteration 2, loss = 0.68705108\n",
      "Iteration 3, loss = 0.68093003\n",
      "Iteration 4, loss = 0.67673577\n",
      "Iteration 5, loss = 0.67186578\n",
      "Iteration 6, loss = 0.66440248\n",
      "Iteration 7, loss = 0.66217799\n",
      "Iteration 8, loss = 0.65984293\n",
      "Iteration 9, loss = 0.65394442\n",
      "Iteration 10, loss = 0.64431580\n",
      "Iteration 11, loss = 0.63306724\n",
      "Iteration 12, loss = 0.63072285\n",
      "Iteration 13, loss = 0.62437513\n",
      "Iteration 14, loss = 0.62123580\n",
      "Iteration 15, loss = 0.61020789\n",
      "Iteration 16, loss = 0.60634734\n",
      "Iteration 17, loss = 0.58738376\n",
      "Iteration 18, loss = 0.58311266\n",
      "Iteration 19, loss = 0.57544785\n",
      "Iteration 20, loss = 0.57209708\n",
      "Iteration 21, loss = 0.58257870\n",
      "Iteration 22, loss = 0.57572247\n",
      "Iteration 23, loss = 0.57630234\n",
      "Iteration 24, loss = 0.57721747\n",
      "Iteration 25, loss = 0.56667154\n",
      "Iteration 26, loss = 0.55322125\n",
      "Iteration 27, loss = 0.55363218\n",
      "Iteration 28, loss = 0.55386305\n",
      "Iteration 29, loss = 0.54050872\n",
      "Iteration 30, loss = 0.54706957\n",
      "Iteration 31, loss = 0.54341954\n",
      "Iteration 32, loss = 0.53819534\n",
      "Iteration 33, loss = 0.54790263\n",
      "Iteration 34, loss = 0.54542786\n",
      "Iteration 35, loss = 0.54337644\n",
      "Iteration 36, loss = 0.54174221\n",
      "Iteration 37, loss = 0.54694629\n",
      "Iteration 38, loss = 0.54015944\n",
      "Iteration 39, loss = 0.54017855\n",
      "Iteration 40, loss = 0.54054483\n",
      "Iteration 41, loss = 0.54104472\n",
      "Iteration 42, loss = 0.54147683\n",
      "Iteration 43, loss = 0.54169873\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70525105\n",
      "Iteration 2, loss = 0.63672104\n",
      "Iteration 3, loss = 0.61471115\n",
      "Iteration 4, loss = 0.59370879\n",
      "Iteration 5, loss = 0.56597982\n",
      "Iteration 6, loss = 0.55686265\n",
      "Iteration 7, loss = 0.54990547\n",
      "Iteration 8, loss = 0.52354952\n",
      "Iteration 9, loss = 0.51012936\n",
      "Iteration 10, loss = 0.50670360\n",
      "Iteration 11, loss = 0.49847495\n",
      "Iteration 12, loss = 0.49849532\n",
      "Iteration 13, loss = 0.48062806\n",
      "Iteration 14, loss = 0.45397612\n",
      "Iteration 15, loss = 0.43520136\n",
      "Iteration 16, loss = 0.42734398\n",
      "Iteration 17, loss = 0.47017737\n",
      "Iteration 18, loss = 0.47142937\n",
      "Iteration 19, loss = 0.44988236\n",
      "Iteration 20, loss = 0.44331479\n",
      "Iteration 21, loss = 0.42686188\n",
      "Iteration 22, loss = 0.40464999\n",
      "Iteration 23, loss = 0.39610083\n",
      "Iteration 24, loss = 0.40610077\n",
      "Iteration 25, loss = 0.40029053\n",
      "Iteration 26, loss = 0.40680372\n",
      "Iteration 27, loss = 0.40831965\n",
      "Iteration 28, loss = 0.37172302\n",
      "Iteration 29, loss = 0.40422067\n",
      "Iteration 30, loss = 0.37841188\n",
      "Iteration 31, loss = 0.35939637\n",
      "Iteration 32, loss = 0.34465024\n",
      "Iteration 33, loss = 0.34260810\n",
      "Iteration 34, loss = 0.35023225\n",
      "Iteration 35, loss = 0.35779659\n",
      "Iteration 36, loss = 0.39230216\n",
      "Iteration 37, loss = 0.37694332\n",
      "Iteration 38, loss = 0.39233356\n",
      "Iteration 39, loss = 0.40300207\n",
      "Iteration 40, loss = 0.38981106\n",
      "Iteration 41, loss = 0.38887989\n",
      "Iteration 42, loss = 0.41430604\n",
      "Iteration 43, loss = 0.40619435\n",
      "Iteration 44, loss = 0.40625739\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73501928\n",
      "Iteration 2, loss = 0.63078926\n",
      "Iteration 3, loss = 0.58553941\n",
      "Iteration 4, loss = 0.53859229\n",
      "Iteration 5, loss = 0.50920902\n",
      "Iteration 6, loss = 0.49858707\n",
      "Iteration 7, loss = 0.49531266\n",
      "Iteration 8, loss = 0.46681041\n",
      "Iteration 9, loss = 0.48192934\n",
      "Iteration 10, loss = 0.44957184\n",
      "Iteration 11, loss = 0.43447334\n",
      "Iteration 12, loss = 0.44510716\n",
      "Iteration 13, loss = 0.43905466\n",
      "Iteration 14, loss = 0.41375552\n",
      "Iteration 15, loss = 0.40101595\n",
      "Iteration 16, loss = 0.38906911\n",
      "Iteration 17, loss = 0.38429020\n",
      "Iteration 18, loss = 0.39452173\n",
      "Iteration 19, loss = 0.39196061\n",
      "Iteration 20, loss = 0.39452039\n",
      "Iteration 21, loss = 0.39467291\n",
      "Iteration 22, loss = 0.40114576\n",
      "Iteration 23, loss = 0.37720721\n",
      "Iteration 24, loss = 0.37201906\n",
      "Iteration 25, loss = 0.36187026\n",
      "Iteration 26, loss = 0.35416468\n",
      "Iteration 27, loss = 0.35084698\n",
      "Iteration 28, loss = 0.37027127\n",
      "Iteration 29, loss = 0.40043266\n",
      "Iteration 30, loss = 0.38206282\n",
      "Iteration 31, loss = 0.35108142\n",
      "Iteration 32, loss = 0.36925880\n",
      "Iteration 33, loss = 0.37470888\n",
      "Iteration 34, loss = 0.38817581\n",
      "Iteration 35, loss = 0.37994572\n",
      "Iteration 36, loss = 0.37440934\n",
      "Iteration 37, loss = 0.37640879\n",
      "Iteration 38, loss = 0.37346922\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70245455\n",
      "Iteration 2, loss = 0.68359017\n",
      "Iteration 3, loss = 0.62722960\n",
      "Iteration 4, loss = 0.59323550\n",
      "Iteration 5, loss = 0.56059847\n",
      "Iteration 6, loss = 0.54324156\n",
      "Iteration 7, loss = 0.52140011\n",
      "Iteration 8, loss = 0.49783896\n",
      "Iteration 9, loss = 0.47559466\n",
      "Iteration 10, loss = 0.44492605\n",
      "Iteration 11, loss = 0.44205806\n",
      "Iteration 12, loss = 0.43625533\n",
      "Iteration 13, loss = 0.42781308\n",
      "Iteration 14, loss = 0.42059340\n",
      "Iteration 15, loss = 0.42731098\n",
      "Iteration 16, loss = 0.41499681\n",
      "Iteration 17, loss = 0.41873317\n",
      "Iteration 18, loss = 0.42822510\n",
      "Iteration 19, loss = 0.41321632\n",
      "Iteration 20, loss = 0.41036013\n",
      "Iteration 21, loss = 0.38794509\n",
      "Iteration 22, loss = 0.38676159\n",
      "Iteration 23, loss = 0.38465813\n",
      "Iteration 24, loss = 0.39772773\n",
      "Iteration 25, loss = 0.38482958\n",
      "Iteration 26, loss = 0.38747373\n",
      "Iteration 27, loss = 0.38691045\n",
      "Iteration 28, loss = 0.39282310\n",
      "Iteration 29, loss = 0.40489174\n",
      "Iteration 30, loss = 0.37907589\n",
      "Iteration 31, loss = 0.37743965\n",
      "Iteration 32, loss = 0.37160597\n",
      "Iteration 33, loss = 0.36659274\n",
      "Iteration 34, loss = 0.36446552\n",
      "Iteration 35, loss = 0.36836412\n",
      "Iteration 36, loss = 0.36797222\n",
      "Iteration 37, loss = 0.37009942\n",
      "Iteration 38, loss = 0.36481720\n",
      "Iteration 39, loss = 0.36632922\n",
      "Iteration 40, loss = 0.36150765\n",
      "Iteration 41, loss = 0.36654717\n",
      "Iteration 42, loss = 0.36233724\n",
      "Iteration 43, loss = 0.35202892\n",
      "Iteration 44, loss = 0.34250812\n",
      "Iteration 45, loss = 0.34707802\n",
      "Iteration 46, loss = 0.37051980\n",
      "Iteration 47, loss = 0.35402167\n",
      "Iteration 48, loss = 0.34344705\n",
      "Iteration 49, loss = 0.35190036\n",
      "Iteration 50, loss = 0.34266685\n",
      "Iteration 51, loss = 0.35541864\n",
      "Iteration 52, loss = 0.36614359\n",
      "Iteration 53, loss = 0.36653624\n",
      "Iteration 54, loss = 0.35171287\n",
      "Iteration 55, loss = 0.35215618\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70326580\n",
      "Iteration 2, loss = 0.61657308\n",
      "Iteration 3, loss = 0.54761831\n",
      "Iteration 4, loss = 0.50753904\n",
      "Iteration 5, loss = 0.49712441\n",
      "Iteration 6, loss = 0.47244406\n",
      "Iteration 7, loss = 0.45857351\n",
      "Iteration 8, loss = 0.45090963\n",
      "Iteration 9, loss = 0.44548239\n",
      "Iteration 10, loss = 0.41636819\n",
      "Iteration 11, loss = 0.41461707\n",
      "Iteration 12, loss = 0.40515575\n",
      "Iteration 13, loss = 0.39029732\n",
      "Iteration 14, loss = 0.38569531\n",
      "Iteration 15, loss = 0.40047555\n",
      "Iteration 16, loss = 0.40600726\n",
      "Iteration 17, loss = 0.41575833\n",
      "Iteration 18, loss = 0.41885151\n",
      "Iteration 19, loss = 0.40898434\n",
      "Iteration 20, loss = 0.38566203\n",
      "Iteration 21, loss = 0.37093646\n",
      "Iteration 22, loss = 0.37244791\n",
      "Iteration 23, loss = 0.37573577\n",
      "Iteration 24, loss = 0.37322427\n",
      "Iteration 25, loss = 0.39229304\n",
      "Iteration 26, loss = 0.38445805\n",
      "Iteration 27, loss = 0.37304891\n",
      "Iteration 28, loss = 0.36888391\n",
      "Iteration 29, loss = 0.36676304\n",
      "Iteration 30, loss = 0.37700284\n",
      "Iteration 31, loss = 0.38202574\n",
      "Iteration 32, loss = 0.37033693\n",
      "Iteration 33, loss = 0.36405435\n",
      "Iteration 34, loss = 0.34877976\n",
      "Iteration 35, loss = 0.36954026\n",
      "Iteration 36, loss = 0.45281933\n",
      "Iteration 37, loss = 0.43419088\n",
      "Iteration 38, loss = 0.40606355\n",
      "Iteration 39, loss = 0.37790768\n",
      "Iteration 40, loss = 0.38343067\n",
      "Iteration 41, loss = 0.38034297\n",
      "Iteration 42, loss = 0.38410318\n",
      "Iteration 43, loss = 0.38828104\n",
      "Iteration 44, loss = 0.38452541\n",
      "Iteration 45, loss = 0.37700750\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69468745\n",
      "Iteration 2, loss = 0.63094783\n",
      "Iteration 3, loss = 0.57598907\n",
      "Iteration 4, loss = 0.54348842\n",
      "Iteration 5, loss = 0.51498354\n",
      "Iteration 6, loss = 0.48592308\n",
      "Iteration 7, loss = 0.46449996\n",
      "Iteration 8, loss = 0.44518690\n",
      "Iteration 9, loss = 0.43385188\n",
      "Iteration 10, loss = 0.42740694\n",
      "Iteration 11, loss = 0.41413139\n",
      "Iteration 12, loss = 0.40511020\n",
      "Iteration 13, loss = 0.38826497\n",
      "Iteration 14, loss = 0.40936397\n",
      "Iteration 15, loss = 0.38232738\n",
      "Iteration 16, loss = 0.37784700\n",
      "Iteration 17, loss = 0.36263015\n",
      "Iteration 18, loss = 0.35433119\n",
      "Iteration 19, loss = 0.36065015\n",
      "Iteration 20, loss = 0.39018090\n",
      "Iteration 21, loss = 0.38653433\n",
      "Iteration 22, loss = 0.39178551\n",
      "Iteration 23, loss = 0.39290872\n",
      "Iteration 24, loss = 0.40770104\n",
      "Iteration 25, loss = 0.40408795\n",
      "Iteration 26, loss = 0.41075710\n",
      "Iteration 27, loss = 0.40161343\n",
      "Iteration 28, loss = 0.39974560\n",
      "Iteration 29, loss = 0.39779261\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69419908\n",
      "Iteration 2, loss = 0.44871784\n",
      "Iteration 3, loss = 0.45456497\n",
      "Iteration 4, loss = 0.41763361\n",
      "Iteration 5, loss = 0.41963581\n",
      "Iteration 6, loss = 0.38125258\n",
      "Iteration 7, loss = 0.42629730\n",
      "Iteration 8, loss = 0.38436635\n",
      "Iteration 9, loss = 0.38026514\n",
      "Iteration 10, loss = 0.36813375\n",
      "Iteration 11, loss = 0.36278196\n",
      "Iteration 12, loss = 0.40174442\n",
      "Iteration 13, loss = 0.38278785\n",
      "Iteration 14, loss = 0.33234497\n",
      "Iteration 15, loss = 0.32668616\n",
      "Iteration 16, loss = 0.30689637\n",
      "Iteration 17, loss = 0.29667062\n",
      "Iteration 18, loss = 0.29803161\n",
      "Iteration 19, loss = 0.30249477\n",
      "Iteration 20, loss = 0.27819619\n",
      "Iteration 21, loss = 0.27583063\n",
      "Iteration 22, loss = 0.28181895\n",
      "Iteration 23, loss = 0.26395186\n",
      "Iteration 24, loss = 0.27068826\n",
      "Iteration 25, loss = 0.25202965\n",
      "Iteration 26, loss = 0.24499522\n",
      "Iteration 27, loss = 0.25664191\n",
      "Iteration 28, loss = 0.26762906\n",
      "Iteration 29, loss = 0.27803972\n",
      "Iteration 30, loss = 0.26517055\n",
      "Iteration 31, loss = 0.22521552\n",
      "Iteration 32, loss = 0.22757828\n",
      "Iteration 33, loss = 0.22006327\n",
      "Iteration 34, loss = 0.22025245\n",
      "Iteration 35, loss = 0.21283849\n",
      "Iteration 36, loss = 0.21133464\n",
      "Iteration 37, loss = 0.21318238\n",
      "Iteration 38, loss = 0.21259090\n",
      "Iteration 39, loss = 0.20492096\n",
      "Iteration 40, loss = 0.19721546\n",
      "Iteration 41, loss = 0.18792263\n",
      "Iteration 42, loss = 0.18403185\n",
      "Iteration 43, loss = 0.18942718\n",
      "Iteration 44, loss = 0.17935734\n",
      "Iteration 45, loss = 0.18198067\n",
      "Iteration 46, loss = 0.17465198\n",
      "Iteration 47, loss = 0.16648968\n",
      "Iteration 48, loss = 0.16300504\n",
      "Iteration 49, loss = 0.15866777\n",
      "Iteration 50, loss = 0.15693692\n",
      "Iteration 51, loss = 0.19092521\n",
      "Iteration 52, loss = 0.16309438\n",
      "Iteration 53, loss = 0.17797521\n",
      "Iteration 54, loss = 0.17900305\n",
      "Iteration 55, loss = 0.17133339\n",
      "Iteration 56, loss = 0.18047892\n",
      "Iteration 57, loss = 0.16337732\n",
      "Iteration 58, loss = 0.14754666\n",
      "Iteration 59, loss = 0.15283506\n",
      "Iteration 60, loss = 0.17496269\n",
      "Iteration 61, loss = 0.16529962\n",
      "Iteration 62, loss = 0.17458486\n",
      "Iteration 63, loss = 0.16969790\n",
      "Iteration 64, loss = 0.16767777\n",
      "Iteration 65, loss = 0.15619712\n",
      "Iteration 66, loss = 0.16306729\n",
      "Iteration 67, loss = 0.15556296\n",
      "Iteration 68, loss = 0.16851329\n",
      "Iteration 69, loss = 0.15398738\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76280122\n",
      "Iteration 2, loss = 0.54395159\n",
      "Iteration 3, loss = 0.46942386\n",
      "Iteration 4, loss = 0.40099450\n",
      "Iteration 5, loss = 0.41925343\n",
      "Iteration 6, loss = 0.41741169\n",
      "Iteration 7, loss = 0.37038174\n",
      "Iteration 8, loss = 0.37458996\n",
      "Iteration 9, loss = 0.38316544\n",
      "Iteration 10, loss = 0.36265466\n",
      "Iteration 11, loss = 0.36291211\n",
      "Iteration 12, loss = 0.36514501\n",
      "Iteration 13, loss = 0.36292033\n",
      "Iteration 14, loss = 0.35457085\n",
      "Iteration 15, loss = 0.35905823\n",
      "Iteration 16, loss = 0.35010248\n",
      "Iteration 17, loss = 0.33660865\n",
      "Iteration 18, loss = 0.32887388\n",
      "Iteration 19, loss = 0.33442632\n",
      "Iteration 20, loss = 0.33238467\n",
      "Iteration 21, loss = 0.32398084\n",
      "Iteration 22, loss = 0.32444379\n",
      "Iteration 23, loss = 0.32036506\n",
      "Iteration 24, loss = 0.32031864\n",
      "Iteration 25, loss = 0.31166344\n",
      "Iteration 26, loss = 0.31146212\n",
      "Iteration 27, loss = 0.30230174\n",
      "Iteration 28, loss = 0.29314576\n",
      "Iteration 29, loss = 0.28369531\n",
      "Iteration 30, loss = 0.29559588\n",
      "Iteration 31, loss = 0.28364819\n",
      "Iteration 32, loss = 0.27388112\n",
      "Iteration 33, loss = 0.27330619\n",
      "Iteration 34, loss = 0.25901563\n",
      "Iteration 35, loss = 0.24349296\n",
      "Iteration 36, loss = 0.25063969\n",
      "Iteration 37, loss = 0.25012644\n",
      "Iteration 38, loss = 0.24474756\n",
      "Iteration 39, loss = 0.25175597\n",
      "Iteration 40, loss = 0.27069019\n",
      "Iteration 41, loss = 0.26495892\n",
      "Iteration 42, loss = 0.25670558\n",
      "Iteration 43, loss = 0.25307995\n",
      "Iteration 44, loss = 0.24172903\n",
      "Iteration 45, loss = 0.23807797\n",
      "Iteration 46, loss = 0.24364116\n",
      "Iteration 47, loss = 0.24100376\n",
      "Iteration 48, loss = 0.23723324\n",
      "Iteration 49, loss = 0.22806161\n",
      "Iteration 50, loss = 0.22492949\n",
      "Iteration 51, loss = 0.23357439\n",
      "Iteration 52, loss = 0.24638656\n",
      "Iteration 53, loss = 0.26380907\n",
      "Iteration 54, loss = 0.27450297\n",
      "Iteration 55, loss = 0.26957096\n",
      "Iteration 56, loss = 0.27182469\n",
      "Iteration 57, loss = 0.27121974\n",
      "Iteration 58, loss = 0.26379508\n",
      "Iteration 59, loss = 0.24941453\n",
      "Iteration 60, loss = 0.26204459\n",
      "Iteration 61, loss = 0.25974247\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67750578\n",
      "Iteration 2, loss = 0.44276413\n",
      "Iteration 3, loss = 0.41159113\n",
      "Iteration 4, loss = 0.38937923\n",
      "Iteration 5, loss = 0.37376505\n",
      "Iteration 6, loss = 0.34283905\n",
      "Iteration 7, loss = 0.33681949\n",
      "Iteration 8, loss = 0.32158346\n",
      "Iteration 9, loss = 0.31690190\n",
      "Iteration 10, loss = 0.31823817\n",
      "Iteration 11, loss = 0.32280268\n",
      "Iteration 12, loss = 0.32958044\n",
      "Iteration 13, loss = 0.33584451\n",
      "Iteration 14, loss = 0.31471596\n",
      "Iteration 15, loss = 0.29487436\n",
      "Iteration 16, loss = 0.30101550\n",
      "Iteration 17, loss = 0.26885605\n",
      "Iteration 18, loss = 0.25409667\n",
      "Iteration 19, loss = 0.28918754\n",
      "Iteration 20, loss = 0.29739532\n",
      "Iteration 21, loss = 0.27956295\n",
      "Iteration 22, loss = 0.27353059\n",
      "Iteration 23, loss = 0.26423498\n",
      "Iteration 24, loss = 0.24092883\n",
      "Iteration 25, loss = 0.27724850\n",
      "Iteration 26, loss = 0.26033672\n",
      "Iteration 27, loss = 0.24506413\n",
      "Iteration 28, loss = 0.24127594\n",
      "Iteration 29, loss = 0.22980038\n",
      "Iteration 30, loss = 0.23768971\n",
      "Iteration 31, loss = 0.24151374\n",
      "Iteration 32, loss = 0.22629746\n",
      "Iteration 33, loss = 0.23424229\n",
      "Iteration 34, loss = 0.22768507\n",
      "Iteration 35, loss = 0.24209219\n",
      "Iteration 36, loss = 0.24953159\n",
      "Iteration 37, loss = 0.25079718\n",
      "Iteration 38, loss = 0.24461538\n",
      "Iteration 39, loss = 0.21954928\n",
      "Iteration 40, loss = 0.22611388\n",
      "Iteration 41, loss = 0.21217594\n",
      "Iteration 42, loss = 0.20035933\n",
      "Iteration 43, loss = 0.20634385\n",
      "Iteration 44, loss = 0.22110273\n",
      "Iteration 45, loss = 0.22190113\n",
      "Iteration 46, loss = 0.25447985\n",
      "Iteration 47, loss = 0.23722788\n",
      "Iteration 48, loss = 0.21259630\n",
      "Iteration 49, loss = 0.19614250\n",
      "Iteration 50, loss = 0.19846448\n",
      "Iteration 51, loss = 0.19707468\n",
      "Iteration 52, loss = 0.20072544\n",
      "Iteration 53, loss = 0.19625230\n",
      "Iteration 54, loss = 0.19844719\n",
      "Iteration 55, loss = 0.19733651\n",
      "Iteration 56, loss = 0.20025150\n",
      "Iteration 57, loss = 0.20486168\n",
      "Iteration 58, loss = 0.20309128\n",
      "Iteration 59, loss = 0.19292678\n",
      "Iteration 60, loss = 0.18732841\n",
      "Iteration 61, loss = 0.18314593\n",
      "Iteration 62, loss = 0.17862607\n",
      "Iteration 63, loss = 0.17705778\n",
      "Iteration 64, loss = 0.17512812\n",
      "Iteration 65, loss = 0.17794929\n",
      "Iteration 66, loss = 0.17608124\n",
      "Iteration 67, loss = 0.17343246\n",
      "Iteration 68, loss = 0.17268651\n",
      "Iteration 69, loss = 0.16936189\n",
      "Iteration 70, loss = 0.16645937\n",
      "Iteration 71, loss = 0.16506468\n",
      "Iteration 72, loss = 0.16395817\n",
      "Iteration 73, loss = 0.17797666\n",
      "Iteration 74, loss = 0.16189688\n",
      "Iteration 75, loss = 0.16364800\n",
      "Iteration 76, loss = 0.15654903\n",
      "Iteration 77, loss = 0.15488016\n",
      "Iteration 78, loss = 0.15320000\n",
      "Iteration 79, loss = 0.15200468\n",
      "Iteration 80, loss = 0.15076635\n",
      "Iteration 81, loss = 0.14908117\n",
      "Iteration 82, loss = 0.14849215\n",
      "Iteration 83, loss = 0.14603509\n",
      "Iteration 84, loss = 0.14452000\n",
      "Iteration 85, loss = 0.14346753\n",
      "Iteration 86, loss = 0.14211203\n",
      "Iteration 87, loss = 0.14109482\n",
      "Iteration 88, loss = 0.13999742\n",
      "Iteration 89, loss = 0.13909106\n",
      "Iteration 90, loss = 0.13794372\n",
      "Iteration 91, loss = 0.13679733\n",
      "Iteration 92, loss = 0.13596912\n",
      "Iteration 93, loss = 0.13403217\n",
      "Iteration 94, loss = 0.13898116\n",
      "Iteration 95, loss = 0.13466428\n",
      "Iteration 96, loss = 0.18723861\n",
      "Iteration 97, loss = 0.20883322\n",
      "Iteration 98, loss = 0.23514526\n",
      "Iteration 99, loss = 0.19774892\n",
      "Iteration 100, loss = 0.18244390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.64542032\n",
      "Iteration 2, loss = 0.46673069\n",
      "Iteration 3, loss = 0.41269953\n",
      "Iteration 4, loss = 0.41591852\n",
      "Iteration 5, loss = 0.38591883\n",
      "Iteration 6, loss = 0.37139793\n",
      "Iteration 7, loss = 0.38692360\n",
      "Iteration 8, loss = 0.36382843\n",
      "Iteration 9, loss = 0.34600355\n",
      "Iteration 10, loss = 0.31031781\n",
      "Iteration 11, loss = 0.32000040\n",
      "Iteration 12, loss = 0.31244483\n",
      "Iteration 13, loss = 0.32628523\n",
      "Iteration 14, loss = 0.31141771\n",
      "Iteration 15, loss = 0.30021497\n",
      "Iteration 16, loss = 0.28193386\n",
      "Iteration 17, loss = 0.26474443\n",
      "Iteration 18, loss = 0.25993719\n",
      "Iteration 19, loss = 0.24808876\n",
      "Iteration 20, loss = 0.24901844\n",
      "Iteration 21, loss = 0.24882017\n",
      "Iteration 22, loss = 0.25824523\n",
      "Iteration 23, loss = 0.25601393\n",
      "Iteration 24, loss = 0.24957049\n",
      "Iteration 25, loss = 0.24658711\n",
      "Iteration 26, loss = 0.23581444\n",
      "Iteration 27, loss = 0.23883966\n",
      "Iteration 28, loss = 0.22917125\n",
      "Iteration 29, loss = 0.23022357\n",
      "Iteration 30, loss = 0.23510205\n",
      "Iteration 31, loss = 0.22342732\n",
      "Iteration 32, loss = 0.23217514\n",
      "Iteration 33, loss = 0.23697995\n",
      "Iteration 34, loss = 0.22820280\n",
      "Iteration 35, loss = 0.23372403\n",
      "Iteration 36, loss = 0.23542537\n",
      "Iteration 37, loss = 0.22604902\n",
      "Iteration 38, loss = 0.21454992\n",
      "Iteration 39, loss = 0.20803629\n",
      "Iteration 40, loss = 0.18969183\n",
      "Iteration 41, loss = 0.17914310\n",
      "Iteration 42, loss = 0.18723758\n",
      "Iteration 43, loss = 0.17685192\n",
      "Iteration 44, loss = 0.18378658\n",
      "Iteration 45, loss = 0.18186911\n",
      "Iteration 46, loss = 0.17991145\n",
      "Iteration 47, loss = 0.18551907\n",
      "Iteration 48, loss = 0.19074045\n",
      "Iteration 49, loss = 0.18518548\n",
      "Iteration 50, loss = 0.18882193\n",
      "Iteration 51, loss = 0.18669901\n",
      "Iteration 52, loss = 0.18638308\n",
      "Iteration 53, loss = 0.17065540\n",
      "Iteration 54, loss = 0.17613533\n",
      "Iteration 55, loss = 0.15725243\n",
      "Iteration 56, loss = 0.15500010\n",
      "Iteration 57, loss = 0.14307136\n",
      "Iteration 58, loss = 0.13261375\n",
      "Iteration 59, loss = 0.13309228\n",
      "Iteration 60, loss = 0.12885518\n",
      "Iteration 61, loss = 0.12998628\n",
      "Iteration 62, loss = 0.12789328\n",
      "Iteration 63, loss = 0.14438975\n",
      "Iteration 64, loss = 0.14180335\n",
      "Iteration 65, loss = 0.14040926\n",
      "Iteration 66, loss = 0.14148228\n",
      "Iteration 67, loss = 0.13527721\n",
      "Iteration 68, loss = 0.13540795\n",
      "Iteration 69, loss = 0.13503423\n",
      "Iteration 70, loss = 0.13294432\n",
      "Iteration 71, loss = 0.13059886\n",
      "Iteration 72, loss = 0.12425598\n",
      "Iteration 73, loss = 0.12357544\n",
      "Iteration 74, loss = 0.11867790\n",
      "Iteration 75, loss = 0.11602549\n",
      "Iteration 76, loss = 0.11224103\n",
      "Iteration 77, loss = 0.10977220\n",
      "Iteration 78, loss = 0.11065433\n",
      "Iteration 79, loss = 0.10792640\n",
      "Iteration 80, loss = 0.10594214\n",
      "Iteration 81, loss = 0.10480188\n",
      "Iteration 82, loss = 0.10454672\n",
      "Iteration 83, loss = 0.10295686\n",
      "Iteration 84, loss = 0.10234392\n",
      "Iteration 85, loss = 0.10137177\n",
      "Iteration 86, loss = 0.10118833\n",
      "Iteration 87, loss = 0.09861099\n",
      "Iteration 88, loss = 0.09878901\n",
      "Iteration 89, loss = 0.09815281\n",
      "Iteration 90, loss = 0.09982134\n",
      "Iteration 91, loss = 0.09694025\n",
      "Iteration 92, loss = 0.09709133\n",
      "Iteration 93, loss = 0.09893431\n",
      "Iteration 94, loss = 0.10798435\n",
      "Iteration 95, loss = 0.10759923\n",
      "Iteration 96, loss = 0.11056584\n",
      "Iteration 97, loss = 0.11012147\n",
      "Iteration 98, loss = 0.12046341\n",
      "Iteration 99, loss = 0.11862554\n",
      "Iteration 100, loss = 0.11830554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.76650240\n",
      "Iteration 2, loss = 0.52366452\n",
      "Iteration 3, loss = 0.43488553\n",
      "Iteration 4, loss = 0.42272073\n",
      "Iteration 5, loss = 0.38160689\n",
      "Iteration 6, loss = 0.39846795\n",
      "Iteration 7, loss = 0.36953434\n",
      "Iteration 8, loss = 0.35944257\n",
      "Iteration 9, loss = 0.38571033\n",
      "Iteration 10, loss = 0.36244647\n",
      "Iteration 11, loss = 0.33465603\n",
      "Iteration 12, loss = 0.33887765\n",
      "Iteration 13, loss = 0.34213454\n",
      "Iteration 14, loss = 0.31991259\n",
      "Iteration 15, loss = 0.31892021\n",
      "Iteration 16, loss = 0.31357971\n",
      "Iteration 17, loss = 0.31497476\n",
      "Iteration 18, loss = 0.30141909\n",
      "Iteration 19, loss = 0.29520865\n",
      "Iteration 20, loss = 0.28552553\n",
      "Iteration 21, loss = 0.27772760\n",
      "Iteration 22, loss = 0.27167203\n",
      "Iteration 23, loss = 0.26832741\n",
      "Iteration 24, loss = 0.26244432\n",
      "Iteration 25, loss = 0.25271199\n",
      "Iteration 26, loss = 0.24377741\n",
      "Iteration 27, loss = 0.23382224\n",
      "Iteration 28, loss = 0.22851571\n",
      "Iteration 29, loss = 0.24144106\n",
      "Iteration 30, loss = 0.24442847\n",
      "Iteration 31, loss = 0.24038481\n",
      "Iteration 32, loss = 0.23544897\n",
      "Iteration 33, loss = 0.23777386\n",
      "Iteration 34, loss = 0.22532797\n",
      "Iteration 35, loss = 0.21304891\n",
      "Iteration 36, loss = 0.21623354\n",
      "Iteration 37, loss = 0.22238984\n",
      "Iteration 38, loss = 0.22743054\n",
      "Iteration 39, loss = 0.24087363\n",
      "Iteration 40, loss = 0.23521621\n",
      "Iteration 41, loss = 0.22618696\n",
      "Iteration 42, loss = 0.21432596\n",
      "Iteration 43, loss = 0.21042069\n",
      "Iteration 44, loss = 0.21145535\n",
      "Iteration 45, loss = 0.19682606\n",
      "Iteration 46, loss = 0.20189199\n",
      "Iteration 47, loss = 0.18287285\n",
      "Iteration 48, loss = 0.17802865\n",
      "Iteration 49, loss = 0.18793206\n",
      "Iteration 50, loss = 0.18109285\n",
      "Iteration 51, loss = 0.17592977\n",
      "Iteration 52, loss = 0.16215317\n",
      "Iteration 53, loss = 0.15891500\n",
      "Iteration 54, loss = 0.15144097\n",
      "Iteration 55, loss = 0.15241598\n",
      "Iteration 56, loss = 0.15513218\n",
      "Iteration 57, loss = 0.15268462\n",
      "Iteration 58, loss = 0.14673694\n",
      "Iteration 59, loss = 0.14422152\n",
      "Iteration 60, loss = 0.14340336\n",
      "Iteration 61, loss = 0.14019164\n",
      "Iteration 62, loss = 0.13736627\n",
      "Iteration 63, loss = 0.12578581\n",
      "Iteration 64, loss = 0.12523080\n",
      "Iteration 65, loss = 0.13353744\n",
      "Iteration 66, loss = 0.12155036\n",
      "Iteration 67, loss = 0.11629012\n",
      "Iteration 68, loss = 0.14406642\n",
      "Iteration 69, loss = 0.15862758\n",
      "Iteration 70, loss = 0.13924759\n",
      "Iteration 71, loss = 0.13168002\n",
      "Iteration 72, loss = 0.14649067\n",
      "Iteration 73, loss = 0.13486875\n",
      "Iteration 74, loss = 0.14384936\n",
      "Iteration 75, loss = 0.12991958\n",
      "Iteration 76, loss = 0.12865319\n",
      "Iteration 77, loss = 0.12944252\n",
      "Iteration 78, loss = 0.12738420\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.02925886\n",
      "Iteration 2, loss = 0.93626538\n",
      "Iteration 3, loss = 0.93359197\n",
      "Iteration 4, loss = 0.93216718\n",
      "Iteration 5, loss = 0.93077831\n",
      "Iteration 6, loss = 0.92941322\n",
      "Iteration 7, loss = 0.92800570\n",
      "Iteration 8, loss = 0.92490084\n",
      "Iteration 9, loss = 0.92282059\n",
      "Iteration 10, loss = 0.92147074\n",
      "Iteration 11, loss = 0.92015476\n",
      "Iteration 12, loss = 0.91884671\n",
      "Iteration 13, loss = 0.91754463\n",
      "Iteration 14, loss = 0.91624787\n",
      "Iteration 15, loss = 0.91495593\n",
      "Iteration 16, loss = 0.91366842\n",
      "Iteration 17, loss = 0.91238502\n",
      "Iteration 18, loss = 0.91110548\n",
      "Iteration 19, loss = 0.90982958\n",
      "Iteration 20, loss = 0.90855715\n",
      "Iteration 21, loss = 0.90728804\n",
      "Iteration 22, loss = 0.90602213\n",
      "Iteration 23, loss = 0.90475933\n",
      "Iteration 24, loss = 0.90349956\n",
      "Iteration 25, loss = 0.90224275\n",
      "Iteration 26, loss = 0.90098886\n",
      "Iteration 27, loss = 0.89973784\n",
      "Iteration 28, loss = 0.89848968\n",
      "Iteration 29, loss = 0.89724434\n",
      "Iteration 30, loss = 0.89600182\n",
      "Iteration 31, loss = 0.89476211\n",
      "Iteration 32, loss = 0.89352521\n",
      "Iteration 33, loss = 0.89229112\n",
      "Iteration 34, loss = 0.89105985\n",
      "Iteration 35, loss = 0.88983141\n",
      "Iteration 36, loss = 0.88860582\n",
      "Iteration 37, loss = 0.88738308\n",
      "Iteration 38, loss = 0.88616323\n",
      "Iteration 39, loss = 0.88494626\n",
      "Iteration 40, loss = 0.88373222\n",
      "Iteration 41, loss = 0.88252111\n",
      "Iteration 42, loss = 0.88131297\n",
      "Iteration 43, loss = 0.88010780\n",
      "Iteration 44, loss = 0.87890565\n",
      "Iteration 45, loss = 0.87770653\n",
      "Iteration 46, loss = 0.87651046\n",
      "Iteration 47, loss = 0.87531747\n",
      "Iteration 48, loss = 0.87412758\n",
      "Iteration 49, loss = 0.87294082\n",
      "Iteration 50, loss = 0.87175721\n",
      "Iteration 51, loss = 0.87057676\n",
      "Iteration 52, loss = 0.86939952\n",
      "Iteration 53, loss = 0.86822549\n",
      "Iteration 54, loss = 0.86705470\n",
      "Iteration 55, loss = 0.86588716\n",
      "Iteration 56, loss = 0.86472291\n",
      "Iteration 57, loss = 0.86356196\n",
      "Iteration 58, loss = 0.86240432\n",
      "Iteration 59, loss = 0.86125002\n",
      "Iteration 60, loss = 0.86009907\n",
      "Iteration 61, loss = 0.85895150\n",
      "Iteration 62, loss = 0.85780731\n",
      "Iteration 63, loss = 0.85666653\n",
      "Iteration 64, loss = 0.85552917\n",
      "Iteration 65, loss = 0.85439525\n",
      "Iteration 66, loss = 0.85326477\n",
      "Iteration 67, loss = 0.85213775\n",
      "Iteration 68, loss = 0.85101421\n",
      "Iteration 69, loss = 0.84989416\n",
      "Iteration 70, loss = 0.84877760\n",
      "Iteration 71, loss = 0.84766455\n",
      "Iteration 72, loss = 0.84655503\n",
      "Iteration 73, loss = 0.84544903\n",
      "Iteration 74, loss = 0.84434658\n",
      "Iteration 75, loss = 0.84324767\n",
      "Iteration 76, loss = 0.84215232\n",
      "Iteration 77, loss = 0.84106053\n",
      "Iteration 78, loss = 0.83997232\n",
      "Iteration 79, loss = 0.83888768\n",
      "Iteration 80, loss = 0.83780663\n",
      "Iteration 81, loss = 0.83672917\n",
      "Iteration 82, loss = 0.83565531\n",
      "Iteration 83, loss = 0.83458504\n",
      "Iteration 84, loss = 0.83351838\n",
      "Iteration 85, loss = 0.83245533\n",
      "Iteration 86, loss = 0.83139590\n",
      "Iteration 87, loss = 0.83034008\n",
      "Iteration 88, loss = 0.82928788\n",
      "Iteration 89, loss = 0.82823930\n",
      "Iteration 90, loss = 0.82719435\n",
      "Iteration 91, loss = 0.82615302\n",
      "Iteration 92, loss = 0.82511533\n",
      "Iteration 93, loss = 0.82408126\n",
      "Iteration 94, loss = 0.82305082\n",
      "Iteration 95, loss = 0.82202401\n",
      "Iteration 96, loss = 0.82100084\n",
      "Iteration 97, loss = 0.81998129\n",
      "Iteration 98, loss = 0.81896538\n",
      "Iteration 99, loss = 0.81795310\n",
      "Iteration 100, loss = 0.81694444\n",
      "Iteration 1, loss = 0.69826531\n",
      "Iteration 2, loss = 0.69803100\n",
      "Iteration 3, loss = 0.69779829\n",
      "Iteration 4, loss = 0.69756719\n",
      "Iteration 5, loss = 0.69733775\n",
      "Iteration 6, loss = 0.69710997\n",
      "Iteration 7, loss = 0.69688387\n",
      "Iteration 8, loss = 0.69665948\n",
      "Iteration 9, loss = 0.69643681\n",
      "Iteration 10, loss = 0.69621588\n",
      "Iteration 11, loss = 0.69599670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.69577930\n",
      "Iteration 13, loss = 0.69556367\n",
      "Iteration 14, loss = 0.69534984\n",
      "Iteration 15, loss = 0.69513780\n",
      "Iteration 16, loss = 0.69492758\n",
      "Iteration 17, loss = 0.69471917\n",
      "Iteration 18, loss = 0.69451259\n",
      "Iteration 19, loss = 0.69430784\n",
      "Iteration 20, loss = 0.69410491\n",
      "Iteration 21, loss = 0.69390382\n",
      "Iteration 22, loss = 0.69370456\n",
      "Iteration 23, loss = 0.69350713\n",
      "Iteration 24, loss = 0.69331154\n",
      "Iteration 25, loss = 0.69311777\n",
      "Iteration 26, loss = 0.69292583\n",
      "Iteration 27, loss = 0.69273570\n",
      "Iteration 28, loss = 0.69254739\n",
      "Iteration 29, loss = 0.69236088\n",
      "Iteration 30, loss = 0.69217618\n",
      "Iteration 31, loss = 0.69199326\n",
      "Iteration 32, loss = 0.69181212\n",
      "Iteration 33, loss = 0.69163275\n",
      "Iteration 34, loss = 0.69145515\n",
      "Iteration 35, loss = 0.69127928\n",
      "Iteration 36, loss = 0.69110516\n",
      "Iteration 37, loss = 0.69093276\n",
      "Iteration 38, loss = 0.69076206\n",
      "Iteration 39, loss = 0.69059307\n",
      "Iteration 40, loss = 0.69042575\n",
      "Iteration 41, loss = 0.69026011\n",
      "Iteration 42, loss = 0.69009611\n",
      "Iteration 43, loss = 0.68993376\n",
      "Iteration 44, loss = 0.68977303\n",
      "Iteration 45, loss = 0.68961390\n",
      "Iteration 46, loss = 0.68945637\n",
      "Iteration 47, loss = 0.68930041\n",
      "Iteration 48, loss = 0.68914602\n",
      "Iteration 49, loss = 0.68899317\n",
      "Iteration 50, loss = 0.68884185\n",
      "Iteration 51, loss = 0.68869204\n",
      "Iteration 52, loss = 0.68854374\n",
      "Iteration 53, loss = 0.68839691\n",
      "Iteration 54, loss = 0.68825156\n",
      "Iteration 55, loss = 0.68810765\n",
      "Iteration 56, loss = 0.68796518\n",
      "Iteration 57, loss = 0.68782414\n",
      "Iteration 58, loss = 0.68768450\n",
      "Iteration 59, loss = 0.68754625\n",
      "Iteration 60, loss = 0.68740938\n",
      "Iteration 61, loss = 0.68727388\n",
      "Iteration 62, loss = 0.68713973\n",
      "Iteration 63, loss = 0.68700691\n",
      "Iteration 64, loss = 0.68687542\n",
      "Iteration 65, loss = 0.68674524\n",
      "Iteration 66, loss = 0.68661636\n",
      "Iteration 67, loss = 0.68648876\n",
      "Iteration 68, loss = 0.68636244\n",
      "Iteration 69, loss = 0.68623738\n",
      "Iteration 70, loss = 0.68611357\n",
      "Iteration 71, loss = 0.68599100\n",
      "Iteration 72, loss = 0.68586966\n",
      "Iteration 73, loss = 0.68574954\n",
      "Iteration 74, loss = 0.68563063\n",
      "Iteration 75, loss = 0.68551292\n",
      "Iteration 76, loss = 0.68539639\n",
      "Iteration 77, loss = 0.68528105\n",
      "Iteration 78, loss = 0.68516688\n",
      "Iteration 79, loss = 0.68505388\n",
      "Iteration 80, loss = 0.68494203\n",
      "Iteration 81, loss = 0.68483133\n",
      "Iteration 82, loss = 0.68472176\n",
      "Iteration 83, loss = 0.68461333\n",
      "Iteration 84, loss = 0.68450603\n",
      "Iteration 85, loss = 0.68439985\n",
      "Iteration 86, loss = 0.68429477\n",
      "Iteration 87, loss = 0.68419081\n",
      "Iteration 88, loss = 0.68408794\n",
      "Iteration 89, loss = 0.68398616\n",
      "Iteration 90, loss = 0.68388548\n",
      "Iteration 91, loss = 0.68378587\n",
      "Iteration 92, loss = 0.68368735\n",
      "Iteration 93, loss = 0.68358989\n",
      "Iteration 94, loss = 0.68349350\n",
      "Iteration 95, loss = 0.68339817\n",
      "Iteration 96, loss = 0.68330390\n",
      "Iteration 97, loss = 0.68321068\n",
      "Iteration 98, loss = 0.68311851\n",
      "Iteration 99, loss = 0.68302738\n",
      "Iteration 100, loss = 0.68293729\n",
      "Iteration 1, loss = 3.55587683\n",
      "Iteration 2, loss = 0.79226186\n",
      "Iteration 3, loss = 0.74014889\n",
      "Iteration 4, loss = 0.73909728\n",
      "Iteration 5, loss = 0.73797398\n",
      "Iteration 6, loss = 0.73685978\n",
      "Iteration 7, loss = 0.73575903\n",
      "Iteration 8, loss = 0.73466504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.73357794\n",
      "Iteration 10, loss = 0.73249849\n",
      "Iteration 11, loss = 0.73142733\n",
      "Iteration 12, loss = 0.73036495\n",
      "Iteration 13, loss = 0.72931172\n",
      "Iteration 14, loss = 0.72826792\n",
      "Iteration 15, loss = 0.72723376\n",
      "Iteration 16, loss = 0.72620942\n",
      "Iteration 17, loss = 0.72519505\n",
      "Iteration 18, loss = 0.72419077\n",
      "Iteration 19, loss = 0.72319666\n",
      "Iteration 20, loss = 0.72221281\n",
      "Iteration 21, loss = 0.72123927\n",
      "Iteration 22, loss = 0.72027611\n",
      "Iteration 23, loss = 0.71932335\n",
      "Iteration 24, loss = 0.71838103\n",
      "Iteration 25, loss = 0.71744917\n",
      "Iteration 26, loss = 0.71652778\n",
      "Iteration 27, loss = 0.71561687\n",
      "Iteration 28, loss = 0.71471643\n",
      "Iteration 29, loss = 0.71382646\n",
      "Iteration 30, loss = 0.71294695\n",
      "Iteration 31, loss = 0.71207787\n",
      "Iteration 32, loss = 0.71121921\n",
      "Iteration 33, loss = 0.71037093\n",
      "Iteration 34, loss = 0.70953300\n",
      "Iteration 35, loss = 0.70870538\n",
      "Iteration 36, loss = 0.70788804\n",
      "Iteration 37, loss = 0.70708092\n",
      "Iteration 38, loss = 0.70628399\n",
      "Iteration 39, loss = 0.70549718\n",
      "Iteration 40, loss = 0.70472045\n",
      "Iteration 41, loss = 0.70396790\n",
      "Iteration 42, loss = 0.70329814\n",
      "Iteration 43, loss = 0.70263625\n",
      "Iteration 44, loss = 0.70198214\n",
      "Iteration 45, loss = 0.70133575\n",
      "Iteration 46, loss = 0.70069700\n",
      "Iteration 47, loss = 0.70006584\n",
      "Iteration 48, loss = 0.69944219\n",
      "Iteration 49, loss = 0.69882601\n",
      "Iteration 50, loss = 0.69821723\n",
      "Iteration 51, loss = 0.69761580\n",
      "Iteration 52, loss = 0.69702166\n",
      "Iteration 53, loss = 0.69643476\n",
      "Iteration 54, loss = 0.69585505\n",
      "Iteration 55, loss = 0.69528247\n",
      "Iteration 56, loss = 0.69471698\n",
      "Iteration 57, loss = 0.69415853\n",
      "Iteration 58, loss = 0.69360707\n",
      "Iteration 59, loss = 0.69306254\n",
      "Iteration 60, loss = 0.69252490\n",
      "Iteration 61, loss = 0.69199410\n",
      "Iteration 62, loss = 0.69147010\n",
      "Iteration 63, loss = 0.69095284\n",
      "Iteration 64, loss = 0.69044227\n",
      "Iteration 65, loss = 0.68993836\n",
      "Iteration 66, loss = 0.68944104\n",
      "Iteration 67, loss = 0.68895028\n",
      "Iteration 68, loss = 0.68846601\n",
      "Iteration 69, loss = 0.68798821\n",
      "Iteration 70, loss = 0.68751681\n",
      "Iteration 71, loss = 0.68705176\n",
      "Iteration 72, loss = 0.68659303\n",
      "Iteration 73, loss = 0.68614055\n",
      "Iteration 74, loss = 0.68569429\n",
      "Iteration 75, loss = 0.68525418\n",
      "Iteration 76, loss = 0.68482019\n",
      "Iteration 77, loss = 0.68439226\n",
      "Iteration 78, loss = 0.68397035\n",
      "Iteration 79, loss = 0.68355440\n",
      "Iteration 80, loss = 0.68314436\n",
      "Iteration 81, loss = 0.68274018\n",
      "Iteration 82, loss = 0.68234181\n",
      "Iteration 83, loss = 0.68194921\n",
      "Iteration 84, loss = 0.68156232\n",
      "Iteration 85, loss = 0.68118109\n",
      "Iteration 86, loss = 0.68080547\n",
      "Iteration 87, loss = 0.68043541\n",
      "Iteration 88, loss = 0.68007087\n",
      "Iteration 89, loss = 0.67971177\n",
      "Iteration 90, loss = 0.67935809\n",
      "Iteration 91, loss = 0.67900976\n",
      "Iteration 92, loss = 0.67866674\n",
      "Iteration 93, loss = 0.67832897\n",
      "Iteration 94, loss = 0.67799641\n",
      "Iteration 95, loss = 0.67766899\n",
      "Iteration 96, loss = 0.67734668\n",
      "Iteration 97, loss = 0.67702942\n",
      "Iteration 98, loss = 0.67671715\n",
      "Iteration 99, loss = 0.67640984\n",
      "Iteration 100, loss = 0.67610742\n",
      "Iteration 1, loss = 0.77169570\n",
      "Iteration 2, loss = 0.56573062\n",
      "Iteration 3, loss = 0.54419478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.53487477\n",
      "Iteration 5, loss = 0.52559285\n",
      "Iteration 6, loss = 0.52529305\n",
      "Iteration 7, loss = 0.52512212\n",
      "Iteration 8, loss = 0.52494611\n",
      "Iteration 9, loss = 0.52225051\n",
      "Iteration 10, loss = 0.51665763\n",
      "Iteration 11, loss = 0.51503017\n",
      "Iteration 12, loss = 0.51481938\n",
      "Iteration 13, loss = 0.51461627\n",
      "Iteration 14, loss = 0.51441462\n",
      "Iteration 15, loss = 0.51027920\n",
      "Iteration 16, loss = 0.49737929\n",
      "Iteration 17, loss = 0.47723512\n",
      "Iteration 18, loss = 0.47544921\n",
      "Iteration 19, loss = 0.47512983\n",
      "Iteration 20, loss = 0.50213273\n",
      "Iteration 21, loss = 0.47452865\n",
      "Iteration 22, loss = 0.47422938\n",
      "Iteration 23, loss = 0.47393591\n",
      "Iteration 24, loss = 0.47375078\n",
      "Iteration 25, loss = 0.47532611\n",
      "Iteration 26, loss = 0.47556691\n",
      "Iteration 27, loss = 0.47299039\n",
      "Iteration 28, loss = 0.47251555\n",
      "Iteration 29, loss = 0.47222725\n",
      "Iteration 30, loss = 0.47195593\n",
      "Iteration 31, loss = 0.47169006\n",
      "Iteration 32, loss = 0.47142876\n",
      "Iteration 33, loss = 0.47117201\n",
      "Iteration 34, loss = 0.47091986\n",
      "Iteration 35, loss = 0.47067233\n",
      "Iteration 36, loss = 0.47042947\n",
      "Iteration 37, loss = 0.47019126\n",
      "Iteration 38, loss = 0.46995772\n",
      "Iteration 39, loss = 0.46972884\n",
      "Iteration 40, loss = 0.46950458\n",
      "Iteration 41, loss = 0.46928493\n",
      "Iteration 42, loss = 0.46906984\n",
      "Iteration 43, loss = 0.46885928\n",
      "Iteration 44, loss = 0.46865321\n",
      "Iteration 45, loss = 0.46845156\n",
      "Iteration 46, loss = 0.46825429\n",
      "Iteration 47, loss = 0.46806135\n",
      "Iteration 48, loss = 0.46787266\n",
      "Iteration 49, loss = 0.46768818\n",
      "Iteration 50, loss = 0.46750784\n",
      "Iteration 51, loss = 0.46733157\n",
      "Iteration 52, loss = 0.46715931\n",
      "Iteration 53, loss = 0.46699100\n",
      "Iteration 54, loss = 0.46682656\n",
      "Iteration 55, loss = 0.46666594\n",
      "Iteration 56, loss = 0.46650906\n",
      "Iteration 57, loss = 0.46635585\n",
      "Iteration 58, loss = 0.46620626\n",
      "Iteration 59, loss = 0.46606022\n",
      "Iteration 60, loss = 0.46591765\n",
      "Iteration 61, loss = 0.46577849\n",
      "Iteration 62, loss = 0.46564268\n",
      "Iteration 63, loss = 0.46551016\n",
      "Iteration 64, loss = 0.46538085\n",
      "Iteration 65, loss = 0.46525470\n",
      "Iteration 66, loss = 0.46513165\n",
      "Iteration 67, loss = 0.46501162\n",
      "Iteration 68, loss = 0.46489457\n",
      "Iteration 69, loss = 0.46478043\n",
      "Iteration 70, loss = 0.46466915\n",
      "Iteration 71, loss = 0.46456065\n",
      "Iteration 72, loss = 0.46445490\n",
      "Iteration 73, loss = 0.46435182\n",
      "Iteration 74, loss = 0.46425137\n",
      "Iteration 75, loss = 0.46415349\n",
      "Iteration 76, loss = 0.46405813\n",
      "Iteration 77, loss = 0.46396523\n",
      "Iteration 78, loss = 0.46387474\n",
      "Iteration 79, loss = 0.46378661\n",
      "Iteration 80, loss = 0.46370079\n",
      "Iteration 81, loss = 0.46361723\n",
      "Iteration 82, loss = 0.46353588\n",
      "Iteration 83, loss = 0.46345670\n",
      "Iteration 84, loss = 0.46337963\n",
      "Iteration 85, loss = 0.46330463\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72863553\n",
      "Iteration 2, loss = 0.66602691\n",
      "Iteration 3, loss = 0.60870421\n",
      "Iteration 4, loss = 0.58503143\n",
      "Iteration 5, loss = 0.56289265\n",
      "Iteration 6, loss = 0.55650195\n",
      "Iteration 7, loss = 0.54145016\n",
      "Iteration 8, loss = 0.52763117\n",
      "Iteration 9, loss = 0.54327488\n",
      "Iteration 10, loss = 0.52901312\n",
      "Iteration 11, loss = 0.51635462\n",
      "Iteration 12, loss = 0.51647862\n",
      "Iteration 13, loss = 0.51658680\n",
      "Iteration 14, loss = 0.51613541\n",
      "Iteration 15, loss = 0.51496033\n",
      "Iteration 16, loss = 0.51346684\n",
      "Iteration 17, loss = 0.51207953\n",
      "Iteration 18, loss = 0.51092619\n",
      "Iteration 19, loss = 0.50996141\n",
      "Iteration 20, loss = 0.50911402\n",
      "Iteration 21, loss = 0.50833383\n",
      "Iteration 22, loss = 0.50759189\n",
      "Iteration 23, loss = 0.50687259\n",
      "Iteration 24, loss = 0.50616765\n",
      "Iteration 25, loss = 0.50547264\n",
      "Iteration 26, loss = 0.50478518\n",
      "Iteration 27, loss = 0.50410398\n",
      "Iteration 28, loss = 0.50342833\n",
      "Iteration 29, loss = 0.50275784\n",
      "Iteration 30, loss = 0.50209232\n",
      "Iteration 31, loss = 0.50143168\n",
      "Iteration 32, loss = 0.50077588\n",
      "Iteration 33, loss = 0.50012493\n",
      "Iteration 34, loss = 0.49947883\n",
      "Iteration 35, loss = 0.49883762\n",
      "Iteration 36, loss = 0.49820132\n",
      "Iteration 37, loss = 0.49756998\n",
      "Iteration 38, loss = 0.49694361\n",
      "Iteration 39, loss = 0.49632225\n",
      "Iteration 40, loss = 0.49570592\n",
      "Iteration 41, loss = 0.49509464\n",
      "Iteration 42, loss = 0.49448844\n",
      "Iteration 43, loss = 0.49388732\n",
      "Iteration 44, loss = 0.49329130\n",
      "Iteration 45, loss = 0.49270040\n",
      "Iteration 46, loss = 0.49211461\n",
      "Iteration 47, loss = 0.49153395\n",
      "Iteration 48, loss = 0.49095841\n",
      "Iteration 49, loss = 0.49038800\n",
      "Iteration 50, loss = 0.48982272\n",
      "Iteration 51, loss = 0.48926256\n",
      "Iteration 52, loss = 0.48870752\n",
      "Iteration 53, loss = 0.48815758\n",
      "Iteration 54, loss = 0.48761276\n",
      "Iteration 55, loss = 0.48707303\n",
      "Iteration 56, loss = 0.48653839\n",
      "Iteration 57, loss = 0.48600882\n",
      "Iteration 58, loss = 0.48548432\n",
      "Iteration 59, loss = 0.48496487\n",
      "Iteration 60, loss = 0.48445046\n",
      "Iteration 61, loss = 0.48394107\n",
      "Iteration 62, loss = 0.48343670\n",
      "Iteration 63, loss = 0.48293732\n",
      "Iteration 64, loss = 0.48244293\n",
      "Iteration 65, loss = 0.48195349\n",
      "Iteration 66, loss = 0.48146901\n",
      "Iteration 67, loss = 0.48098945\n",
      "Iteration 68, loss = 0.48051481\n",
      "Iteration 69, loss = 0.48004507\n",
      "Iteration 70, loss = 0.47958020\n",
      "Iteration 71, loss = 0.47912018\n",
      "Iteration 72, loss = 0.47866501\n",
      "Iteration 73, loss = 0.47821466\n",
      "Iteration 74, loss = 0.47776911\n",
      "Iteration 75, loss = 0.47732833\n",
      "Iteration 76, loss = 0.47689232\n",
      "Iteration 77, loss = 0.47646104\n",
      "Iteration 78, loss = 0.47603449\n",
      "Iteration 79, loss = 0.47561263\n",
      "Iteration 80, loss = 0.47519545\n",
      "Iteration 81, loss = 0.47478292\n",
      "Iteration 82, loss = 0.47437502\n",
      "Iteration 83, loss = 0.47397173\n",
      "Iteration 84, loss = 0.47357303\n",
      "Iteration 85, loss = 0.47317889\n",
      "Iteration 86, loss = 0.47278929\n",
      "Iteration 87, loss = 0.47240421\n",
      "Iteration 88, loss = 0.47202362\n",
      "Iteration 89, loss = 0.47164751\n",
      "Iteration 90, loss = 0.47127583\n",
      "Iteration 91, loss = 0.47090858\n",
      "Iteration 92, loss = 0.47054573\n",
      "Iteration 93, loss = 0.47018724\n",
      "Iteration 94, loss = 0.46983310\n",
      "Iteration 95, loss = 0.46948328\n",
      "Iteration 96, loss = 0.46913776\n",
      "Iteration 97, loss = 0.46879650\n",
      "Iteration 98, loss = 0.46845948\n",
      "Iteration 99, loss = 0.46812668\n",
      "Iteration 100, loss = 0.46779807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 9.21234119\n",
      "Iteration 2, loss = 6.21824195\n",
      "Iteration 3, loss = 2.32764792\n",
      "Iteration 4, loss = 2.39873701\n",
      "Iteration 5, loss = 2.38157134\n",
      "Iteration 6, loss = 1.62830137\n",
      "Iteration 7, loss = 1.01695149\n",
      "Iteration 8, loss = 1.12325203\n",
      "Iteration 9, loss = 0.46851575\n",
      "Iteration 10, loss = 0.23331721\n",
      "Iteration 11, loss = 0.26325195\n",
      "Iteration 12, loss = 0.15316586\n",
      "Iteration 13, loss = 0.18369952\n",
      "Iteration 14, loss = 0.11211996\n",
      "Iteration 15, loss = 0.06502578\n",
      "Iteration 16, loss = 0.04906428\n",
      "Iteration 17, loss = 0.04121603\n",
      "Iteration 18, loss = 0.03369286\n",
      "Iteration 19, loss = 0.02680019\n",
      "Iteration 20, loss = 0.02132385\n",
      "Iteration 21, loss = 0.01709966\n",
      "Iteration 22, loss = 0.01413904\n",
      "Iteration 23, loss = 0.01236078\n",
      "Iteration 24, loss = 0.01151969\n",
      "Iteration 25, loss = 0.01122764\n",
      "Iteration 26, loss = 0.01126029\n",
      "Iteration 27, loss = 0.01134897\n",
      "Iteration 28, loss = 0.01134107\n",
      "Iteration 29, loss = 0.01112146\n",
      "Iteration 30, loss = 0.01065086\n",
      "Iteration 31, loss = 0.01001418\n",
      "Iteration 32, loss = 0.00924600\n",
      "Iteration 33, loss = 0.00836664\n",
      "Iteration 34, loss = 0.00749073\n",
      "Iteration 35, loss = 0.00651598\n",
      "Iteration 36, loss = 0.00558805\n",
      "Iteration 37, loss = 0.00481357\n",
      "Iteration 38, loss = 0.00420584\n",
      "Iteration 39, loss = 0.00372975\n",
      "Iteration 40, loss = 0.00339502\n",
      "Iteration 41, loss = 0.00314174\n",
      "Iteration 42, loss = 0.00276583\n",
      "Iteration 43, loss = 0.00248640\n",
      "Iteration 44, loss = 0.00229390\n",
      "Iteration 45, loss = 0.00215925\n",
      "Iteration 46, loss = 0.00205367\n",
      "Iteration 47, loss = 0.00199834\n",
      "Iteration 48, loss = 0.00194622\n",
      "Iteration 49, loss = 0.00189594\n",
      "Iteration 50, loss = 0.00184936\n",
      "Iteration 51, loss = 0.00180726\n",
      "Iteration 52, loss = 0.00176951\n",
      "Iteration 53, loss = 0.00173595\n",
      "Iteration 54, loss = 0.00170605\n",
      "Iteration 55, loss = 0.00167912\n",
      "Iteration 56, loss = 0.00165449\n",
      "Iteration 57, loss = 0.00163158\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.93108294\n",
      "Iteration 2, loss = 7.21773544\n",
      "Iteration 3, loss = 1.79137278\n",
      "Iteration 4, loss = 2.65468684\n",
      "Iteration 5, loss = 3.05858919\n",
      "Iteration 6, loss = 2.87862989\n",
      "Iteration 7, loss = 2.09292839\n",
      "Iteration 8, loss = 1.25442229\n",
      "Iteration 9, loss = 0.86715074\n",
      "Iteration 10, loss = 0.73814721\n",
      "Iteration 11, loss = 0.57661555\n",
      "Iteration 12, loss = 0.52133192\n",
      "Iteration 13, loss = 0.65902999\n",
      "Iteration 14, loss = 0.77353836\n",
      "Iteration 15, loss = 0.75940219\n",
      "Iteration 16, loss = 0.60597809\n",
      "Iteration 17, loss = 0.41616928\n",
      "Iteration 18, loss = 0.28682068\n",
      "Iteration 19, loss = 0.24111519\n",
      "Iteration 20, loss = 0.24822651\n",
      "Iteration 21, loss = 0.27615141\n",
      "Iteration 22, loss = 0.30048150\n",
      "Iteration 23, loss = 0.30288597\n",
      "Iteration 24, loss = 0.27869155\n",
      "Iteration 25, loss = 0.24794897\n",
      "Iteration 26, loss = 0.21633547\n",
      "Iteration 27, loss = 0.19091904\n",
      "Iteration 28, loss = 0.17686368\n",
      "Iteration 29, loss = 0.17594847\n",
      "Iteration 30, loss = 0.18435417\n",
      "Iteration 31, loss = 0.19458821\n",
      "Iteration 32, loss = 0.19710201\n",
      "Iteration 33, loss = 0.18960130\n",
      "Iteration 34, loss = 0.16799995\n",
      "Iteration 35, loss = 0.14082591\n",
      "Iteration 36, loss = 0.12050386\n",
      "Iteration 37, loss = 0.11170899\n",
      "Iteration 38, loss = 0.10886846\n",
      "Iteration 39, loss = 0.10938262\n",
      "Iteration 40, loss = 0.10931378\n",
      "Iteration 41, loss = 0.10893888\n",
      "Iteration 42, loss = 0.10617783\n",
      "Iteration 43, loss = 0.09982439\n",
      "Iteration 44, loss = 0.09282025\n",
      "Iteration 45, loss = 0.08670030\n",
      "Iteration 46, loss = 0.08208482\n",
      "Iteration 47, loss = 0.07887770\n",
      "Iteration 48, loss = 0.07659834\n",
      "Iteration 49, loss = 0.07475131\n",
      "Iteration 50, loss = 0.07300789\n",
      "Iteration 51, loss = 0.07110777\n",
      "Iteration 52, loss = 0.06893523\n",
      "Iteration 53, loss = 0.06655297\n",
      "Iteration 54, loss = 0.06409860\n",
      "Iteration 55, loss = 0.06172618\n",
      "Iteration 56, loss = 0.05860169\n",
      "Iteration 57, loss = 0.05409567\n",
      "Iteration 58, loss = 0.04875548\n",
      "Iteration 59, loss = 0.04400599\n",
      "Iteration 60, loss = 0.04056955\n",
      "Iteration 61, loss = 0.03852350\n",
      "Iteration 62, loss = 0.03690111\n",
      "Iteration 63, loss = 0.03496718\n",
      "Iteration 64, loss = 0.03265980\n",
      "Iteration 65, loss = 0.03036646\n",
      "Iteration 66, loss = 0.02846464\n",
      "Iteration 67, loss = 0.02701317\n",
      "Iteration 68, loss = 0.02601152\n",
      "Iteration 69, loss = 0.02541270\n",
      "Iteration 70, loss = 0.02498907\n",
      "Iteration 71, loss = 0.02462368\n",
      "Iteration 72, loss = 0.02422927\n",
      "Iteration 73, loss = 0.02375929\n",
      "Iteration 74, loss = 0.02320115\n",
      "Iteration 75, loss = 0.02256397\n",
      "Iteration 76, loss = 0.02187128\n",
      "Iteration 77, loss = 0.02117302\n",
      "Iteration 78, loss = 0.02049339\n",
      "Iteration 79, loss = 0.01984172\n",
      "Iteration 80, loss = 0.01923163\n",
      "Iteration 81, loss = 0.01867848\n",
      "Iteration 82, loss = 0.01819133\n",
      "Iteration 83, loss = 0.01777237\n",
      "Iteration 84, loss = 0.01745839\n",
      "Iteration 85, loss = 0.01716090\n",
      "Iteration 86, loss = 0.01686149\n",
      "Iteration 87, loss = 0.01654764\n",
      "Iteration 88, loss = 0.01621737\n",
      "Iteration 89, loss = 0.01587361\n",
      "Iteration 90, loss = 0.01552520\n",
      "Iteration 91, loss = 0.01517724\n",
      "Iteration 92, loss = 0.01483726\n",
      "Iteration 93, loss = 0.01450812\n",
      "Iteration 94, loss = 0.01419908\n",
      "Iteration 95, loss = 0.01390946\n",
      "Iteration 96, loss = 0.01364345\n",
      "Iteration 97, loss = 0.01339670\n",
      "Iteration 98, loss = 0.01317629\n",
      "Iteration 99, loss = 0.01296957\n",
      "Iteration 100, loss = 0.01276835\n",
      "Iteration 1, loss = 3.81180596\n",
      "Iteration 2, loss = 4.84982253\n",
      "Iteration 3, loss = 1.79981167\n",
      "Iteration 4, loss = 0.86062356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.43893326\n",
      "Iteration 6, loss = 0.38726315\n",
      "Iteration 7, loss = 0.38766218\n",
      "Iteration 8, loss = 0.39457637\n",
      "Iteration 9, loss = 0.41755027\n",
      "Iteration 10, loss = 0.42340765\n",
      "Iteration 11, loss = 0.42452659\n",
      "Iteration 12, loss = 0.40649570\n",
      "Iteration 13, loss = 0.38726191\n",
      "Iteration 14, loss = 0.37083458\n",
      "Iteration 15, loss = 0.35945395\n",
      "Iteration 16, loss = 0.34489955\n",
      "Iteration 17, loss = 0.33860550\n",
      "Iteration 18, loss = 0.33049752\n",
      "Iteration 19, loss = 0.32660405\n",
      "Iteration 20, loss = 0.31650096\n",
      "Iteration 21, loss = 0.31396033\n",
      "Iteration 22, loss = 0.31705243\n",
      "Iteration 23, loss = 0.31491805\n",
      "Iteration 24, loss = 0.31445166\n",
      "Iteration 25, loss = 0.30772214\n",
      "Iteration 26, loss = 0.29930654\n",
      "Iteration 27, loss = 0.29244866\n",
      "Iteration 28, loss = 0.28662742\n",
      "Iteration 29, loss = 0.28373808\n",
      "Iteration 30, loss = 0.27737363\n",
      "Iteration 31, loss = 0.27363677\n",
      "Iteration 32, loss = 0.26976182\n",
      "Iteration 33, loss = 0.26609002\n",
      "Iteration 34, loss = 0.26284693\n",
      "Iteration 35, loss = 0.26129466\n",
      "Iteration 36, loss = 0.25494854\n",
      "Iteration 37, loss = 0.25139220\n",
      "Iteration 38, loss = 0.24870838\n",
      "Iteration 39, loss = 0.24632437\n",
      "Iteration 40, loss = 0.24545284\n",
      "Iteration 41, loss = 0.24387141\n",
      "Iteration 42, loss = 0.24196577\n",
      "Iteration 43, loss = 0.23901503\n",
      "Iteration 44, loss = 0.23724900\n",
      "Iteration 45, loss = 0.23580908\n",
      "Iteration 46, loss = 0.23344968\n",
      "Iteration 47, loss = 0.23034678\n",
      "Iteration 48, loss = 0.22833628\n",
      "Iteration 49, loss = 0.22756451\n",
      "Iteration 50, loss = 0.22664300\n",
      "Iteration 51, loss = 0.22561142\n",
      "Iteration 52, loss = 0.22450822\n",
      "Iteration 53, loss = 0.22337981\n",
      "Iteration 54, loss = 0.22314146\n",
      "Iteration 55, loss = 0.22230014\n",
      "Iteration 56, loss = 0.22097281\n",
      "Iteration 57, loss = 0.22038355\n",
      "Iteration 58, loss = 0.21972517\n",
      "Iteration 59, loss = 0.21901199\n",
      "Iteration 60, loss = 0.21826009\n",
      "Iteration 61, loss = 0.21748544\n",
      "Iteration 62, loss = 0.21670244\n",
      "Iteration 63, loss = 0.21592297\n",
      "Iteration 64, loss = 0.21629451\n",
      "Iteration 65, loss = 0.20845530\n",
      "Iteration 66, loss = 0.20698457\n",
      "Iteration 67, loss = 0.20556963\n",
      "Iteration 68, loss = 0.20506225\n",
      "Iteration 69, loss = 0.20328145\n",
      "Iteration 70, loss = 0.20292964\n",
      "Iteration 71, loss = 0.20235506\n",
      "Iteration 72, loss = 0.20095115\n",
      "Iteration 73, loss = 0.20023276\n",
      "Iteration 74, loss = 0.19943238\n",
      "Iteration 75, loss = 0.19857356\n",
      "Iteration 76, loss = 0.19768062\n",
      "Iteration 77, loss = 0.19677450\n",
      "Iteration 78, loss = 0.19587331\n",
      "Iteration 79, loss = 0.19511227\n",
      "Iteration 80, loss = 0.19472567\n",
      "Iteration 81, loss = 0.19374525\n",
      "Iteration 82, loss = 0.19315880\n",
      "Iteration 83, loss = 0.19255209\n",
      "Iteration 84, loss = 0.19211586\n",
      "Iteration 85, loss = 0.19147249\n",
      "Iteration 86, loss = 0.19098426\n",
      "Iteration 87, loss = 0.19047045\n",
      "Iteration 88, loss = 0.18993517\n",
      "Iteration 89, loss = 0.18938324\n",
      "Iteration 90, loss = 0.18881970\n",
      "Iteration 91, loss = 0.18824944\n",
      "Iteration 92, loss = 0.18767688\n",
      "Iteration 93, loss = 0.18710582\n",
      "Iteration 94, loss = 0.18688933\n",
      "Iteration 95, loss = 0.18638718\n",
      "Iteration 96, loss = 0.18566803\n",
      "Iteration 97, loss = 0.18528057\n",
      "Iteration 98, loss = 0.18487505\n",
      "Iteration 99, loss = 0.18445218\n",
      "Iteration 100, loss = 0.18401346\n",
      "Iteration 1, loss = 2.85444253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 7.64124303\n",
      "Iteration 3, loss = 2.84706512\n",
      "Iteration 4, loss = 2.14006786\n",
      "Iteration 5, loss = 1.19718079\n",
      "Iteration 6, loss = 1.46743916\n",
      "Iteration 7, loss = 1.56235899\n",
      "Iteration 8, loss = 1.18496589\n",
      "Iteration 9, loss = 0.66009620\n",
      "Iteration 10, loss = 0.48719760\n",
      "Iteration 11, loss = 0.57273500\n",
      "Iteration 12, loss = 0.36490031\n",
      "Iteration 13, loss = 0.20703461\n",
      "Iteration 14, loss = 0.14938027\n",
      "Iteration 15, loss = 0.12801800\n",
      "Iteration 16, loss = 0.09315619\n",
      "Iteration 17, loss = 0.05371892\n",
      "Iteration 18, loss = 0.04152806\n",
      "Iteration 19, loss = 0.03719274\n",
      "Iteration 20, loss = 0.03892885\n",
      "Iteration 21, loss = 0.04064947\n",
      "Iteration 22, loss = 0.03448268\n",
      "Iteration 23, loss = 0.02646892\n",
      "Iteration 24, loss = 0.02257913\n",
      "Iteration 25, loss = 0.02119082\n",
      "Iteration 26, loss = 0.02014087\n",
      "Iteration 27, loss = 0.01863455\n",
      "Iteration 28, loss = 0.01666472\n",
      "Iteration 29, loss = 0.01457863\n",
      "Iteration 30, loss = 0.01273887\n",
      "Iteration 31, loss = 0.01147741\n",
      "Iteration 32, loss = 0.01051518\n",
      "Iteration 33, loss = 0.00977685\n",
      "Iteration 34, loss = 0.00918289\n",
      "Iteration 35, loss = 0.00866684\n",
      "Iteration 36, loss = 0.00818379\n",
      "Iteration 37, loss = 0.00770951\n",
      "Iteration 38, loss = 0.00723379\n",
      "Iteration 39, loss = 0.00676399\n",
      "Iteration 40, loss = 0.00631014\n",
      "Iteration 41, loss = 0.00588332\n",
      "Iteration 42, loss = 0.00549313\n",
      "Iteration 43, loss = 0.00514428\n",
      "Iteration 44, loss = 0.00483718\n",
      "Iteration 45, loss = 0.00457318\n",
      "Iteration 46, loss = 0.00434893\n",
      "Iteration 47, loss = 0.00415931\n",
      "Iteration 48, loss = 0.00399832\n",
      "Iteration 49, loss = 0.00384795\n",
      "Iteration 50, loss = 0.00371347\n",
      "Iteration 51, loss = 0.00359121\n",
      "Iteration 52, loss = 0.00347704\n",
      "Iteration 53, loss = 0.00337367\n",
      "Iteration 54, loss = 0.00327378\n",
      "Iteration 55, loss = 0.00317602\n",
      "Iteration 56, loss = 0.00308001\n",
      "Iteration 57, loss = 0.00298603\n",
      "Iteration 58, loss = 0.00289670\n",
      "Iteration 59, loss = 0.00281443\n",
      "Iteration 60, loss = 0.00273600\n",
      "Iteration 61, loss = 0.00266206\n",
      "Iteration 62, loss = 0.00259305\n",
      "Iteration 63, loss = 0.00252917\n",
      "Iteration 64, loss = 0.00247041\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.48083425\n",
      "Iteration 2, loss = 13.58758482\n",
      "Iteration 3, loss = 8.80350585\n",
      "Iteration 4, loss = 1.41427540\n",
      "Iteration 5, loss = 1.03378518\n",
      "Iteration 6, loss = 0.83181829\n",
      "Iteration 7, loss = 0.71123626\n",
      "Iteration 8, loss = 0.67681399\n",
      "Iteration 9, loss = 0.67775096\n",
      "Iteration 10, loss = 0.67849002\n",
      "Iteration 11, loss = 0.67845301\n",
      "Iteration 12, loss = 0.67841995\n",
      "Iteration 13, loss = 0.67839023\n",
      "Iteration 14, loss = 0.67836338\n",
      "Iteration 15, loss = 0.67833900\n",
      "Iteration 16, loss = 0.67831676\n",
      "Iteration 17, loss = 0.67829640\n",
      "Iteration 18, loss = 0.67827768\n",
      "Iteration 19, loss = 0.67826041\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 10.46899067\n",
      "Iteration 2, loss = 17.25669587\n",
      "Iteration 3, loss = 9.42414675\n",
      "Iteration 4, loss = 5.00455350\n",
      "Iteration 5, loss = 1.87863022\n",
      "Iteration 6, loss = 1.02462060\n",
      "Iteration 7, loss = 0.95424870\n",
      "Iteration 8, loss = 1.27802161\n",
      "Iteration 9, loss = 1.36931517\n",
      "Iteration 10, loss = 1.20299472\n",
      "Iteration 11, loss = 0.90076158\n",
      "Iteration 12, loss = 0.59858296\n",
      "Iteration 13, loss = 0.46434220\n",
      "Iteration 14, loss = 0.56198396\n",
      "Iteration 15, loss = 0.64677455\n",
      "Iteration 16, loss = 0.55306692\n",
      "Iteration 17, loss = 0.36061472\n",
      "Iteration 18, loss = 0.24932189\n",
      "Iteration 19, loss = 0.24847337\n",
      "Iteration 20, loss = 0.29037222\n",
      "Iteration 21, loss = 0.30469910\n",
      "Iteration 22, loss = 0.26291309\n",
      "Iteration 23, loss = 0.19149843\n",
      "Iteration 24, loss = 0.14433947\n",
      "Iteration 25, loss = 0.13424034\n",
      "Iteration 26, loss = 0.14009559\n",
      "Iteration 27, loss = 0.14914471\n",
      "Iteration 28, loss = 0.15169963\n",
      "Iteration 29, loss = 0.14306549\n",
      "Iteration 30, loss = 0.12637564\n",
      "Iteration 31, loss = 0.10917517\n",
      "Iteration 32, loss = 0.09710108\n",
      "Iteration 33, loss = 0.09097393\n",
      "Iteration 34, loss = 0.08864854\n",
      "Iteration 35, loss = 0.08750090\n",
      "Iteration 36, loss = 0.08549470\n",
      "Iteration 37, loss = 0.08166628\n",
      "Iteration 38, loss = 0.07607444\n",
      "Iteration 39, loss = 0.06958949\n",
      "Iteration 40, loss = 0.06333035\n",
      "Iteration 41, loss = 0.05815231\n",
      "Iteration 42, loss = 0.05438638\n",
      "Iteration 43, loss = 0.05190806\n",
      "Iteration 44, loss = 0.05030739\n",
      "Iteration 45, loss = 0.04908237\n",
      "Iteration 46, loss = 0.04781429\n",
      "Iteration 47, loss = 0.04623375\n",
      "Iteration 48, loss = 0.04427911\n",
      "Iteration 49, loss = 0.04205418\n",
      "Iteration 50, loss = 0.03973916\n",
      "Iteration 51, loss = 0.03753482\n",
      "Iteration 52, loss = 0.03557971\n",
      "Iteration 53, loss = 0.03394728\n",
      "Iteration 54, loss = 0.03264332\n",
      "Iteration 55, loss = 0.03161703\n",
      "Iteration 56, loss = 0.03078038\n",
      "Iteration 57, loss = 0.03003173\n",
      "Iteration 58, loss = 0.02928094\n",
      "Iteration 59, loss = 0.02846986\n",
      "Iteration 60, loss = 0.02758269\n",
      "Iteration 61, loss = 0.02664246\n",
      "Iteration 62, loss = 0.02569360\n",
      "Iteration 63, loss = 0.02478367\n",
      "Iteration 64, loss = 0.02395517\n",
      "Iteration 65, loss = 0.02321722\n",
      "Iteration 66, loss = 0.02256934\n",
      "Iteration 67, loss = 0.02199851\n",
      "Iteration 68, loss = 0.02148488\n",
      "Iteration 69, loss = 0.02100714\n",
      "Iteration 70, loss = 0.02054722\n",
      "Iteration 71, loss = 0.02009267\n",
      "Iteration 72, loss = 0.01963689\n",
      "Iteration 73, loss = 0.01917905\n",
      "Iteration 74, loss = 0.01872268\n",
      "Iteration 75, loss = 0.01827301\n",
      "Iteration 76, loss = 0.01783689\n",
      "Iteration 77, loss = 0.01741993\n",
      "Iteration 78, loss = 0.01702583\n",
      "Iteration 79, loss = 0.01665520\n",
      "Iteration 80, loss = 0.01630626\n",
      "Iteration 81, loss = 0.01597804\n",
      "Iteration 82, loss = 0.01566685\n",
      "Iteration 83, loss = 0.01536870\n",
      "Iteration 84, loss = 0.01507998\n",
      "Iteration 85, loss = 0.01479796\n",
      "Iteration 86, loss = 0.01452108\n",
      "Iteration 87, loss = 0.01424888\n",
      "Iteration 88, loss = 0.01398034\n",
      "Iteration 89, loss = 0.01371615\n",
      "Iteration 90, loss = 0.01345885\n",
      "Iteration 91, loss = 0.01320943\n",
      "Iteration 92, loss = 0.01296931\n",
      "Iteration 93, loss = 0.01273910\n",
      "Iteration 94, loss = 0.01251699\n",
      "Iteration 95, loss = 0.01230248\n",
      "Iteration 96, loss = 0.01209488\n",
      "Iteration 97, loss = 0.01189414\n",
      "Iteration 98, loss = 0.01170117\n",
      "Iteration 99, loss = 0.01151247\n",
      "Iteration 100, loss = 0.01132763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 5.67768096\n",
      "Iteration 2, loss = 6.78287041\n",
      "Iteration 3, loss = 3.28626173\n",
      "Iteration 4, loss = 5.00097025\n",
      "Iteration 5, loss = 2.44063544\n",
      "Iteration 6, loss = 0.24354698\n",
      "Iteration 7, loss = 0.55630849\n",
      "Iteration 8, loss = 0.12587447\n",
      "Iteration 9, loss = 0.07769611\n",
      "Iteration 10, loss = 0.16008668\n",
      "Iteration 11, loss = 0.09442123\n",
      "Iteration 12, loss = 0.00957337\n",
      "Iteration 13, loss = 0.02482890\n",
      "Iteration 14, loss = 0.03723705\n",
      "Iteration 15, loss = 0.02726619\n",
      "Iteration 16, loss = 0.00665253\n",
      "Iteration 17, loss = 0.00134808\n",
      "Iteration 18, loss = 0.00221824\n",
      "Iteration 19, loss = 0.00267777\n",
      "Iteration 20, loss = 0.00157345\n",
      "Iteration 21, loss = 0.00066364\n",
      "Iteration 22, loss = 0.00032822\n",
      "Iteration 23, loss = 0.00022733\n",
      "Iteration 24, loss = 0.00019709\n",
      "Iteration 25, loss = 0.00018780\n",
      "Iteration 26, loss = 0.00018507\n",
      "Iteration 27, loss = 0.00018454\n",
      "Iteration 28, loss = 0.00018487\n",
      "Iteration 29, loss = 0.00018552\n",
      "Iteration 30, loss = 0.00018629\n",
      "Iteration 31, loss = 0.00018708\n",
      "Iteration 32, loss = 0.00018785\n",
      "Iteration 33, loss = 0.00018857\n",
      "Iteration 34, loss = 0.00018923\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.14081441\n",
      "Iteration 2, loss = 4.54976140\n",
      "Iteration 3, loss = 6.75675333\n",
      "Iteration 4, loss = 0.72458325\n",
      "Iteration 5, loss = 0.42397901\n",
      "Iteration 6, loss = 0.44709656\n",
      "Iteration 7, loss = 0.45588129\n",
      "Iteration 8, loss = 0.45150326\n",
      "Iteration 9, loss = 0.44564313\n",
      "Iteration 10, loss = 0.44297832\n",
      "Iteration 11, loss = 0.44146489\n",
      "Iteration 12, loss = 0.44056822\n",
      "Iteration 13, loss = 0.43967010\n",
      "Iteration 14, loss = 0.43603034\n",
      "Iteration 15, loss = 0.43231255\n",
      "Iteration 16, loss = 0.42824273\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 6.17911742\n",
      "Iteration 2, loss = 20.24447938\n",
      "Iteration 3, loss = 8.28322426\n",
      "Iteration 4, loss = 6.48367759\n",
      "Iteration 5, loss = 5.42704091\n",
      "Iteration 6, loss = 3.38559845\n",
      "Iteration 7, loss = 2.57855142\n",
      "Iteration 8, loss = 2.48217022\n",
      "Iteration 9, loss = 0.63377003\n",
      "Iteration 10, loss = 0.93968920\n",
      "Iteration 11, loss = 1.46158672\n",
      "Iteration 12, loss = 0.72824653\n",
      "Iteration 13, loss = 0.20068461\n",
      "Iteration 14, loss = 0.08556190\n",
      "Iteration 15, loss = 0.22327743\n",
      "Iteration 16, loss = 0.43449539\n",
      "Iteration 17, loss = 0.30712309\n",
      "Iteration 18, loss = 0.16294167\n",
      "Iteration 19, loss = 0.04932080\n",
      "Iteration 20, loss = 0.00064346\n",
      "Iteration 21, loss = 0.00044909\n",
      "Iteration 22, loss = 0.00246444\n",
      "Iteration 23, loss = 0.01110886\n",
      "Iteration 24, loss = 0.02351107\n",
      "Iteration 25, loss = 0.02827132\n",
      "Iteration 26, loss = 0.02243816\n",
      "Iteration 27, loss = 0.01146748\n",
      "Iteration 28, loss = 0.00383248\n",
      "Iteration 29, loss = 0.00123479\n",
      "Iteration 30, loss = 0.00050531\n",
      "Iteration 31, loss = 0.00028913\n",
      "Iteration 32, loss = 0.00021806\n",
      "Iteration 33, loss = 0.00019116\n",
      "Iteration 34, loss = 0.00017988\n",
      "Iteration 35, loss = 0.00017497\n",
      "Iteration 36, loss = 0.00017304\n",
      "Iteration 37, loss = 0.00017270\n",
      "Iteration 38, loss = 0.00017335\n",
      "Iteration 39, loss = 0.00017473\n",
      "Iteration 40, loss = 0.00017672\n",
      "Iteration 41, loss = 0.00017926\n",
      "Iteration 42, loss = 0.00018230\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 6.32507044\n",
      "Iteration 2, loss = 20.85422500\n",
      "Iteration 3, loss = 12.05163794\n",
      "Iteration 4, loss = 6.39667154\n",
      "Iteration 5, loss = 8.27442166\n",
      "Iteration 6, loss = 8.20315384\n",
      "Iteration 7, loss = 5.45779539\n",
      "Iteration 8, loss = 1.31089206\n",
      "Iteration 9, loss = 3.96744968\n",
      "Iteration 10, loss = 2.81759665\n",
      "Iteration 11, loss = 1.83326976\n",
      "Iteration 12, loss = 0.86988148\n",
      "Iteration 13, loss = 0.56649796\n",
      "Iteration 14, loss = 0.70603479\n",
      "Iteration 15, loss = 0.99359880\n",
      "Iteration 16, loss = 0.85184924\n",
      "Iteration 17, loss = 0.50014276\n",
      "Iteration 18, loss = 0.19906219\n",
      "Iteration 19, loss = 0.02124594\n",
      "Iteration 20, loss = 0.02899134\n",
      "Iteration 21, loss = 0.09654116\n",
      "Iteration 22, loss = 0.18772545\n",
      "Iteration 23, loss = 0.23007686\n",
      "Iteration 24, loss = 0.17159028\n",
      "Iteration 25, loss = 0.08281774\n",
      "Iteration 26, loss = 0.02613542\n",
      "Iteration 27, loss = 0.00524356\n",
      "Iteration 28, loss = 0.00166902\n",
      "Iteration 29, loss = 0.00167871\n",
      "Iteration 30, loss = 0.00644739\n",
      "Iteration 31, loss = 0.01691356\n",
      "Iteration 32, loss = 0.02478166\n",
      "Iteration 33, loss = 0.02658326\n",
      "Iteration 34, loss = 0.02234018\n",
      "Iteration 35, loss = 0.01359288\n",
      "Iteration 36, loss = 0.00499044\n",
      "Iteration 37, loss = 0.00136909\n",
      "Iteration 38, loss = 0.00056479\n",
      "Iteration 39, loss = 0.00039641\n",
      "Iteration 40, loss = 0.00034967\n",
      "Iteration 41, loss = 0.00033278\n",
      "Iteration 42, loss = 0.00032300\n",
      "Iteration 43, loss = 0.00031612\n",
      "Iteration 44, loss = 0.00031095\n",
      "Iteration 45, loss = 0.00030703\n",
      "Iteration 46, loss = 0.00030410\n",
      "Iteration 47, loss = 0.00030193\n",
      "Iteration 48, loss = 0.00030035\n",
      "Iteration 49, loss = 0.00029916\n",
      "Iteration 50, loss = 0.00029820\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77895453\n",
      "Iteration 2, loss = 0.69767747\n",
      "Iteration 3, loss = 0.69680312\n",
      "Iteration 4, loss = 0.69589493\n",
      "Iteration 5, loss = 0.69497902\n",
      "Iteration 6, loss = 0.69406832\n",
      "Iteration 7, loss = 0.69317042\n",
      "Iteration 8, loss = 0.69229030\n",
      "Iteration 9, loss = 0.69143146\n",
      "Iteration 10, loss = 0.69059645\n",
      "Iteration 11, loss = 0.68978718\n",
      "Iteration 12, loss = 0.68900510\n",
      "Iteration 13, loss = 0.68825132\n",
      "Iteration 14, loss = 0.68752664\n",
      "Iteration 15, loss = 0.68683165\n",
      "Iteration 16, loss = 0.68616675\n",
      "Iteration 17, loss = 0.68553216\n",
      "Iteration 18, loss = 0.68492797\n",
      "Iteration 19, loss = 0.68435411\n",
      "Iteration 20, loss = 0.68381043\n",
      "Iteration 21, loss = 0.68329663\n",
      "Iteration 22, loss = 0.68281235\n",
      "Iteration 23, loss = 0.68235712\n",
      "Iteration 24, loss = 0.68193038\n",
      "Iteration 25, loss = 0.68153150\n",
      "Iteration 26, loss = 0.68115979\n",
      "Iteration 27, loss = 0.68081447\n",
      "Iteration 28, loss = 0.68049473\n",
      "Iteration 29, loss = 0.68019970\n",
      "Iteration 30, loss = 0.67992843\n",
      "Iteration 31, loss = 0.67967998\n",
      "Iteration 32, loss = 0.67945333\n",
      "Iteration 33, loss = 0.67924746\n",
      "Iteration 34, loss = 0.67906131\n",
      "Iteration 35, loss = 0.67889380\n",
      "Iteration 36, loss = 0.67874384\n",
      "Iteration 37, loss = 0.67861034\n",
      "Iteration 38, loss = 0.67849221\n",
      "Iteration 39, loss = 0.67838835\n",
      "Iteration 40, loss = 0.67829769\n",
      "Iteration 41, loss = 0.67821916\n",
      "Iteration 42, loss = 0.67815173\n",
      "Iteration 43, loss = 0.67809437\n",
      "Iteration 44, loss = 0.67804612\n",
      "Iteration 45, loss = 0.67800603\n",
      "Iteration 46, loss = 0.67797320\n",
      "Iteration 47, loss = 0.67794678\n",
      "Iteration 48, loss = 0.67792596\n",
      "Iteration 49, loss = 0.67790998\n",
      "Iteration 50, loss = 0.67789815\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94957990\n",
      "Iteration 2, loss = 0.74780467\n",
      "Iteration 3, loss = 0.74620054\n",
      "Iteration 4, loss = 0.74440922\n",
      "Iteration 5, loss = 0.74252690\n",
      "Iteration 6, loss = 0.74060905\n",
      "Iteration 7, loss = 0.73869665\n",
      "Iteration 8, loss = 0.73677766\n",
      "Iteration 9, loss = 0.73486335\n",
      "Iteration 10, loss = 0.73296188\n",
      "Iteration 11, loss = 0.73107792\n",
      "Iteration 12, loss = 0.72921609\n",
      "Iteration 13, loss = 0.72738006\n",
      "Iteration 14, loss = 0.72559332\n",
      "Iteration 15, loss = 0.72384629\n",
      "Iteration 16, loss = 0.72213121\n",
      "Iteration 17, loss = 0.72044965\n",
      "Iteration 18, loss = 0.71880290\n",
      "Iteration 19, loss = 0.71719200\n",
      "Iteration 20, loss = 0.71561779\n",
      "Iteration 21, loss = 0.71408095\n",
      "Iteration 22, loss = 0.71258198\n",
      "Iteration 23, loss = 0.71112126\n",
      "Iteration 24, loss = 0.70969905\n",
      "Iteration 25, loss = 0.70831552\n",
      "Iteration 26, loss = 0.70697074\n",
      "Iteration 27, loss = 0.70566469\n",
      "Iteration 28, loss = 0.70439729\n",
      "Iteration 29, loss = 0.70316840\n",
      "Iteration 30, loss = 0.70197779\n",
      "Iteration 31, loss = 0.70082521\n",
      "Iteration 32, loss = 0.69971034\n",
      "Iteration 33, loss = 0.69863282\n",
      "Iteration 34, loss = 0.69759223\n",
      "Iteration 35, loss = 0.69658813\n",
      "Iteration 36, loss = 0.69562005\n",
      "Iteration 37, loss = 0.69468746\n",
      "Iteration 38, loss = 0.69378981\n",
      "Iteration 39, loss = 0.69292653\n",
      "Iteration 40, loss = 0.69209702\n",
      "Iteration 41, loss = 0.69130066\n",
      "Iteration 42, loss = 0.69053678\n",
      "Iteration 43, loss = 0.68980474\n",
      "Iteration 44, loss = 0.68910384\n",
      "Iteration 45, loss = 0.68843339\n",
      "Iteration 46, loss = 0.68779266\n",
      "Iteration 47, loss = 0.68718095\n",
      "Iteration 48, loss = 0.68659750\n",
      "Iteration 49, loss = 0.68604314\n",
      "Iteration 50, loss = 0.68551763\n",
      "Iteration 51, loss = 0.68501780\n",
      "Iteration 52, loss = 0.68454289\n",
      "Iteration 53, loss = 0.68409215\n",
      "Iteration 54, loss = 0.68366481\n",
      "Iteration 55, loss = 0.68326012\n",
      "Iteration 56, loss = 0.68287767\n",
      "Iteration 57, loss = 0.68251849\n",
      "Iteration 58, loss = 0.68217943\n",
      "Iteration 59, loss = 0.68185975\n",
      "Iteration 60, loss = 0.68155872\n",
      "Iteration 61, loss = 0.68127561\n",
      "Iteration 62, loss = 0.68100970\n",
      "Iteration 63, loss = 0.68076028\n",
      "Iteration 64, loss = 0.68052666\n",
      "Iteration 65, loss = 0.68030814\n",
      "Iteration 66, loss = 0.68010403\n",
      "Iteration 67, loss = 0.67991368\n",
      "Iteration 68, loss = 0.67973643\n",
      "Iteration 69, loss = 0.67957163\n",
      "Iteration 70, loss = 0.67941866\n",
      "Iteration 71, loss = 0.67927692\n",
      "Iteration 72, loss = 0.67914582\n",
      "Iteration 73, loss = 0.67902474\n",
      "Iteration 74, loss = 0.67891312\n",
      "Iteration 75, loss = 0.67881040\n",
      "Iteration 76, loss = 0.67871606\n",
      "Iteration 77, loss = 0.67862958\n",
      "Iteration 78, loss = 0.67855046\n",
      "Iteration 79, loss = 0.67847823\n",
      "Iteration 80, loss = 0.67841243\n",
      "Iteration 81, loss = 0.67835262\n",
      "Iteration 82, loss = 0.67829838\n",
      "Iteration 83, loss = 0.67824930\n",
      "Iteration 84, loss = 0.67820500\n",
      "Iteration 85, loss = 0.67816512\n",
      "Iteration 86, loss = 0.67812931\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.83891577\n",
      "Iteration 2, loss = 0.74000430\n",
      "Iteration 3, loss = 0.73898246\n",
      "Iteration 4, loss = 0.73723333\n",
      "Iteration 5, loss = 0.73537116\n",
      "Iteration 6, loss = 0.73345139\n",
      "Iteration 7, loss = 0.73153344\n",
      "Iteration 8, loss = 0.72960621\n",
      "Iteration 9, loss = 0.72768262\n",
      "Iteration 10, loss = 0.72577229\n",
      "Iteration 11, loss = 0.72388241\n",
      "Iteration 12, loss = 0.72201948\n",
      "Iteration 13, loss = 0.72019501\n",
      "Iteration 14, loss = 0.71840367\n",
      "Iteration 15, loss = 0.71664829\n",
      "Iteration 16, loss = 0.71493116\n",
      "Iteration 17, loss = 0.71325414\n",
      "Iteration 18, loss = 0.71161873\n",
      "Iteration 19, loss = 0.71002614\n",
      "Iteration 20, loss = 0.70847732\n",
      "Iteration 21, loss = 0.70697301\n",
      "Iteration 22, loss = 0.70551374\n",
      "Iteration 23, loss = 0.70410269\n",
      "Iteration 24, loss = 0.70274120\n",
      "Iteration 25, loss = 0.70142491\n",
      "Iteration 26, loss = 0.70015381\n",
      "Iteration 27, loss = 0.69894042\n",
      "Iteration 28, loss = 0.69777661\n",
      "Iteration 29, loss = 0.69665610\n",
      "Iteration 30, loss = 0.69557851\n",
      "Iteration 31, loss = 0.69454338\n",
      "Iteration 32, loss = 0.69356009\n",
      "Iteration 33, loss = 0.69262363\n",
      "Iteration 34, loss = 0.69172662\n",
      "Iteration 35, loss = 0.69086838\n",
      "Iteration 36, loss = 0.69004818\n",
      "Iteration 37, loss = 0.68926527\n",
      "Iteration 38, loss = 0.68851883\n",
      "Iteration 39, loss = 0.68780806\n",
      "Iteration 40, loss = 0.68713208\n",
      "Iteration 41, loss = 0.68649001\n",
      "Iteration 42, loss = 0.68588095\n",
      "Iteration 43, loss = 0.68530398\n",
      "Iteration 44, loss = 0.68475814\n",
      "Iteration 45, loss = 0.68424249\n",
      "Iteration 46, loss = 0.68375604\n",
      "Iteration 47, loss = 0.68329783\n",
      "Iteration 48, loss = 0.68286687\n",
      "Iteration 49, loss = 0.68246215\n",
      "Iteration 50, loss = 0.68208270\n",
      "Iteration 51, loss = 0.68172751\n",
      "Iteration 52, loss = 0.68139559\n",
      "Iteration 53, loss = 0.68108596\n",
      "Iteration 54, loss = 0.68079763\n",
      "Iteration 55, loss = 0.68052963\n",
      "Iteration 56, loss = 0.68028283\n",
      "Iteration 57, loss = 0.68005541\n",
      "Iteration 58, loss = 0.67984507\n",
      "Iteration 59, loss = 0.67965092\n",
      "Iteration 60, loss = 0.67947209\n",
      "Iteration 61, loss = 0.67930772\n",
      "Iteration 62, loss = 0.67915696\n",
      "Iteration 63, loss = 0.67901901\n",
      "Iteration 64, loss = 0.67889321\n",
      "Iteration 65, loss = 0.67877870\n",
      "Iteration 66, loss = 0.67867467\n",
      "Iteration 67, loss = 0.67858040\n",
      "Iteration 68, loss = 0.67849523\n",
      "Iteration 69, loss = 0.67841848\n",
      "Iteration 70, loss = 0.67834953\n",
      "Iteration 71, loss = 0.67828778\n",
      "Iteration 72, loss = 0.67823266\n",
      "Iteration 73, loss = 0.67818362\n",
      "Iteration 74, loss = 0.67814015\n",
      "Iteration 75, loss = 0.67810201\n",
      "Iteration 76, loss = 0.67806864\n",
      "Iteration 77, loss = 0.67803938\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.11373888\n",
      "Iteration 2, loss = 0.71063014\n",
      "Iteration 3, loss = 0.71007728\n",
      "Iteration 4, loss = 0.70930737\n",
      "Iteration 5, loss = 0.70843105\n",
      "Iteration 6, loss = 0.70749683\n",
      "Iteration 7, loss = 0.70653008\n",
      "Iteration 8, loss = 0.70554609\n",
      "Iteration 9, loss = 0.70455567\n",
      "Iteration 10, loss = 0.70357569\n",
      "Iteration 11, loss = 0.70260057\n",
      "Iteration 12, loss = 0.70163419\n",
      "Iteration 13, loss = 0.70067959\n",
      "Iteration 14, loss = 0.69973919\n",
      "Iteration 15, loss = 0.69881495\n",
      "Iteration 16, loss = 0.69790848\n",
      "Iteration 17, loss = 0.69702110\n",
      "Iteration 18, loss = 0.69615389\n",
      "Iteration 19, loss = 0.69531704\n",
      "Iteration 20, loss = 0.69450166\n",
      "Iteration 21, loss = 0.69370816\n",
      "Iteration 22, loss = 0.69293699\n",
      "Iteration 23, loss = 0.69219073\n",
      "Iteration 24, loss = 0.69146980\n",
      "Iteration 25, loss = 0.69077169\n",
      "Iteration 26, loss = 0.69009647\n",
      "Iteration 27, loss = 0.68944419\n",
      "Iteration 28, loss = 0.68881480\n",
      "Iteration 29, loss = 0.68820820\n",
      "Iteration 30, loss = 0.68762427\n",
      "Iteration 31, loss = 0.68706282\n",
      "Iteration 32, loss = 0.68652361\n",
      "Iteration 33, loss = 0.68600637\n",
      "Iteration 34, loss = 0.68551082\n",
      "Iteration 35, loss = 0.68503661\n",
      "Iteration 36, loss = 0.68458338\n",
      "Iteration 37, loss = 0.68415073\n",
      "Iteration 38, loss = 0.68373825\n",
      "Iteration 39, loss = 0.68334551\n",
      "Iteration 40, loss = 0.68297203\n",
      "Iteration 41, loss = 0.68261736\n",
      "Iteration 42, loss = 0.68228099\n",
      "Iteration 43, loss = 0.68196241\n",
      "Iteration 44, loss = 0.68166113\n",
      "Iteration 45, loss = 0.68137833\n",
      "Iteration 46, loss = 0.68111177\n",
      "Iteration 47, loss = 0.68086068\n",
      "Iteration 48, loss = 0.68062452\n",
      "Iteration 49, loss = 0.68040274\n",
      "Iteration 50, loss = 0.68019480\n",
      "Iteration 51, loss = 0.68000014\n",
      "Iteration 52, loss = 0.67981823\n",
      "Iteration 53, loss = 0.67964852\n",
      "Iteration 54, loss = 0.67949046\n",
      "Iteration 55, loss = 0.67934352\n",
      "Iteration 56, loss = 0.67920718\n",
      "Iteration 57, loss = 0.67908091\n",
      "Iteration 58, loss = 0.67896419\n",
      "Iteration 59, loss = 0.67885652\n",
      "Iteration 60, loss = 0.67875741\n",
      "Iteration 61, loss = 0.67866637\n",
      "Iteration 62, loss = 0.67858294\n",
      "Iteration 63, loss = 0.67850664\n",
      "Iteration 64, loss = 0.67843704\n",
      "Iteration 65, loss = 0.67837371\n",
      "Iteration 66, loss = 0.67831622\n",
      "Iteration 67, loss = 0.67826418\n",
      "Iteration 68, loss = 0.67821720\n",
      "Iteration 69, loss = 0.67817491\n",
      "Iteration 70, loss = 0.67813695\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69074635\n",
      "Iteration 2, loss = 0.89111781\n",
      "Iteration 3, loss = 0.61339989\n",
      "Iteration 4, loss = 0.63159345\n",
      "Iteration 5, loss = 0.62895457\n",
      "Iteration 6, loss = 0.61979171\n",
      "Iteration 7, loss = 0.60753234\n",
      "Iteration 8, loss = 0.59700713\n",
      "Iteration 9, loss = 0.58532093\n",
      "Iteration 10, loss = 0.57354307\n",
      "Iteration 11, loss = 0.56874380\n",
      "Iteration 12, loss = 0.56478026\n",
      "Iteration 13, loss = 0.56045977\n",
      "Iteration 14, loss = 0.55738988\n",
      "Iteration 15, loss = 0.55508219\n",
      "Iteration 16, loss = 0.55321729\n",
      "Iteration 17, loss = 0.55148020\n",
      "Iteration 18, loss = 0.54454779\n",
      "Iteration 19, loss = 0.54109714\n",
      "Iteration 20, loss = 0.53852631\n",
      "Iteration 21, loss = 0.53592566\n",
      "Iteration 22, loss = 0.53355593\n",
      "Iteration 23, loss = 0.53137097\n",
      "Iteration 24, loss = 0.52937915\n",
      "Iteration 25, loss = 0.52756076\n",
      "Iteration 26, loss = 0.52587460\n",
      "Iteration 27, loss = 0.52431065\n",
      "Iteration 28, loss = 0.52285551\n",
      "Iteration 29, loss = 0.52017130\n",
      "Iteration 30, loss = 0.51640574\n",
      "Iteration 31, loss = 0.51176249\n",
      "Iteration 32, loss = 0.50578082\n",
      "Iteration 33, loss = 0.49966572\n",
      "Iteration 34, loss = 0.49316986\n",
      "Iteration 35, loss = 0.48647713\n",
      "Iteration 36, loss = 0.48232064\n",
      "Iteration 37, loss = 0.48009052\n",
      "Iteration 38, loss = 0.47872953\n",
      "Iteration 39, loss = 0.47732219\n",
      "Iteration 40, loss = 0.47583286\n",
      "Iteration 41, loss = 0.47395318\n",
      "Iteration 42, loss = 0.47179856\n",
      "Iteration 43, loss = 0.47074207\n",
      "Iteration 44, loss = 0.46971793\n",
      "Iteration 45, loss = 0.46872581\n",
      "Iteration 46, loss = 0.46776443\n",
      "Iteration 47, loss = 0.46683367\n",
      "Iteration 48, loss = 0.46593342\n",
      "Iteration 49, loss = 0.46506355\n",
      "Iteration 50, loss = 0.46422390\n",
      "Iteration 51, loss = 0.46341427\n",
      "Iteration 52, loss = 0.46263441\n",
      "Iteration 53, loss = 0.46188401\n",
      "Iteration 54, loss = 0.46116271\n",
      "Iteration 55, loss = 0.46047010\n",
      "Iteration 56, loss = 0.45980573\n",
      "Iteration 57, loss = 0.45916912\n",
      "Iteration 58, loss = 0.45855973\n",
      "Iteration 59, loss = 0.45797700\n",
      "Iteration 60, loss = 0.45742034\n",
      "Iteration 61, loss = 0.45688914\n",
      "Iteration 62, loss = 0.45638274\n",
      "Iteration 63, loss = 0.45590050\n",
      "Iteration 64, loss = 0.45544174\n",
      "Iteration 65, loss = 0.45500577\n",
      "Iteration 66, loss = 0.45459190\n",
      "Iteration 67, loss = 0.45419942\n",
      "Iteration 68, loss = 0.45382762\n",
      "Iteration 69, loss = 0.45347579\n",
      "Iteration 70, loss = 0.45314322\n",
      "Iteration 71, loss = 0.45282920\n",
      "Iteration 72, loss = 0.45253301\n",
      "Iteration 73, loss = 0.45225397\n",
      "Iteration 74, loss = 0.45199420\n",
      "Iteration 75, loss = 0.45175037\n",
      "Iteration 76, loss = 0.45152128\n",
      "Iteration 77, loss = 0.45130628\n",
      "Iteration 78, loss = 0.45110472\n",
      "Iteration 79, loss = 0.45091598\n",
      "Iteration 80, loss = 0.45073944\n",
      "Iteration 81, loss = 0.45057452\n",
      "Iteration 82, loss = 0.45042063\n",
      "Iteration 83, loss = 0.45027720\n",
      "Iteration 84, loss = 0.45014377\n",
      "Iteration 85, loss = 0.45001987\n",
      "Iteration 86, loss = 0.44990481\n",
      "Iteration 87, loss = 0.44979810\n",
      "Iteration 88, loss = 0.44969939\n",
      "Iteration 89, loss = 0.44960847\n",
      "Iteration 90, loss = 0.44952445\n",
      "Iteration 91, loss = 0.44944690\n",
      "Iteration 92, loss = 0.44937543\n",
      "Iteration 93, loss = 0.44930963\n",
      "Iteration 94, loss = 0.44924916\n",
      "Iteration 95, loss = 0.44919364\n",
      "Iteration 96, loss = 0.44914276\n",
      "Iteration 97, loss = 0.44909618\n",
      "Iteration 98, loss = 0.44905361\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.65949833\n",
      "Iteration 2, loss = 10.48342586\n",
      "Iteration 3, loss = 6.29879899\n",
      "Iteration 4, loss = 1.93994145\n",
      "Iteration 5, loss = 0.74602655\n",
      "Iteration 6, loss = 0.71371681\n",
      "Iteration 7, loss = 0.56496825\n",
      "Iteration 8, loss = 0.55875947\n",
      "Iteration 9, loss = 0.51214746\n",
      "Iteration 10, loss = 0.41653305\n",
      "Iteration 11, loss = 0.44894070\n",
      "Iteration 12, loss = 0.45541307\n",
      "Iteration 13, loss = 0.36581408\n",
      "Iteration 14, loss = 0.32011737\n",
      "Iteration 15, loss = 0.32026919\n",
      "Iteration 16, loss = 0.31028290\n",
      "Iteration 17, loss = 0.29270504\n",
      "Iteration 18, loss = 0.25818924\n",
      "Iteration 19, loss = 0.24497123\n",
      "Iteration 20, loss = 0.22563201\n",
      "Iteration 21, loss = 0.18855250\n",
      "Iteration 22, loss = 0.16195085\n",
      "Iteration 23, loss = 0.14319609\n",
      "Iteration 24, loss = 0.12578441\n",
      "Iteration 25, loss = 0.11190557\n",
      "Iteration 26, loss = 0.10005686\n",
      "Iteration 27, loss = 0.08855783\n",
      "Iteration 28, loss = 0.07870819\n",
      "Iteration 29, loss = 0.06904884\n",
      "Iteration 30, loss = 0.06068686\n",
      "Iteration 31, loss = 0.05388867\n",
      "Iteration 32, loss = 0.04824763\n",
      "Iteration 33, loss = 0.04330882\n",
      "Iteration 34, loss = 0.03797789\n",
      "Iteration 35, loss = 0.03367599\n",
      "Iteration 36, loss = 0.03026076\n",
      "Iteration 37, loss = 0.02782627\n",
      "Iteration 38, loss = 0.02482878\n",
      "Iteration 39, loss = 0.02221714\n",
      "Iteration 40, loss = 0.02058844\n",
      "Iteration 41, loss = 0.01957580\n",
      "Iteration 42, loss = 0.01873416\n",
      "Iteration 43, loss = 0.01802558\n",
      "Iteration 44, loss = 0.01741960\n",
      "Iteration 45, loss = 0.01688700\n",
      "Iteration 46, loss = 0.01649738\n",
      "Iteration 47, loss = 0.01604438\n",
      "Iteration 48, loss = 0.01559195\n",
      "Iteration 49, loss = 0.01528128\n",
      "Iteration 50, loss = 0.01501749\n",
      "Iteration 51, loss = 0.01479275\n",
      "Iteration 52, loss = 0.01459421\n",
      "Iteration 53, loss = 0.01442845\n",
      "Iteration 54, loss = 0.01428916\n",
      "Iteration 55, loss = 0.01417123\n",
      "Iteration 56, loss = 0.01407017\n",
      "Iteration 57, loss = 0.01398354\n",
      "Iteration 58, loss = 0.01390725\n",
      "Iteration 59, loss = 0.01383847\n",
      "Iteration 60, loss = 0.01377937\n",
      "Iteration 61, loss = 0.01372692\n",
      "Iteration 62, loss = 0.01368007\n",
      "Iteration 63, loss = 0.01363792\n",
      "Iteration 64, loss = 0.01359997\n",
      "Iteration 65, loss = 0.01356572\n",
      "Iteration 66, loss = 0.01353487\n",
      "Iteration 67, loss = 0.01350689\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.50437597\n",
      "Iteration 2, loss = 11.90871204\n",
      "Iteration 3, loss = 5.88582658\n",
      "Iteration 4, loss = 0.71677411\n",
      "Iteration 5, loss = 0.62510124\n",
      "Iteration 6, loss = 0.64132848\n",
      "Iteration 7, loss = 0.64496324\n",
      "Iteration 8, loss = 0.63097004\n",
      "Iteration 9, loss = 0.61901332\n",
      "Iteration 10, loss = 0.60601665\n",
      "Iteration 11, loss = 0.61158249\n",
      "Iteration 12, loss = 0.57639920\n",
      "Iteration 13, loss = 0.57165558\n",
      "Iteration 14, loss = 0.56822333\n",
      "Iteration 15, loss = 0.56205042\n",
      "Iteration 16, loss = 0.55912424\n",
      "Iteration 17, loss = 0.55724242\n",
      "Iteration 18, loss = 0.55551568\n",
      "Iteration 19, loss = 0.55381461\n",
      "Iteration 20, loss = 0.55212443\n",
      "Iteration 21, loss = 0.55044309\n",
      "Iteration 22, loss = 0.54877112\n",
      "Iteration 23, loss = 0.54713722\n",
      "Iteration 24, loss = 0.54551683\n",
      "Iteration 25, loss = 0.54390782\n",
      "Iteration 26, loss = 0.54110366\n",
      "Iteration 27, loss = 0.53514134\n",
      "Iteration 28, loss = 0.53292476\n",
      "Iteration 29, loss = 0.53130124\n",
      "Iteration 30, loss = 0.52974778\n",
      "Iteration 31, loss = 0.52822825\n",
      "Iteration 32, loss = 0.52672102\n",
      "Iteration 33, loss = 0.52522567\n",
      "Iteration 34, loss = 0.52374149\n",
      "Iteration 35, loss = 0.52226968\n",
      "Iteration 36, loss = 0.52081421\n",
      "Iteration 37, loss = 0.51937702\n",
      "Iteration 38, loss = 0.51727862\n",
      "Iteration 39, loss = 0.51455164\n",
      "Iteration 40, loss = 0.51270760\n",
      "Iteration 41, loss = 0.50792496\n",
      "Iteration 42, loss = 0.50563942\n",
      "Iteration 43, loss = 0.50419005\n",
      "Iteration 44, loss = 0.50276444\n",
      "Iteration 45, loss = 0.50135996\n",
      "Iteration 46, loss = 0.49997421\n",
      "Iteration 47, loss = 0.49863506\n",
      "Iteration 48, loss = 0.49731415\n",
      "Iteration 49, loss = 0.49600731\n",
      "Iteration 50, loss = 0.49471393\n",
      "Iteration 51, loss = 0.49343361\n",
      "Iteration 52, loss = 0.49216610\n",
      "Iteration 53, loss = 0.49091123\n",
      "Iteration 54, loss = 0.48966888\n",
      "Iteration 55, loss = 0.48843897\n",
      "Iteration 56, loss = 0.48722144\n",
      "Iteration 57, loss = 0.48601626\n",
      "Iteration 58, loss = 0.48482337\n",
      "Iteration 59, loss = 0.48364276\n",
      "Iteration 60, loss = 0.48247440\n",
      "Iteration 61, loss = 0.48131826\n",
      "Iteration 62, loss = 0.48017430\n",
      "Iteration 63, loss = 0.47904250\n",
      "Iteration 64, loss = 0.47792284\n",
      "Iteration 65, loss = 0.47681527\n",
      "Iteration 66, loss = 0.47571976\n",
      "Iteration 67, loss = 0.47463628\n",
      "Iteration 68, loss = 0.47356663\n",
      "Iteration 69, loss = 0.47251803\n",
      "Iteration 70, loss = 0.47148109\n",
      "Iteration 71, loss = 0.47045576\n",
      "Iteration 72, loss = 0.46944200\n",
      "Iteration 73, loss = 0.46843974\n",
      "Iteration 74, loss = 0.46744896\n",
      "Iteration 75, loss = 0.46646959\n",
      "Iteration 76, loss = 0.46550159\n",
      "Iteration 77, loss = 0.46454491\n",
      "Iteration 78, loss = 0.46359950\n",
      "Iteration 79, loss = 0.46266531\n",
      "Iteration 80, loss = 0.46174569\n",
      "Iteration 81, loss = 0.46084098\n",
      "Iteration 82, loss = 0.45994715\n",
      "Iteration 83, loss = 0.45906415\n",
      "Iteration 84, loss = 0.45819191\n",
      "Iteration 85, loss = 0.45733037\n",
      "Iteration 86, loss = 0.45647949\n",
      "Iteration 87, loss = 0.45563921\n",
      "Iteration 88, loss = 0.45480946\n",
      "Iteration 89, loss = 0.45399019\n",
      "Iteration 90, loss = 0.45318134\n",
      "Iteration 91, loss = 0.45238286\n",
      "Iteration 92, loss = 0.45159468\n",
      "Iteration 93, loss = 0.45081674\n",
      "Iteration 94, loss = 0.45004898\n",
      "Iteration 95, loss = 0.44929135\n",
      "Iteration 96, loss = 0.44854379\n",
      "Iteration 97, loss = 0.44780622\n",
      "Iteration 98, loss = 0.44707860\n",
      "Iteration 99, loss = 0.44636767\n",
      "Iteration 100, loss = 0.44566941\n",
      "Iteration 1, loss = 1.77464561\n",
      "Iteration 2, loss = 1.87405088\n",
      "Iteration 3, loss = 2.33076193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 1.44712484\n",
      "Iteration 5, loss = 0.57436547\n",
      "Iteration 6, loss = 0.36223463\n",
      "Iteration 7, loss = 0.41102527\n",
      "Iteration 8, loss = 0.43649223\n",
      "Iteration 9, loss = 0.44845813\n",
      "Iteration 10, loss = 0.43430824\n",
      "Iteration 11, loss = 0.42642905\n",
      "Iteration 12, loss = 0.41991334\n",
      "Iteration 13, loss = 0.40527143\n",
      "Iteration 14, loss = 0.38948711\n",
      "Iteration 15, loss = 0.38419251\n",
      "Iteration 16, loss = 0.37738929\n",
      "Iteration 17, loss = 0.34943533\n",
      "Iteration 18, loss = 0.34710120\n",
      "Iteration 19, loss = 0.34569245\n",
      "Iteration 20, loss = 0.34179117\n",
      "Iteration 21, loss = 0.33598033\n",
      "Iteration 22, loss = 0.33181199\n",
      "Iteration 23, loss = 0.33483107\n",
      "Iteration 24, loss = 0.33569176\n",
      "Iteration 25, loss = 0.33133551\n",
      "Iteration 26, loss = 0.32851264\n",
      "Iteration 27, loss = 0.32884638\n",
      "Iteration 28, loss = 0.32883402\n",
      "Iteration 29, loss = 0.32828961\n",
      "Iteration 30, loss = 0.32737532\n",
      "Iteration 31, loss = 0.32641085\n",
      "Iteration 32, loss = 0.32556375\n",
      "Iteration 33, loss = 0.32489023\n",
      "Iteration 34, loss = 0.32434791\n",
      "Iteration 35, loss = 0.32389446\n",
      "Iteration 36, loss = 0.32349908\n",
      "Iteration 37, loss = 0.32314314\n",
      "Iteration 38, loss = 0.32282030\n",
      "Iteration 39, loss = 0.32251569\n",
      "Iteration 40, loss = 0.32222481\n",
      "Iteration 41, loss = 0.32194489\n",
      "Iteration 42, loss = 0.32167417\n",
      "Iteration 43, loss = 0.32141149\n",
      "Iteration 44, loss = 0.32115610\n",
      "Iteration 45, loss = 0.32090880\n",
      "Iteration 46, loss = 0.32067012\n",
      "Iteration 47, loss = 0.32014986\n",
      "Iteration 48, loss = 0.31791141\n",
      "Iteration 49, loss = 0.31357304\n",
      "Iteration 50, loss = 0.31360542\n",
      "Iteration 51, loss = 0.31127016\n",
      "Iteration 52, loss = 0.30430351\n",
      "Iteration 53, loss = 0.29809716\n",
      "Iteration 54, loss = 0.29655870\n",
      "Iteration 55, loss = 0.29525536\n",
      "Iteration 56, loss = 0.29283134\n",
      "Iteration 57, loss = 0.29063617\n",
      "Iteration 58, loss = 0.29152247\n",
      "Iteration 59, loss = 0.28761323\n",
      "Iteration 60, loss = 0.28555820\n",
      "Iteration 61, loss = 0.28413332\n",
      "Iteration 62, loss = 0.28331737\n",
      "Iteration 63, loss = 0.28288181\n",
      "Iteration 64, loss = 0.28264862\n",
      "Iteration 65, loss = 0.28251918\n",
      "Iteration 66, loss = 0.28172764\n",
      "Iteration 67, loss = 0.27934867\n",
      "Iteration 68, loss = 0.27634429\n",
      "Iteration 69, loss = 0.27514707\n",
      "Iteration 70, loss = 0.27445328\n",
      "Iteration 71, loss = 0.27410367\n",
      "Iteration 72, loss = 0.27393495\n",
      "Iteration 73, loss = 0.27860434\n",
      "Iteration 74, loss = 0.27392214\n",
      "Iteration 75, loss = 0.27402199\n",
      "Iteration 76, loss = 0.27415711\n",
      "Iteration 77, loss = 0.27432711\n",
      "Iteration 78, loss = 0.27452806\n",
      "Iteration 79, loss = 0.27475834\n",
      "Iteration 80, loss = 0.27501801\n",
      "Iteration 81, loss = 0.27527180\n",
      "Iteration 82, loss = 0.27539115\n",
      "Iteration 83, loss = 0.27523762\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 8.97571671\n",
      "Iteration 2, loss = 4.63936693\n",
      "Iteration 3, loss = 1.03062964\n",
      "Iteration 4, loss = 2.47072110\n",
      "Iteration 5, loss = 1.52415919\n",
      "Iteration 6, loss = 1.00571851\n",
      "Iteration 7, loss = 0.43178055\n",
      "Iteration 8, loss = 0.79205662\n",
      "Iteration 9, loss = 0.34848140\n",
      "Iteration 10, loss = 0.29545553\n",
      "Iteration 11, loss = 0.42711046\n",
      "Iteration 12, loss = 0.23561143\n",
      "Iteration 13, loss = 0.10747454\n",
      "Iteration 14, loss = 0.17297541\n",
      "Iteration 15, loss = 0.18624813\n",
      "Iteration 16, loss = 0.10997452\n",
      "Iteration 17, loss = 0.13517216\n",
      "Iteration 18, loss = 0.05936945\n",
      "Iteration 19, loss = 0.05005617\n",
      "Iteration 20, loss = 0.04639913\n",
      "Iteration 21, loss = 0.04146074\n",
      "Iteration 22, loss = 0.03585813\n",
      "Iteration 23, loss = 0.03071272\n",
      "Iteration 24, loss = 0.02737355\n",
      "Iteration 25, loss = 0.02621893\n",
      "Iteration 26, loss = 0.02494798\n",
      "Iteration 27, loss = 0.02071338\n",
      "Iteration 28, loss = 0.01761759\n",
      "Iteration 29, loss = 0.01547791\n",
      "Iteration 30, loss = 0.01383739\n",
      "Iteration 31, loss = 0.01259402\n",
      "Iteration 32, loss = 0.01157245\n",
      "Iteration 33, loss = 0.01047458\n",
      "Iteration 34, loss = 0.00943859\n",
      "Iteration 35, loss = 0.00845591\n",
      "Iteration 36, loss = 0.00755944\n",
      "Iteration 37, loss = 0.00674798\n",
      "Iteration 38, loss = 0.00605772\n",
      "Iteration 39, loss = 0.00548942\n",
      "Iteration 40, loss = 0.00503029\n",
      "Iteration 41, loss = 0.00465019\n",
      "Iteration 42, loss = 0.00432562\n",
      "Iteration 43, loss = 0.00404102\n",
      "Iteration 44, loss = 0.00378559\n",
      "Iteration 45, loss = 0.00355430\n",
      "Iteration 46, loss = 0.00333973\n",
      "Iteration 47, loss = 0.00314067\n",
      "Iteration 48, loss = 0.00295559\n",
      "Iteration 49, loss = 0.00278617\n",
      "Iteration 50, loss = 0.00263076\n",
      "Iteration 51, loss = 0.00248916\n",
      "Iteration 52, loss = 0.00236104\n",
      "Iteration 53, loss = 0.00224537\n",
      "Iteration 54, loss = 0.00214282\n",
      "Iteration 55, loss = 0.00205334\n",
      "Iteration 56, loss = 0.00197240\n",
      "Iteration 57, loss = 0.00189867\n",
      "Iteration 58, loss = 0.00183098\n",
      "Iteration 59, loss = 0.00176765\n",
      "Iteration 60, loss = 0.00170804\n",
      "Iteration 61, loss = 0.00165213\n",
      "Iteration 62, loss = 0.00159937\n",
      "Iteration 63, loss = 0.00154924\n",
      "Iteration 64, loss = 0.00150161\n",
      "Iteration 65, loss = 0.00145647\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.56478646\n",
      "Iteration 2, loss = 7.32971610\n",
      "Iteration 3, loss = 4.40758423\n",
      "Iteration 4, loss = 1.24463026\n",
      "Iteration 5, loss = 0.52537158\n",
      "Iteration 6, loss = 0.58374076\n",
      "Iteration 7, loss = 0.55696398\n",
      "Iteration 8, loss = 0.53731787\n",
      "Iteration 9, loss = 0.53440141\n",
      "Iteration 10, loss = 0.52930848\n",
      "Iteration 11, loss = 0.52516999\n",
      "Iteration 12, loss = 0.51987999\n",
      "Iteration 13, loss = 0.51441890\n",
      "Iteration 14, loss = 0.50899740\n",
      "Iteration 15, loss = 0.50587400\n",
      "Iteration 16, loss = 0.49774901\n",
      "Iteration 17, loss = 0.48727145\n",
      "Iteration 18, loss = 0.47901381\n",
      "Iteration 19, loss = 0.47272585\n",
      "Iteration 20, loss = 0.46823538\n",
      "Iteration 21, loss = 0.46615778\n",
      "Iteration 22, loss = 0.46177420\n",
      "Iteration 23, loss = 0.45963184\n",
      "Iteration 24, loss = 0.45823527\n",
      "Iteration 25, loss = 0.45662034\n",
      "Iteration 26, loss = 0.45497426\n",
      "Iteration 27, loss = 0.45297320\n",
      "Iteration 28, loss = 0.45094764\n",
      "Iteration 29, loss = 0.44900242\n",
      "Iteration 30, loss = 0.44720796\n",
      "Iteration 31, loss = 0.44556743\n",
      "Iteration 32, loss = 0.44407027\n",
      "Iteration 33, loss = 0.44270487\n",
      "Iteration 34, loss = 0.44145126\n",
      "Iteration 35, loss = 0.44028550\n",
      "Iteration 36, loss = 0.43918948\n",
      "Iteration 37, loss = 0.43814820\n",
      "Iteration 38, loss = 0.43714975\n",
      "Iteration 39, loss = 0.43619896\n",
      "Iteration 40, loss = 0.43528304\n",
      "Iteration 41, loss = 0.43438141\n",
      "Iteration 42, loss = 0.43334959\n",
      "Iteration 43, loss = 0.43231590\n",
      "Iteration 44, loss = 0.43118310\n",
      "Iteration 45, loss = 0.42969625\n",
      "Iteration 46, loss = 0.42808186\n",
      "Iteration 47, loss = 0.42648163\n",
      "Iteration 48, loss = 0.42498184\n",
      "Iteration 49, loss = 0.42329733\n",
      "Iteration 50, loss = 0.42117578\n",
      "Iteration 51, loss = 0.41814327\n",
      "Iteration 52, loss = 0.41528774\n",
      "Iteration 53, loss = 0.41047848\n",
      "Iteration 54, loss = 0.40498263\n",
      "Iteration 55, loss = 0.40262262\n",
      "Iteration 56, loss = 0.40176396\n",
      "Iteration 57, loss = 0.39843120\n",
      "Iteration 58, loss = 0.39604624\n",
      "Iteration 59, loss = 0.39410485\n",
      "Iteration 60, loss = 0.39086110\n",
      "Iteration 61, loss = 0.38793615\n",
      "Iteration 62, loss = 0.38600350\n",
      "Iteration 63, loss = 0.38476612\n",
      "Iteration 64, loss = 0.38390997\n",
      "Iteration 65, loss = 0.38324435\n",
      "Iteration 66, loss = 0.38267534\n",
      "Iteration 67, loss = 0.38215844\n",
      "Iteration 68, loss = 0.38167202\n",
      "Iteration 69, loss = 0.38092718\n",
      "Iteration 70, loss = 0.37921886\n",
      "Iteration 71, loss = 0.37597114\n",
      "Iteration 72, loss = 0.37257176\n",
      "Iteration 73, loss = 0.37033414\n",
      "Iteration 74, loss = 0.36904834\n",
      "Iteration 75, loss = 0.36827885\n",
      "Iteration 76, loss = 0.36774723\n",
      "Iteration 77, loss = 0.36732275\n",
      "Iteration 78, loss = 0.36694891\n",
      "Iteration 79, loss = 0.36612859\n",
      "Iteration 80, loss = 0.36195637\n",
      "Iteration 81, loss = 0.35377750\n",
      "Iteration 82, loss = 0.35098100\n",
      "Iteration 83, loss = 0.35219170\n",
      "Iteration 84, loss = 0.35216193\n",
      "Iteration 85, loss = 0.34807239\n",
      "Iteration 86, loss = 0.34003823\n",
      "Iteration 87, loss = 0.33556738\n",
      "Iteration 88, loss = 0.33358965\n",
      "Iteration 89, loss = 0.33298087\n",
      "Iteration 90, loss = 0.33283062\n",
      "Iteration 91, loss = 0.33337998\n",
      "Iteration 92, loss = 0.33213831\n",
      "Iteration 93, loss = 0.32965007\n",
      "Iteration 94, loss = 0.32924577\n",
      "Iteration 95, loss = 0.32866734\n",
      "Iteration 96, loss = 0.32807329\n",
      "Iteration 97, loss = 0.32756363\n",
      "Iteration 98, loss = 0.32716132\n",
      "Iteration 99, loss = 0.32685381\n",
      "Iteration 100, loss = 0.32661797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.19785143\n",
      "Iteration 2, loss = 7.88467337\n",
      "Iteration 3, loss = 6.75885396\n",
      "Iteration 4, loss = 4.25293164\n",
      "Iteration 5, loss = 2.94819086\n",
      "Iteration 6, loss = 1.97847969\n",
      "Iteration 7, loss = 1.23382238\n",
      "Iteration 8, loss = 1.19113089\n",
      "Iteration 9, loss = 0.04280198\n",
      "Iteration 10, loss = 0.15035730\n",
      "Iteration 11, loss = 0.37061392\n",
      "Iteration 12, loss = 0.24620431\n",
      "Iteration 13, loss = 0.03513021\n",
      "Iteration 14, loss = 0.01509469\n",
      "Iteration 15, loss = 0.00345355\n",
      "Iteration 16, loss = 0.00106686\n",
      "Iteration 17, loss = 0.00184651\n",
      "Iteration 18, loss = 0.01246548\n",
      "Iteration 19, loss = 0.00181032\n",
      "Iteration 20, loss = 0.00232283\n",
      "Iteration 21, loss = 0.00613256\n",
      "Iteration 22, loss = 0.00814851\n",
      "Iteration 23, loss = 0.00493576\n",
      "Iteration 24, loss = 0.00169178\n",
      "Iteration 25, loss = 0.00082788\n",
      "Iteration 26, loss = 0.00061169\n",
      "Iteration 27, loss = 0.00047166\n",
      "Iteration 28, loss = 0.00048901\n",
      "Iteration 29, loss = 0.00053680\n",
      "Iteration 30, loss = 0.00056858\n",
      "Iteration 31, loss = 0.00053298\n",
      "Iteration 32, loss = 0.00049763\n",
      "Iteration 33, loss = 0.00055241\n",
      "Iteration 34, loss = 0.00046609\n",
      "Iteration 35, loss = 0.00041680\n",
      "Iteration 36, loss = 0.00037979\n",
      "Iteration 37, loss = 0.00033844\n",
      "Iteration 38, loss = 0.00030098\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.73105522\n",
      "Iteration 2, loss = 7.72564289\n",
      "Iteration 3, loss = 4.57866682\n",
      "Iteration 4, loss = 2.10651291\n",
      "Iteration 5, loss = 1.55653083\n",
      "Iteration 6, loss = 3.84890957\n",
      "Iteration 7, loss = 0.71483923\n",
      "Iteration 8, loss = 0.39081410\n",
      "Iteration 9, loss = 0.60957545\n",
      "Iteration 10, loss = 0.47965882\n",
      "Iteration 11, loss = 0.46944756\n",
      "Iteration 12, loss = 0.03640612\n",
      "Iteration 13, loss = 0.00375269\n",
      "Iteration 14, loss = 0.24666615\n",
      "Iteration 15, loss = 0.01466420\n",
      "Iteration 16, loss = 0.03243788\n",
      "Iteration 17, loss = 0.03532245\n",
      "Iteration 18, loss = 0.00763172\n",
      "Iteration 19, loss = 0.00199006\n",
      "Iteration 20, loss = 0.00087127\n",
      "Iteration 21, loss = 0.00052988\n",
      "Iteration 22, loss = 0.00043809\n",
      "Iteration 23, loss = 0.00046082\n",
      "Iteration 24, loss = 0.00058481\n",
      "Iteration 25, loss = 0.00076296\n",
      "Iteration 26, loss = 0.00093889\n",
      "Iteration 27, loss = 0.00114883\n",
      "Iteration 28, loss = 0.00114091\n",
      "Iteration 29, loss = 0.00103965\n",
      "Iteration 30, loss = 0.00089605\n",
      "Iteration 31, loss = 0.00071465\n",
      "Iteration 32, loss = 0.00053838\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 6.58992005\n",
      "Iteration 2, loss = 20.79975765\n",
      "Iteration 3, loss = 17.66191905\n",
      "Iteration 4, loss = 1.46958091\n",
      "Iteration 5, loss = 6.70201271\n",
      "Iteration 6, loss = 6.75668482\n",
      "Iteration 7, loss = 1.73146762\n",
      "Iteration 8, loss = 3.48189037\n",
      "Iteration 9, loss = 4.14254856\n",
      "Iteration 10, loss = 1.72465438\n",
      "Iteration 11, loss = 0.67901695\n",
      "Iteration 12, loss = 2.04320424\n",
      "Iteration 13, loss = 2.32649985\n",
      "Iteration 14, loss = 1.29034895\n",
      "Iteration 15, loss = 0.17440413\n",
      "Iteration 16, loss = 0.42613782\n",
      "Iteration 17, loss = 1.16505268\n",
      "Iteration 18, loss = 0.91285689\n",
      "Iteration 19, loss = 0.28135001\n",
      "Iteration 20, loss = 0.02426483\n",
      "Iteration 21, loss = 0.05383529\n",
      "Iteration 22, loss = 0.21286948\n",
      "Iteration 23, loss = 0.34886568\n",
      "Iteration 24, loss = 0.19068438\n",
      "Iteration 25, loss = 0.04394652\n",
      "Iteration 26, loss = 0.00928940\n",
      "Iteration 27, loss = 0.01130944\n",
      "Iteration 28, loss = 0.02889512\n",
      "Iteration 29, loss = 0.06175220\n",
      "Iteration 30, loss = 0.06337988\n",
      "Iteration 31, loss = 0.03172075\n",
      "Iteration 32, loss = 0.01239822\n",
      "Iteration 33, loss = 0.00529432\n",
      "Iteration 34, loss = 0.00244017\n",
      "Iteration 35, loss = 0.00129652\n",
      "Iteration 36, loss = 0.00078605\n",
      "Iteration 37, loss = 0.00054160\n",
      "Iteration 38, loss = 0.00042679\n",
      "Iteration 39, loss = 0.00040112\n",
      "Iteration 40, loss = 0.00048639\n",
      "Iteration 41, loss = 0.00076171\n",
      "Iteration 42, loss = 0.00131453\n",
      "Iteration 43, loss = 0.00204812\n",
      "Iteration 44, loss = 0.00251978\n",
      "Iteration 45, loss = 0.00235015\n",
      "Iteration 46, loss = 0.00173154\n",
      "Iteration 47, loss = 0.00111903\n",
      "Iteration 48, loss = 0.00071850\n",
      "Iteration 49, loss = 0.00050270\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.33877122\n",
      "Iteration 2, loss = 19.61766019\n",
      "Iteration 3, loss = 10.69976542\n",
      "Iteration 4, loss = 7.63831693\n",
      "Iteration 5, loss = 7.19272247\n",
      "Iteration 6, loss = 1.20636833\n",
      "Iteration 7, loss = 8.58249562\n",
      "Iteration 8, loss = 8.25337531\n",
      "Iteration 9, loss = 2.50649243\n",
      "Iteration 10, loss = 1.08174858\n",
      "Iteration 11, loss = 3.47624021\n",
      "Iteration 12, loss = 4.25591308\n",
      "Iteration 13, loss = 2.95590014\n",
      "Iteration 14, loss = 0.89997891\n",
      "Iteration 15, loss = 0.23234595\n",
      "Iteration 16, loss = 0.68378452\n",
      "Iteration 17, loss = 1.25582914\n",
      "Iteration 18, loss = 1.37747392\n",
      "Iteration 19, loss = 0.74775109\n",
      "Iteration 20, loss = 0.20516745\n",
      "Iteration 21, loss = 0.04177188\n",
      "Iteration 22, loss = 0.08432932\n",
      "Iteration 23, loss = 0.16873576\n",
      "Iteration 24, loss = 0.19911015\n",
      "Iteration 25, loss = 0.19407646\n",
      "Iteration 26, loss = 0.20790918\n",
      "Iteration 27, loss = 0.11744581\n",
      "Iteration 28, loss = 0.06100032\n",
      "Iteration 29, loss = 0.01154471\n",
      "Iteration 30, loss = 0.00040423\n",
      "Iteration 31, loss = 0.00053090\n",
      "Iteration 32, loss = 0.00105128\n",
      "Iteration 33, loss = 0.00173162\n",
      "Iteration 34, loss = 0.00261495\n",
      "Iteration 35, loss = 0.00359049\n",
      "Iteration 36, loss = 0.00437784\n",
      "Iteration 37, loss = 0.00464236\n",
      "Iteration 38, loss = 0.00434164\n",
      "Iteration 39, loss = 0.00378886\n",
      "Iteration 40, loss = 0.00331136\n",
      "Iteration 41, loss = 0.00298633\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.55879568\n",
      "Iteration 2, loss = 19.70029446\n",
      "Iteration 3, loss = 3.75733421\n",
      "Iteration 4, loss = 2.52300392\n",
      "Iteration 5, loss = 5.65734094\n",
      "Iteration 6, loss = 0.93665990\n",
      "Iteration 7, loss = 2.18130280\n",
      "Iteration 8, loss = 1.25545283\n",
      "Iteration 9, loss = 0.20606800\n",
      "Iteration 10, loss = 1.16174919\n",
      "Iteration 11, loss = 0.54810064\n",
      "Iteration 12, loss = 0.14293848\n",
      "Iteration 13, loss = 0.17492089\n",
      "Iteration 14, loss = 0.25836079\n",
      "Iteration 15, loss = 0.29710188\n",
      "Iteration 16, loss = 0.19335051\n",
      "Iteration 17, loss = 0.01058177\n",
      "Iteration 18, loss = 0.00043094\n",
      "Iteration 19, loss = 0.00038833\n",
      "Iteration 20, loss = 0.00047344\n",
      "Iteration 21, loss = 0.00057446\n",
      "Iteration 22, loss = 0.00069923\n",
      "Iteration 23, loss = 0.00083861\n",
      "Iteration 24, loss = 0.00097081\n",
      "Iteration 25, loss = 0.00108522\n",
      "Iteration 26, loss = 0.00116116\n",
      "Iteration 27, loss = 0.00119620\n",
      "Iteration 28, loss = 0.00118654\n",
      "Iteration 29, loss = 0.00113650\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.98962752\n",
      "Iteration 2, loss = 0.66835761\n",
      "Iteration 3, loss = 0.71513724\n",
      "Iteration 4, loss = 0.69632998\n",
      "Iteration 5, loss = 0.67103741\n",
      "Iteration 6, loss = 0.65944256\n",
      "Iteration 7, loss = 0.65194354\n",
      "Iteration 8, loss = 0.64646561\n",
      "Iteration 9, loss = 0.64245713\n",
      "Iteration 10, loss = 0.63959201\n",
      "Iteration 11, loss = 0.63746717\n",
      "Iteration 12, loss = 0.63574156\n",
      "Iteration 13, loss = 0.63124858\n",
      "Iteration 14, loss = 0.62166300\n",
      "Iteration 15, loss = 0.60921272\n",
      "Iteration 16, loss = 0.59314598\n",
      "Iteration 17, loss = 0.57649103\n",
      "Iteration 18, loss = 0.56880236\n",
      "Iteration 19, loss = 0.56272724\n",
      "Iteration 20, loss = 0.55214157\n",
      "Iteration 21, loss = 0.54207022\n",
      "Iteration 22, loss = 0.53468902\n",
      "Iteration 23, loss = 0.52956029\n",
      "Iteration 24, loss = 0.52788699\n",
      "Iteration 25, loss = 0.52641178\n",
      "Iteration 26, loss = 0.52507258\n",
      "Iteration 27, loss = 0.52386964\n",
      "Iteration 28, loss = 0.52280477\n",
      "Iteration 29, loss = 0.52187666\n",
      "Iteration 30, loss = 0.51814759\n",
      "Iteration 31, loss = 0.51134945\n",
      "Iteration 32, loss = 0.50979458\n",
      "Iteration 33, loss = 0.50716404\n",
      "Iteration 34, loss = 0.50524683\n",
      "Iteration 35, loss = 0.50114043\n",
      "Iteration 36, loss = 0.49910530\n",
      "Iteration 37, loss = 0.49823449\n",
      "Iteration 38, loss = 0.49198785\n",
      "Iteration 39, loss = 0.47904713\n",
      "Iteration 40, loss = 0.47428445\n",
      "Iteration 41, loss = 0.46036728\n",
      "Iteration 42, loss = 0.44177092\n",
      "Iteration 43, loss = 0.43095358\n",
      "Iteration 44, loss = 0.43628282\n",
      "Iteration 45, loss = 0.46997551\n",
      "Iteration 46, loss = 0.42528789\n",
      "Iteration 47, loss = 0.41399171\n",
      "Iteration 48, loss = 0.41250843\n",
      "Iteration 49, loss = 0.41237659\n",
      "Iteration 50, loss = 0.41240972\n",
      "Iteration 51, loss = 0.41244188\n",
      "Iteration 52, loss = 0.41224501\n",
      "Iteration 53, loss = 0.41175671\n",
      "Iteration 54, loss = 0.41114542\n",
      "Iteration 55, loss = 0.41060890\n",
      "Iteration 56, loss = 0.41022732\n",
      "Iteration 57, loss = 0.40998930\n",
      "Iteration 58, loss = 0.40985670\n",
      "Iteration 59, loss = 0.40979473\n",
      "Iteration 60, loss = 0.40977833\n",
      "Iteration 61, loss = 0.40979015\n",
      "Iteration 62, loss = 0.40981808\n",
      "Iteration 63, loss = 0.40985343\n",
      "Iteration 64, loss = 0.40988989\n",
      "Iteration 65, loss = 0.40992291\n",
      "Iteration 66, loss = 0.40994944\n",
      "Iteration 67, loss = 0.40996762\n",
      "Iteration 68, loss = 0.40997657\n",
      "Iteration 69, loss = 0.40997627\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70965968\n",
      "Iteration 2, loss = 0.68538220\n",
      "Iteration 3, loss = 0.68400998\n",
      "Iteration 4, loss = 0.68270555\n",
      "Iteration 5, loss = 0.68152453\n",
      "Iteration 6, loss = 0.68049749\n",
      "Iteration 7, loss = 0.67964371\n",
      "Iteration 8, loss = 0.67896912\n",
      "Iteration 9, loss = 0.67847030\n",
      "Iteration 10, loss = 0.67813524\n",
      "Iteration 11, loss = 0.67794688\n",
      "Iteration 12, loss = 0.67788104\n",
      "Iteration 13, loss = 0.67791042\n",
      "Iteration 14, loss = 0.67800547\n",
      "Iteration 15, loss = 0.67813669\n",
      "Iteration 16, loss = 0.67827663\n",
      "Iteration 17, loss = 0.67840288\n",
      "Iteration 18, loss = 0.67849898\n",
      "Iteration 19, loss = 0.67855557\n",
      "Iteration 20, loss = 0.67856972\n",
      "Iteration 21, loss = 0.67854402\n",
      "Iteration 22, loss = 0.67848515\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.83315924\n",
      "Iteration 2, loss = 0.68973421\n",
      "Iteration 3, loss = 0.74725576\n",
      "Iteration 4, loss = 0.67259670\n",
      "Iteration 5, loss = 0.65749492\n",
      "Iteration 6, loss = 0.65119764\n",
      "Iteration 7, loss = 0.64536310\n",
      "Iteration 8, loss = 0.63740173\n",
      "Iteration 9, loss = 0.62335531\n",
      "Iteration 10, loss = 0.60769280\n",
      "Iteration 11, loss = 0.58583341\n",
      "Iteration 12, loss = 0.56924995\n",
      "Iteration 13, loss = 0.55301872\n",
      "Iteration 14, loss = 0.54296407\n",
      "Iteration 15, loss = 0.54022477\n",
      "Iteration 16, loss = 0.53617938\n",
      "Iteration 17, loss = 0.52962110\n",
      "Iteration 18, loss = 0.52087903\n",
      "Iteration 19, loss = 0.50618692\n",
      "Iteration 20, loss = 0.49786540\n",
      "Iteration 21, loss = 0.48773273\n",
      "Iteration 22, loss = 0.47462674\n",
      "Iteration 23, loss = 0.46718974\n",
      "Iteration 24, loss = 0.45649231\n",
      "Iteration 25, loss = 0.45363193\n",
      "Iteration 26, loss = 0.45232235\n",
      "Iteration 27, loss = 0.45013587\n",
      "Iteration 28, loss = 0.44825076\n",
      "Iteration 29, loss = 0.44688487\n",
      "Iteration 30, loss = 0.44515023\n",
      "Iteration 31, loss = 0.44322390\n",
      "Iteration 32, loss = 0.44159718\n",
      "Iteration 33, loss = 0.44032769\n",
      "Iteration 34, loss = 0.43931410\n",
      "Iteration 35, loss = 0.43848856\n",
      "Iteration 36, loss = 0.43781488\n",
      "Iteration 37, loss = 0.43727277\n",
      "Iteration 38, loss = 0.43684651\n",
      "Iteration 39, loss = 0.43652362\n",
      "Iteration 40, loss = 0.43629065\n",
      "Iteration 41, loss = 0.43613502\n",
      "Iteration 42, loss = 0.43604445\n",
      "Iteration 43, loss = 0.43600710\n",
      "Iteration 44, loss = 0.43601144\n",
      "Iteration 45, loss = 0.43604699\n",
      "Iteration 46, loss = 0.43610419\n",
      "Iteration 47, loss = 0.43617449\n",
      "Iteration 48, loss = 0.43625048\n",
      "Iteration 49, loss = 0.43632594\n",
      "Iteration 50, loss = 0.43639585\n",
      "Iteration 51, loss = 0.43645648\n",
      "Iteration 52, loss = 0.43650525\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70914735\n",
      "Iteration 2, loss = 0.69268771\n",
      "Iteration 3, loss = 0.69073702\n",
      "Iteration 4, loss = 0.68873655\n",
      "Iteration 5, loss = 0.68682165\n",
      "Iteration 6, loss = 0.68505523\n",
      "Iteration 7, loss = 0.68346148\n",
      "Iteration 8, loss = 0.68206704\n",
      "Iteration 9, loss = 0.68088021\n",
      "Iteration 10, loss = 0.67990671\n",
      "Iteration 11, loss = 0.67914163\n",
      "Iteration 12, loss = 0.67857409\n",
      "Iteration 13, loss = 0.67819038\n",
      "Iteration 14, loss = 0.67796884\n",
      "Iteration 15, loss = 0.67788346\n",
      "Iteration 16, loss = 0.67790609\n",
      "Iteration 17, loss = 0.67800641\n",
      "Iteration 18, loss = 0.67815410\n",
      "Iteration 19, loss = 0.67832094\n",
      "Iteration 20, loss = 0.67848252\n",
      "Iteration 21, loss = 0.67862012\n",
      "Iteration 22, loss = 0.67872055\n",
      "Iteration 23, loss = 0.67877699\n",
      "Iteration 24, loss = 0.67878814\n",
      "Iteration 25, loss = 0.67875729\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77009709\n",
      "Iteration 2, loss = 0.68378935\n",
      "Iteration 3, loss = 0.68431307\n",
      "Iteration 4, loss = 0.68372106\n",
      "Iteration 5, loss = 0.68293055\n",
      "Iteration 6, loss = 0.68207700\n",
      "Iteration 7, loss = 0.68123784\n",
      "Iteration 8, loss = 0.68045588\n",
      "Iteration 9, loss = 0.67976080\n",
      "Iteration 10, loss = 0.67917110\n",
      "Iteration 11, loss = 0.67869494\n",
      "Iteration 12, loss = 0.67833444\n",
      "Iteration 13, loss = 0.67808596\n",
      "Iteration 14, loss = 0.67793964\n",
      "Iteration 15, loss = 0.67788243\n",
      "Iteration 16, loss = 0.67789711\n",
      "Iteration 17, loss = 0.67796367\n",
      "Iteration 18, loss = 0.67806112\n",
      "Iteration 19, loss = 0.67816909\n",
      "Iteration 20, loss = 0.67826997\n",
      "Iteration 21, loss = 0.67835038\n",
      "Iteration 22, loss = 0.67840245\n",
      "Iteration 23, loss = 0.67842277\n",
      "Iteration 24, loss = 0.67841217\n",
      "Iteration 25, loss = 0.67837475\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.20105683\n",
      "Iteration 2, loss = 1.44756230\n",
      "Iteration 3, loss = 1.63584412\n",
      "Iteration 4, loss = 0.71815675\n",
      "Iteration 5, loss = 0.71291897\n",
      "Iteration 6, loss = 0.72789416\n",
      "Iteration 7, loss = 0.57231396\n",
      "Iteration 8, loss = 0.48226803\n",
      "Iteration 9, loss = 0.44806975\n",
      "Iteration 10, loss = 0.43875690\n",
      "Iteration 11, loss = 0.42996456\n",
      "Iteration 12, loss = 0.41917153\n",
      "Iteration 13, loss = 0.39931979\n",
      "Iteration 14, loss = 0.38557941\n",
      "Iteration 15, loss = 0.36961401\n",
      "Iteration 16, loss = 0.35271331\n",
      "Iteration 17, loss = 0.33645266\n",
      "Iteration 18, loss = 0.32138383\n",
      "Iteration 19, loss = 0.31536694\n",
      "Iteration 20, loss = 0.31110421\n",
      "Iteration 21, loss = 0.30711736\n",
      "Iteration 22, loss = 0.30472529\n",
      "Iteration 23, loss = 0.30215879\n",
      "Iteration 24, loss = 0.29731873\n",
      "Iteration 25, loss = 0.28975260\n",
      "Iteration 26, loss = 0.28382617\n",
      "Iteration 27, loss = 0.28951419\n",
      "Iteration 28, loss = 0.27238943\n",
      "Iteration 29, loss = 0.26657634\n",
      "Iteration 30, loss = 0.25739235\n",
      "Iteration 31, loss = 0.25215238\n",
      "Iteration 32, loss = 0.24919535\n",
      "Iteration 33, loss = 0.24394750\n",
      "Iteration 34, loss = 0.24921897\n",
      "Iteration 35, loss = 0.23677288\n",
      "Iteration 36, loss = 0.23668804\n",
      "Iteration 37, loss = 0.23595155\n",
      "Iteration 38, loss = 0.23526654\n",
      "Iteration 39, loss = 0.23236774\n",
      "Iteration 40, loss = 0.22924162\n",
      "Iteration 41, loss = 0.22367593\n",
      "Iteration 42, loss = 0.21430798\n",
      "Iteration 43, loss = 0.21139702\n",
      "Iteration 44, loss = 0.21060378\n",
      "Iteration 45, loss = 0.21254619\n",
      "Iteration 46, loss = 0.20471074\n",
      "Iteration 47, loss = 0.19886541\n",
      "Iteration 48, loss = 0.19813544\n",
      "Iteration 49, loss = 0.20574351\n",
      "Iteration 50, loss = 0.19396588\n",
      "Iteration 51, loss = 0.19033405\n",
      "Iteration 52, loss = 0.18094029\n",
      "Iteration 53, loss = 0.18774797\n",
      "Iteration 54, loss = 0.17474412\n",
      "Iteration 55, loss = 0.17424732\n",
      "Iteration 56, loss = 0.17392500\n",
      "Iteration 57, loss = 0.17373589\n",
      "Iteration 58, loss = 0.17361130\n",
      "Iteration 59, loss = 0.17351490\n",
      "Iteration 60, loss = 0.17220336\n",
      "Iteration 61, loss = 0.16605203\n",
      "Iteration 62, loss = 0.15925682\n",
      "Iteration 63, loss = 0.14075039\n",
      "Iteration 64, loss = 0.13878120\n",
      "Iteration 65, loss = 0.11953288\n",
      "Iteration 66, loss = 0.14101528\n",
      "Iteration 67, loss = 0.11722667\n",
      "Iteration 68, loss = 0.11541084\n",
      "Iteration 69, loss = 0.11528634\n",
      "Iteration 70, loss = 0.11529827\n",
      "Iteration 71, loss = 0.11131547\n",
      "Iteration 72, loss = 0.10878370\n",
      "Iteration 73, loss = 0.10486543\n",
      "Iteration 74, loss = 0.10636491\n",
      "Iteration 75, loss = 0.12174718\n",
      "Iteration 76, loss = 0.10327785\n",
      "Iteration 77, loss = 0.10385981\n",
      "Iteration 78, loss = 0.10513000\n",
      "Iteration 79, loss = 0.10768753\n",
      "Iteration 80, loss = 0.11377628\n",
      "Iteration 81, loss = 0.11119621\n",
      "Iteration 82, loss = 0.10907505\n",
      "Iteration 83, loss = 0.10867935\n",
      "Iteration 84, loss = 0.10798174\n",
      "Iteration 85, loss = 0.09578228\n",
      "Iteration 86, loss = 0.09775116\n",
      "Iteration 87, loss = 0.08865594\n",
      "Iteration 88, loss = 0.08866083\n",
      "Iteration 89, loss = 0.08868382\n",
      "Iteration 90, loss = 0.08873192\n",
      "Iteration 91, loss = 0.08881657\n",
      "Iteration 92, loss = 0.08893715\n",
      "Iteration 93, loss = 0.08905955\n",
      "Iteration 94, loss = 0.08911390\n",
      "Iteration 95, loss = 0.08906363\n",
      "Iteration 96, loss = 0.08894532\n",
      "Iteration 97, loss = 0.08882244\n",
      "Iteration 98, loss = 0.08872556\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.83244653\n",
      "Iteration 2, loss = 1.54410801\n",
      "Iteration 3, loss = 0.96179110\n",
      "Iteration 4, loss = 0.64390678\n",
      "Iteration 5, loss = 0.38141714\n",
      "Iteration 6, loss = 0.40936557\n",
      "Iteration 7, loss = 0.36973386\n",
      "Iteration 8, loss = 0.34330644\n",
      "Iteration 9, loss = 0.31659253\n",
      "Iteration 10, loss = 0.30313106\n",
      "Iteration 11, loss = 0.29602214\n",
      "Iteration 12, loss = 0.27697023\n",
      "Iteration 13, loss = 0.26538486\n",
      "Iteration 14, loss = 0.24367288\n",
      "Iteration 15, loss = 0.23638829\n",
      "Iteration 16, loss = 0.22863034\n",
      "Iteration 17, loss = 0.21720642\n",
      "Iteration 18, loss = 0.20784524\n",
      "Iteration 19, loss = 0.20214569\n",
      "Iteration 20, loss = 0.19772354\n",
      "Iteration 21, loss = 0.18371917\n",
      "Iteration 22, loss = 0.17908176\n",
      "Iteration 23, loss = 0.17368693\n",
      "Iteration 24, loss = 0.16826440\n",
      "Iteration 25, loss = 0.16508378\n",
      "Iteration 26, loss = 0.16307886\n",
      "Iteration 27, loss = 0.16279723\n",
      "Iteration 28, loss = 0.15963842\n",
      "Iteration 29, loss = 0.15809147\n",
      "Iteration 30, loss = 0.15640738\n",
      "Iteration 31, loss = 0.15122770\n",
      "Iteration 32, loss = 0.15007983\n",
      "Iteration 33, loss = 0.14612250\n",
      "Iteration 34, loss = 0.14099387\n",
      "Iteration 35, loss = 0.13315958\n",
      "Iteration 36, loss = 0.11444141\n",
      "Iteration 37, loss = 0.09504812\n",
      "Iteration 38, loss = 0.07590823\n",
      "Iteration 39, loss = 0.07267570\n",
      "Iteration 40, loss = 0.10048422\n",
      "Iteration 41, loss = 0.06627300\n",
      "Iteration 42, loss = 0.07362451\n",
      "Iteration 43, loss = 0.05265179\n",
      "Iteration 44, loss = 0.04754876\n",
      "Iteration 45, loss = 0.04512697\n",
      "Iteration 46, loss = 0.04306367\n",
      "Iteration 47, loss = 0.04054630\n",
      "Iteration 48, loss = 0.04006817\n",
      "Iteration 49, loss = 0.04154902\n",
      "Iteration 50, loss = 0.04061528\n",
      "Iteration 51, loss = 0.03677052\n",
      "Iteration 52, loss = 0.03573924\n",
      "Iteration 53, loss = 0.03518180\n",
      "Iteration 54, loss = 0.03486567\n",
      "Iteration 55, loss = 0.03465907\n",
      "Iteration 56, loss = 0.03450170\n",
      "Iteration 57, loss = 0.03436935\n",
      "Iteration 58, loss = 0.03425423\n",
      "Iteration 59, loss = 0.03415760\n",
      "Iteration 60, loss = 0.03408659\n",
      "Iteration 61, loss = 0.03404357\n",
      "Iteration 62, loss = 0.03399480\n",
      "Iteration 63, loss = 0.03388207\n",
      "Iteration 64, loss = 0.03371847\n",
      "Iteration 65, loss = 0.03355975\n",
      "Iteration 66, loss = 0.03342376\n",
      "Iteration 67, loss = 0.03330509\n",
      "Iteration 68, loss = 0.03319685\n",
      "Iteration 69, loss = 0.03309445\n",
      "Iteration 70, loss = 0.03299572\n",
      "Iteration 71, loss = 0.03289955\n",
      "Iteration 72, loss = 0.03280553\n",
      "Iteration 73, loss = 0.03271328\n",
      "Iteration 74, loss = 0.03262257\n",
      "Iteration 75, loss = 0.03253319\n",
      "Iteration 76, loss = 0.03244511\n",
      "Iteration 77, loss = 0.03235830\n",
      "Iteration 78, loss = 0.03227257\n",
      "Iteration 79, loss = 0.03218786\n",
      "Iteration 80, loss = 0.03210415\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.81664044\n",
      "Iteration 2, loss = 1.52272108\n",
      "Iteration 3, loss = 1.82297950\n",
      "Iteration 4, loss = 0.91645407\n",
      "Iteration 5, loss = 0.58409480\n",
      "Iteration 6, loss = 0.59406150\n",
      "Iteration 7, loss = 0.53589688\n",
      "Iteration 8, loss = 0.55236580\n",
      "Iteration 9, loss = 0.46652418\n",
      "Iteration 10, loss = 0.40757079\n",
      "Iteration 11, loss = 0.39661710\n",
      "Iteration 12, loss = 0.36442671\n",
      "Iteration 13, loss = 0.31523361\n",
      "Iteration 14, loss = 0.29526765\n",
      "Iteration 15, loss = 0.28548820\n",
      "Iteration 16, loss = 0.26964487\n",
      "Iteration 17, loss = 0.24411789\n",
      "Iteration 18, loss = 0.23017781\n",
      "Iteration 19, loss = 0.23102633\n",
      "Iteration 20, loss = 0.21155328\n",
      "Iteration 21, loss = 0.20960178\n",
      "Iteration 22, loss = 0.20301712\n",
      "Iteration 23, loss = 0.18354529\n",
      "Iteration 24, loss = 0.17446544\n",
      "Iteration 25, loss = 0.16763803\n",
      "Iteration 26, loss = 0.16492986\n",
      "Iteration 27, loss = 0.16051401\n",
      "Iteration 28, loss = 0.15169577\n",
      "Iteration 29, loss = 0.13346772\n",
      "Iteration 30, loss = 0.12079260\n",
      "Iteration 31, loss = 0.12158387\n",
      "Iteration 32, loss = 0.11664381\n",
      "Iteration 33, loss = 0.10708931\n",
      "Iteration 34, loss = 0.09649049\n",
      "Iteration 35, loss = 0.09417987\n",
      "Iteration 36, loss = 0.09277218\n",
      "Iteration 37, loss = 0.09162816\n",
      "Iteration 38, loss = 0.09050775\n",
      "Iteration 39, loss = 0.08941473\n",
      "Iteration 40, loss = 0.08831517\n",
      "Iteration 41, loss = 0.08721425\n",
      "Iteration 42, loss = 0.08537315\n",
      "Iteration 43, loss = 0.07990229\n",
      "Iteration 44, loss = 0.08163514\n",
      "Iteration 45, loss = 0.07702125\n",
      "Iteration 46, loss = 0.08664190\n",
      "Iteration 47, loss = 0.06831872\n",
      "Iteration 48, loss = 0.06327759\n",
      "Iteration 49, loss = 0.06146086\n",
      "Iteration 50, loss = 0.06035586\n",
      "Iteration 51, loss = 0.05951814\n",
      "Iteration 52, loss = 0.05991741\n",
      "Iteration 53, loss = 0.05813518\n",
      "Iteration 54, loss = 0.05749291\n",
      "Iteration 55, loss = 0.05686180\n",
      "Iteration 56, loss = 0.05624310\n",
      "Iteration 57, loss = 0.05563418\n",
      "Iteration 58, loss = 0.05503185\n",
      "Iteration 59, loss = 0.05443599\n",
      "Iteration 60, loss = 0.05399955\n",
      "Iteration 61, loss = 0.05329281\n",
      "Iteration 62, loss = 0.05274869\n",
      "Iteration 63, loss = 0.05221945\n",
      "Iteration 64, loss = 0.05170438\n",
      "Iteration 65, loss = 0.05120573\n",
      "Iteration 66, loss = 0.05072520\n",
      "Iteration 67, loss = 0.05025870\n",
      "Iteration 68, loss = 0.04980052\n",
      "Iteration 69, loss = 0.04933934\n",
      "Iteration 70, loss = 0.04885944\n",
      "Iteration 71, loss = 0.04834776\n",
      "Iteration 72, loss = 0.04780154\n",
      "Iteration 73, loss = 0.04723208\n",
      "Iteration 74, loss = 0.04665517\n",
      "Iteration 75, loss = 0.04608425\n",
      "Iteration 76, loss = 0.04552672\n",
      "Iteration 77, loss = 0.04498918\n",
      "Iteration 78, loss = 0.04446703\n",
      "Iteration 79, loss = 0.04395910\n",
      "Iteration 80, loss = 0.04346484\n",
      "Iteration 81, loss = 0.04297966\n",
      "Iteration 82, loss = 0.04250185\n",
      "Iteration 83, loss = 0.04203127\n",
      "Iteration 84, loss = 0.04157009\n",
      "Iteration 85, loss = 0.04111452\n",
      "Iteration 86, loss = 0.04066385\n",
      "Iteration 87, loss = 0.04021904\n",
      "Iteration 88, loss = 0.03977920\n",
      "Iteration 89, loss = 0.03934394\n",
      "Iteration 90, loss = 0.03891131\n",
      "Iteration 91, loss = 0.03848119\n",
      "Iteration 92, loss = 0.03805429\n",
      "Iteration 93, loss = 0.03763127\n",
      "Iteration 94, loss = 0.03721166\n",
      "Iteration 95, loss = 0.03679524\n",
      "Iteration 96, loss = 0.03638130\n",
      "Iteration 97, loss = 0.03597088\n",
      "Iteration 98, loss = 0.03589788\n",
      "Iteration 99, loss = 0.03518033\n",
      "Iteration 100, loss = 0.03481264\n",
      "Iteration 1, loss = 1.49363701\n",
      "Iteration 2, loss = 0.64841573\n",
      "Iteration 3, loss = 0.55099013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.45091454\n",
      "Iteration 5, loss = 0.43368069\n",
      "Iteration 6, loss = 0.41798193\n",
      "Iteration 7, loss = 0.38968798\n",
      "Iteration 8, loss = 0.37797241\n",
      "Iteration 9, loss = 0.35075767\n",
      "Iteration 10, loss = 0.33824149\n",
      "Iteration 11, loss = 0.32335091\n",
      "Iteration 12, loss = 0.30490281\n",
      "Iteration 13, loss = 0.28917467\n",
      "Iteration 14, loss = 0.27088649\n",
      "Iteration 15, loss = 0.25570799\n",
      "Iteration 16, loss = 0.25029018\n",
      "Iteration 17, loss = 0.25249857\n",
      "Iteration 18, loss = 0.23799109\n",
      "Iteration 19, loss = 0.22704775\n",
      "Iteration 20, loss = 0.22066987\n",
      "Iteration 21, loss = 0.22424977\n",
      "Iteration 22, loss = 0.21587165\n",
      "Iteration 23, loss = 0.22764682\n",
      "Iteration 24, loss = 0.21511168\n",
      "Iteration 25, loss = 0.21485969\n",
      "Iteration 26, loss = 0.21646225\n",
      "Iteration 27, loss = 0.21994906\n",
      "Iteration 28, loss = 0.21984467\n",
      "Iteration 29, loss = 0.21976110\n",
      "Iteration 30, loss = 0.21968298\n",
      "Iteration 31, loss = 0.21964198\n",
      "Iteration 32, loss = 0.21973654\n",
      "Iteration 33, loss = 0.22004906\n",
      "Iteration 34, loss = 0.22009725\n",
      "Iteration 35, loss = 0.21971218\n",
      "Iteration 36, loss = 0.21947882\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79309856\n",
      "Iteration 2, loss = 2.34600141\n",
      "Iteration 3, loss = 1.42439136\n",
      "Iteration 4, loss = 0.48436132\n",
      "Iteration 5, loss = 0.51648140\n",
      "Iteration 6, loss = 0.45020042\n",
      "Iteration 7, loss = 0.38183876\n",
      "Iteration 8, loss = 0.33328312\n",
      "Iteration 9, loss = 0.30013781\n",
      "Iteration 10, loss = 0.27296463\n",
      "Iteration 11, loss = 0.25061612\n",
      "Iteration 12, loss = 0.23882970\n",
      "Iteration 13, loss = 0.21757340\n",
      "Iteration 14, loss = 0.19185108\n",
      "Iteration 15, loss = 0.17042565\n",
      "Iteration 16, loss = 0.15706385\n",
      "Iteration 17, loss = 0.14430517\n",
      "Iteration 18, loss = 0.12449194\n",
      "Iteration 19, loss = 0.11248550\n",
      "Iteration 20, loss = 0.09815015\n",
      "Iteration 21, loss = 0.09383340\n",
      "Iteration 22, loss = 0.09186608\n",
      "Iteration 23, loss = 0.08953309\n",
      "Iteration 24, loss = 0.08774218\n",
      "Iteration 25, loss = 0.08660561\n",
      "Iteration 26, loss = 0.08584066\n",
      "Iteration 27, loss = 0.08526834\n",
      "Iteration 28, loss = 0.08469818\n",
      "Iteration 29, loss = 0.08252285\n",
      "Iteration 30, loss = 0.08071704\n",
      "Iteration 31, loss = 0.07932927\n",
      "Iteration 32, loss = 0.07842404\n",
      "Iteration 33, loss = 0.07597163\n",
      "Iteration 34, loss = 0.07069456\n",
      "Iteration 35, loss = 0.06559278\n",
      "Iteration 36, loss = 0.06216230\n",
      "Iteration 37, loss = 0.06106726\n",
      "Iteration 38, loss = 0.05954074\n",
      "Iteration 39, loss = 0.05896548\n",
      "Iteration 40, loss = 0.05846697\n",
      "Iteration 41, loss = 0.05803257\n",
      "Iteration 42, loss = 0.05861285\n",
      "Iteration 43, loss = 0.05738914\n",
      "Iteration 44, loss = 0.05713058\n",
      "Iteration 45, loss = 0.05686760\n",
      "Iteration 46, loss = 0.05659708\n",
      "Iteration 47, loss = 0.05631883\n",
      "Iteration 48, loss = 0.05603541\n",
      "Iteration 49, loss = 0.05574694\n",
      "Iteration 50, loss = 0.05545492\n",
      "Iteration 51, loss = 0.05516167\n",
      "Iteration 52, loss = 0.05486827\n",
      "Iteration 53, loss = 0.05457629\n",
      "Iteration 54, loss = 0.05428683\n",
      "Iteration 55, loss = 0.05399885\n",
      "Iteration 56, loss = 0.05371159\n",
      "Iteration 57, loss = 0.05342403\n",
      "Iteration 58, loss = 0.05313611\n",
      "Iteration 59, loss = 0.05284927\n",
      "Iteration 60, loss = 0.05256478\n",
      "Iteration 61, loss = 0.05228302\n",
      "Iteration 62, loss = 0.05200557\n",
      "Iteration 63, loss = 0.05173343\n",
      "Iteration 64, loss = 0.05146736\n",
      "Iteration 65, loss = 0.05120845\n",
      "Iteration 66, loss = 0.05095647\n",
      "Iteration 67, loss = 0.05071075\n",
      "Iteration 68, loss = 0.05047144\n",
      "Iteration 69, loss = 0.05023833\n",
      "Iteration 70, loss = 0.05001132\n",
      "Iteration 71, loss = 0.04979014\n",
      "Iteration 72, loss = 0.04957427\n",
      "Iteration 73, loss = 0.04936339\n",
      "Iteration 74, loss = 0.04915742\n",
      "Iteration 75, loss = 0.04895642\n",
      "Iteration 76, loss = 0.04876048\n",
      "Iteration 77, loss = 0.04856931\n",
      "Iteration 78, loss = 0.04838273\n",
      "Iteration 79, loss = 0.04820069\n",
      "Iteration 80, loss = 0.04802334\n",
      "Iteration 81, loss = 0.04785058\n",
      "Iteration 82, loss = 0.04768275\n",
      "Iteration 83, loss = 0.04751974\n",
      "Iteration 84, loss = 0.04736109\n",
      "Iteration 85, loss = 0.04720699\n",
      "Iteration 86, loss = 0.04705754\n",
      "Iteration 87, loss = 0.04691241\n",
      "Iteration 88, loss = 0.04677180\n",
      "Iteration 89, loss = 0.04663544\n",
      "Iteration 90, loss = 0.04650361\n",
      "Iteration 91, loss = 0.04637645\n",
      "Iteration 92, loss = 0.04625368\n",
      "Iteration 93, loss = 0.04613511\n",
      "Iteration 94, loss = 0.04602053\n",
      "Iteration 95, loss = 0.04591002\n",
      "Iteration 96, loss = 0.04580334\n",
      "Iteration 97, loss = 0.04570043\n",
      "Iteration 98, loss = 0.04560126\n",
      "Iteration 99, loss = 0.04550591\n",
      "Iteration 100, loss = 0.04541423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.99494579\n",
      "Iteration 2, loss = 20.04884469\n",
      "Iteration 3, loss = 11.11293150\n",
      "Iteration 4, loss = 7.90012228\n",
      "Iteration 5, loss = 7.95025063\n",
      "Iteration 6, loss = 4.62146427\n",
      "Iteration 7, loss = 1.34505441\n",
      "Iteration 8, loss = 2.52063068\n",
      "Iteration 9, loss = 2.71677062\n",
      "Iteration 10, loss = 1.32528514\n",
      "Iteration 11, loss = 0.95521493\n",
      "Iteration 12, loss = 0.96670061\n",
      "Iteration 13, loss = 0.83569454\n",
      "Iteration 14, loss = 0.59978901\n",
      "Iteration 15, loss = 0.42297020\n",
      "Iteration 16, loss = 0.33661459\n",
      "Iteration 17, loss = 0.28579645\n",
      "Iteration 18, loss = 0.25736413\n",
      "Iteration 19, loss = 0.24058998\n",
      "Iteration 20, loss = 0.22923091\n",
      "Iteration 21, loss = 0.22588669\n",
      "Iteration 22, loss = 0.22482676\n",
      "Iteration 23, loss = 0.22625575\n",
      "Iteration 24, loss = 0.22550484\n",
      "Iteration 25, loss = 0.22058692\n",
      "Iteration 26, loss = 0.21111315\n",
      "Iteration 27, loss = 0.19858827\n",
      "Iteration 28, loss = 0.18313276\n",
      "Iteration 29, loss = 0.16607182\n",
      "Iteration 30, loss = 0.14996804\n",
      "Iteration 31, loss = 0.13571085\n",
      "Iteration 32, loss = 0.12342508\n",
      "Iteration 33, loss = 0.11481640\n",
      "Iteration 34, loss = 0.10940481\n",
      "Iteration 35, loss = 0.10379823\n",
      "Iteration 36, loss = 0.09802639\n",
      "Iteration 37, loss = 0.09165897\n",
      "Iteration 38, loss = 0.08552651\n",
      "Iteration 39, loss = 0.07997671\n",
      "Iteration 40, loss = 0.07383355\n",
      "Iteration 41, loss = 0.06765174\n",
      "Iteration 42, loss = 0.06184487\n",
      "Iteration 43, loss = 0.05596793\n",
      "Iteration 44, loss = 0.05036082\n",
      "Iteration 45, loss = 0.04488984\n",
      "Iteration 46, loss = 0.03982069\n",
      "Iteration 47, loss = 0.03542808\n",
      "Iteration 48, loss = 0.03189685\n",
      "Iteration 49, loss = 0.02873043\n",
      "Iteration 50, loss = 0.02565243\n",
      "Iteration 51, loss = 0.02261851\n",
      "Iteration 52, loss = 0.01982517\n",
      "Iteration 53, loss = 0.01738311\n",
      "Iteration 54, loss = 0.01527535\n",
      "Iteration 55, loss = 0.01358857\n",
      "Iteration 56, loss = 0.01226043\n",
      "Iteration 57, loss = 0.01122859\n",
      "Iteration 58, loss = 0.01042365\n",
      "Iteration 59, loss = 0.00979829\n",
      "Iteration 60, loss = 0.00930839\n",
      "Iteration 61, loss = 0.00891907\n",
      "Iteration 62, loss = 0.00860450\n",
      "Iteration 63, loss = 0.00834714\n",
      "Iteration 64, loss = 0.00813711\n",
      "Iteration 65, loss = 0.00780838\n",
      "Iteration 66, loss = 0.00656830\n",
      "Iteration 67, loss = 0.00504895\n",
      "Iteration 68, loss = 0.00367767\n",
      "Iteration 69, loss = 0.00264484\n",
      "Iteration 70, loss = 0.00195908\n",
      "Iteration 71, loss = 0.00153260\n",
      "Iteration 72, loss = 0.00127187\n",
      "Iteration 73, loss = 0.00110976\n",
      "Iteration 74, loss = 0.00100457\n",
      "Iteration 75, loss = 0.00093268\n",
      "Iteration 76, loss = 0.00087990\n",
      "Iteration 77, loss = 0.00083885\n",
      "Iteration 78, loss = 0.00080508\n",
      "Iteration 79, loss = 0.00077644\n",
      "Iteration 80, loss = 0.00075113\n",
      "Iteration 81, loss = 0.00072806\n",
      "Iteration 82, loss = 0.00070668\n",
      "Iteration 83, loss = 0.00068634\n",
      "Iteration 84, loss = 0.00066705\n",
      "Iteration 85, loss = 0.00064928\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.70418471\n",
      "Iteration 2, loss = 13.14237672\n",
      "Iteration 3, loss = 8.02345203\n",
      "Iteration 4, loss = 2.26114668\n",
      "Iteration 5, loss = 7.77755579\n",
      "Iteration 6, loss = 3.60428769\n",
      "Iteration 7, loss = 4.45983115\n",
      "Iteration 8, loss = 4.28838485\n",
      "Iteration 9, loss = 1.52186860\n",
      "Iteration 10, loss = 1.27676597\n",
      "Iteration 11, loss = 1.98025589\n",
      "Iteration 12, loss = 0.45206981\n",
      "Iteration 13, loss = 0.45145396\n",
      "Iteration 14, loss = 0.74380246\n",
      "Iteration 15, loss = 0.72152019\n",
      "Iteration 16, loss = 0.39688036\n",
      "Iteration 17, loss = 0.15620679\n",
      "Iteration 18, loss = 0.14831165\n",
      "Iteration 19, loss = 0.21620850\n",
      "Iteration 20, loss = 0.21693758\n",
      "Iteration 21, loss = 0.10512955\n",
      "Iteration 22, loss = 0.02784356\n",
      "Iteration 23, loss = 0.01582042\n",
      "Iteration 24, loss = 0.03346989\n",
      "Iteration 25, loss = 0.04851046\n",
      "Iteration 26, loss = 0.03600714\n",
      "Iteration 27, loss = 0.01698646\n",
      "Iteration 28, loss = 0.00799029\n",
      "Iteration 29, loss = 0.00509433\n",
      "Iteration 30, loss = 0.00600006\n",
      "Iteration 31, loss = 0.00958728\n",
      "Iteration 32, loss = 0.01098959\n",
      "Iteration 33, loss = 0.00834422\n",
      "Iteration 34, loss = 0.00615075\n",
      "Iteration 35, loss = 0.00459486\n",
      "Iteration 36, loss = 0.00335559\n",
      "Iteration 37, loss = 0.00239748\n",
      "Iteration 38, loss = 0.00173249\n",
      "Iteration 39, loss = 0.00131719\n",
      "Iteration 40, loss = 0.00105650\n",
      "Iteration 41, loss = 0.00088879\n",
      "Iteration 42, loss = 0.00078421\n",
      "Iteration 43, loss = 0.00071926\n",
      "Iteration 44, loss = 0.00067461\n",
      "Iteration 45, loss = 0.00064186\n",
      "Iteration 46, loss = 0.00061501\n",
      "Iteration 47, loss = 0.00059088\n",
      "Iteration 48, loss = 0.00056845\n",
      "Iteration 49, loss = 0.00054691\n",
      "Iteration 50, loss = 0.00052612\n",
      "Iteration 51, loss = 0.00050673\n",
      "Iteration 52, loss = 0.00048818\n",
      "Iteration 53, loss = 0.00047051\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.61716230\n",
      "Iteration 2, loss = 17.06362273\n",
      "Iteration 3, loss = 7.23049176\n",
      "Iteration 4, loss = 4.35482634\n",
      "Iteration 5, loss = 2.31343896\n",
      "Iteration 6, loss = 1.54219397\n",
      "Iteration 7, loss = 1.32381093\n",
      "Iteration 8, loss = 1.52001480\n",
      "Iteration 9, loss = 0.42919163\n",
      "Iteration 10, loss = 0.49127171\n",
      "Iteration 11, loss = 0.69912339\n",
      "Iteration 12, loss = 0.23251374\n",
      "Iteration 13, loss = 0.09709900\n",
      "Iteration 14, loss = 0.14081656\n",
      "Iteration 15, loss = 0.19530537\n",
      "Iteration 16, loss = 0.17020629\n",
      "Iteration 17, loss = 0.08623105\n",
      "Iteration 18, loss = 0.04182246\n",
      "Iteration 19, loss = 0.03335447\n",
      "Iteration 20, loss = 0.01645273\n",
      "Iteration 21, loss = 0.01683371\n",
      "Iteration 22, loss = 0.01733085\n",
      "Iteration 23, loss = 0.01654035\n",
      "Iteration 24, loss = 0.01464443\n",
      "Iteration 25, loss = 0.01251329\n",
      "Iteration 26, loss = 0.01025447\n",
      "Iteration 27, loss = 0.00809548\n",
      "Iteration 28, loss = 0.00634741\n",
      "Iteration 29, loss = 0.00491691\n",
      "Iteration 30, loss = 0.00381820\n",
      "Iteration 31, loss = 0.00299240\n",
      "Iteration 32, loss = 0.00240280\n",
      "Iteration 33, loss = 0.00200548\n",
      "Iteration 34, loss = 0.00172764\n",
      "Iteration 35, loss = 0.00151657\n",
      "Iteration 36, loss = 0.00135779\n",
      "Iteration 37, loss = 0.00122514\n",
      "Iteration 38, loss = 0.00110794\n",
      "Iteration 39, loss = 0.00099426\n",
      "Iteration 40, loss = 0.00088376\n",
      "Iteration 41, loss = 0.00077963\n",
      "Iteration 42, loss = 0.00068766\n",
      "Iteration 43, loss = 0.00060818\n",
      "Iteration 44, loss = 0.00054306\n",
      "Iteration 45, loss = 0.00049177\n",
      "Iteration 46, loss = 0.00044976\n",
      "Iteration 47, loss = 0.00041521\n",
      "Iteration 48, loss = 0.00038848\n",
      "Iteration 49, loss = 0.00036790\n",
      "Iteration 50, loss = 0.00035181\n",
      "Iteration 51, loss = 0.00033884\n",
      "Iteration 52, loss = 0.00032822\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.73494577\n",
      "Iteration 2, loss = 8.27431179\n",
      "Iteration 3, loss = 10.51419789\n",
      "Iteration 4, loss = 5.20869807\n",
      "Iteration 5, loss = 4.38998609\n",
      "Iteration 6, loss = 4.46994633\n",
      "Iteration 7, loss = 1.03857373\n",
      "Iteration 8, loss = 3.93835185\n",
      "Iteration 9, loss = 2.48113462\n",
      "Iteration 10, loss = 0.31025612\n",
      "Iteration 11, loss = 1.38554774\n",
      "Iteration 12, loss = 1.45902208\n",
      "Iteration 13, loss = 0.43827594\n",
      "Iteration 14, loss = 0.12259215\n",
      "Iteration 15, loss = 0.39648806\n",
      "Iteration 16, loss = 0.36647621\n",
      "Iteration 17, loss = 0.09986504\n",
      "Iteration 18, loss = 0.01369378\n",
      "Iteration 19, loss = 0.05485567\n",
      "Iteration 20, loss = 0.17953930\n",
      "Iteration 21, loss = 0.10413011\n",
      "Iteration 22, loss = 0.00669239\n",
      "Iteration 23, loss = 0.00237154\n",
      "Iteration 24, loss = 0.00352903\n",
      "Iteration 25, loss = 0.00586207\n",
      "Iteration 26, loss = 0.00928825\n",
      "Iteration 27, loss = 0.01311453\n",
      "Iteration 28, loss = 0.01598785\n",
      "Iteration 29, loss = 0.01631238\n",
      "Iteration 30, loss = 0.01345210\n",
      "Iteration 31, loss = 0.00930434\n",
      "Iteration 32, loss = 0.00614840\n",
      "Iteration 33, loss = 0.00400367\n",
      "Iteration 34, loss = 0.00264417\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12367714\n",
      "Iteration 2, loss = 5.74487468\n",
      "Iteration 3, loss = 12.33381834\n",
      "Iteration 4, loss = 2.22484260\n",
      "Iteration 5, loss = 1.91686598\n",
      "Iteration 6, loss = 1.03324497\n",
      "Iteration 7, loss = 1.00347832\n",
      "Iteration 8, loss = 0.28332164\n",
      "Iteration 9, loss = 0.28721146\n",
      "Iteration 10, loss = 0.20639947\n",
      "Iteration 11, loss = 0.11457225\n",
      "Iteration 12, loss = 0.01666864\n",
      "Iteration 13, loss = 0.00454474\n",
      "Iteration 14, loss = 0.00973249\n",
      "Iteration 15, loss = 0.02967076\n",
      "Iteration 16, loss = 0.03707300\n",
      "Iteration 17, loss = 0.02042138\n",
      "Iteration 18, loss = 0.01062103\n",
      "Iteration 19, loss = 0.00923131\n",
      "Iteration 20, loss = 0.00792074\n",
      "Iteration 21, loss = 0.00599262\n",
      "Iteration 22, loss = 0.00467579\n",
      "Iteration 23, loss = 0.00366274\n",
      "Iteration 24, loss = 0.00281915\n",
      "Iteration 25, loss = 0.00210281\n",
      "Iteration 26, loss = 0.00156158\n",
      "Iteration 27, loss = 0.00117890\n",
      "Iteration 28, loss = 0.00092219\n",
      "Iteration 29, loss = 0.00075552\n",
      "Iteration 30, loss = 0.00064837\n",
      "Iteration 31, loss = 0.00057900\n",
      "Iteration 32, loss = 0.00053487\n",
      "Iteration 33, loss = 0.00051063\n",
      "Iteration 34, loss = 0.00049857\n",
      "Iteration 35, loss = 0.00049218\n",
      "Iteration 36, loss = 0.00048633\n",
      "Iteration 37, loss = 0.00047806\n",
      "Iteration 38, loss = 0.00046556\n",
      "Iteration 39, loss = 0.00044920\n",
      "Iteration 40, loss = 0.00043043\n",
      "Iteration 41, loss = 0.00041051\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "hidden_layers = [1, 10, 100]\n",
    "hidden_layer_size = [10, 100, 1000]\n",
    "activation_function = ['tanh', 'relu']\n",
    "\n",
    "models = []\n",
    "this_model = []\n",
    "\n",
    "for k in range(len(activation_function)):\n",
    "    for j in range(len(hidden_layer_size)):\n",
    "        for i in range(len(hidden_layers)):\n",
    "            this_model = []\n",
    "            for x in range(5):\n",
    "                model = MLPClassifier(max_iter=100, random_state=None, verbose=True, hidden_layer_sizes=[hidden_layers[i], hidden_layer_size[j]], activation=activation_function[k])\n",
    "                model.fit(train_songs, train_labels)\n",
    "                this_model.append(model)\n",
    "\n",
    "            models.append(this_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, iteration 0\n",
      "Hidden layers: 1 \t Hidden layer size: 10 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      1.00      0.63        17\n",
      "         1.0       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.46        37\n",
      "   macro avg       0.23      0.50      0.31        37\n",
      "weighted avg       0.21      0.46      0.29        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 0, iteration 1\n",
      "Hidden layers: 1 \t Hidden layer size: 10 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      1.00      0.74        17\n",
      "         1.0       1.00      0.40      0.57        20\n",
      "\n",
      "    accuracy                           0.68        37\n",
      "   macro avg       0.79      0.70      0.66        37\n",
      "weighted avg       0.81      0.68      0.65        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 0, iteration 2\n",
      "Hidden layers: 1 \t Hidden layer size: 10 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      1.00      0.63        17\n",
      "         1.0       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.46        37\n",
      "   macro avg       0.23      0.50      0.31        37\n",
      "weighted avg       0.21      0.46      0.29        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 0, iteration 3\n",
      "Hidden layers: 1 \t Hidden layer size: 10 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      1.00      0.77        17\n",
      "         1.0       1.00      0.50      0.67        20\n",
      "\n",
      "    accuracy                           0.73        37\n",
      "   macro avg       0.81      0.75      0.72        37\n",
      "weighted avg       0.83      0.73      0.72        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 0, iteration 4\n",
      "Hidden layers: 1 \t Hidden layer size: 10 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        17\n",
      "         1.0       0.54      1.00      0.70        20\n",
      "\n",
      "    accuracy                           0.54        37\n",
      "   macro avg       0.27      0.50      0.35        37\n",
      "weighted avg       0.29      0.54      0.38        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 1, iteration 0\n",
      "Hidden layers: 1 \t Hidden layer size: 100 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.88      0.86        17\n",
      "         1.0       0.89      0.85      0.87        20\n",
      "\n",
      "    accuracy                           0.86        37\n",
      "   macro avg       0.86      0.87      0.86        37\n",
      "weighted avg       0.87      0.86      0.87        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 1, iteration 1\n",
      "Hidden layers: 1 \t Hidden layer size: 100 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      1.00      0.79        17\n",
      "         1.0       1.00      0.55      0.71        20\n",
      "\n",
      "    accuracy                           0.76        37\n",
      "   macro avg       0.83      0.78      0.75        37\n",
      "weighted avg       0.84      0.76      0.75        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 1, iteration 2\n",
      "Hidden layers: 1 \t Hidden layer size: 100 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      1.00      0.79        17\n",
      "         1.0       1.00      0.55      0.71        20\n",
      "\n",
      "    accuracy                           0.76        37\n",
      "   macro avg       0.83      0.78      0.75        37\n",
      "weighted avg       0.84      0.76      0.75        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 1, iteration 3\n",
      "Hidden layers: 1 \t Hidden layer size: 100 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.94      0.89        17\n",
      "         1.0       0.94      0.85      0.89        20\n",
      "\n",
      "    accuracy                           0.89        37\n",
      "   macro avg       0.89      0.90      0.89        37\n",
      "weighted avg       0.90      0.89      0.89        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 1, iteration 4\n",
      "Hidden layers: 1 \t Hidden layer size: 100 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      1.00      0.87        17\n",
      "         1.0       1.00      0.75      0.86        20\n",
      "\n",
      "    accuracy                           0.86        37\n",
      "   macro avg       0.89      0.88      0.86        37\n",
      "weighted avg       0.90      0.86      0.86        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 2, iteration 0\n",
      "Hidden layers: 1 \t Hidden layer size: 1000 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.88      0.91        17\n",
      "         1.0       0.90      0.95      0.93        20\n",
      "\n",
      "    accuracy                           0.92        37\n",
      "   macro avg       0.92      0.92      0.92        37\n",
      "weighted avg       0.92      0.92      0.92        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 2, iteration 1\n",
      "Hidden layers: 1 \t Hidden layer size: 1000 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.94      0.86        17\n",
      "         1.0       0.94      0.80      0.86        20\n",
      "\n",
      "    accuracy                           0.86        37\n",
      "   macro avg       0.87      0.87      0.86        37\n",
      "weighted avg       0.88      0.86      0.86        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 2, iteration 2\n",
      "Hidden layers: 1 \t Hidden layer size: 1000 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      1.00      0.92        17\n",
      "         1.0       1.00      0.85      0.92        20\n",
      "\n",
      "    accuracy                           0.92        37\n",
      "   macro avg       0.93      0.93      0.92        37\n",
      "weighted avg       0.93      0.92      0.92        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 2, iteration 3\n",
      "Hidden layers: 1 \t Hidden layer size: 1000 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.94      0.84        17\n",
      "         1.0       0.94      0.75      0.83        20\n",
      "\n",
      "    accuracy                           0.84        37\n",
      "   macro avg       0.85      0.85      0.84        37\n",
      "weighted avg       0.86      0.84      0.84        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 2, iteration 4\n",
      "Hidden layers: 1 \t Hidden layer size: 1000 \t Activation function: tanh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.71      0.75        17\n",
      "         1.0       0.77      0.85      0.81        20\n",
      "\n",
      "    accuracy                           0.78        37\n",
      "   macro avg       0.79      0.78      0.78        37\n",
      "weighted avg       0.79      0.78      0.78        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 3, iteration 0\n",
      "Hidden layers: 10 \t Hidden layer size: 10 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      1.00      0.63        17\n",
      "         1.0       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.46        37\n",
      "   macro avg       0.23      0.50      0.31        37\n",
      "weighted avg       0.21      0.46      0.29        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 3, iteration 1\n",
      "Hidden layers: 10 \t Hidden layer size: 10 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.59      0.71        17\n",
      "         1.0       0.73      0.95      0.83        20\n",
      "\n",
      "    accuracy                           0.78        37\n",
      "   macro avg       0.82      0.77      0.77        37\n",
      "weighted avg       0.81      0.78      0.77        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 3, iteration 2\n",
      "Hidden layers: 10 \t Hidden layer size: 10 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      1.00      0.69        17\n",
      "         1.0       1.00      0.25      0.40        20\n",
      "\n",
      "    accuracy                           0.59        37\n",
      "   macro avg       0.77      0.62      0.55        37\n",
      "weighted avg       0.78      0.59      0.54        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 3, iteration 3\n",
      "Hidden layers: 10 \t Hidden layer size: 10 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      1.00      0.77        17\n",
      "         1.0       1.00      0.50      0.67        20\n",
      "\n",
      "    accuracy                           0.73        37\n",
      "   macro avg       0.81      0.75      0.72        37\n",
      "weighted avg       0.83      0.73      0.72        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 3, iteration 4\n",
      "Hidden layers: 10 \t Hidden layer size: 10 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82        17\n",
      "         1.0       0.85      0.85      0.85        20\n",
      "\n",
      "    accuracy                           0.84        37\n",
      "   macro avg       0.84      0.84      0.84        37\n",
      "weighted avg       0.84      0.84      0.84        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 4, iteration 0\n",
      "Hidden layers: 10 \t Hidden layer size: 100 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.88      0.86        17\n",
      "         1.0       0.89      0.85      0.87        20\n",
      "\n",
      "    accuracy                           0.86        37\n",
      "   macro avg       0.86      0.87      0.86        37\n",
      "weighted avg       0.87      0.86      0.87        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 4, iteration 1\n",
      "Hidden layers: 10 \t Hidden layer size: 100 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.94      0.84        17\n",
      "         1.0       0.94      0.75      0.83        20\n",
      "\n",
      "    accuracy                           0.84        37\n",
      "   macro avg       0.85      0.85      0.84        37\n",
      "weighted avg       0.86      0.84      0.84        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 4, iteration 2\n",
      "Hidden layers: 10 \t Hidden layer size: 100 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.94      0.89        17\n",
      "         1.0       0.94      0.85      0.89        20\n",
      "\n",
      "    accuracy                           0.89        37\n",
      "   macro avg       0.89      0.90      0.89        37\n",
      "weighted avg       0.90      0.89      0.89        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 4, iteration 3\n",
      "Hidden layers: 10 \t Hidden layer size: 100 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82        17\n",
      "         1.0       0.85      0.85      0.85        20\n",
      "\n",
      "    accuracy                           0.84        37\n",
      "   macro avg       0.84      0.84      0.84        37\n",
      "weighted avg       0.84      0.84      0.84        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 4, iteration 4\n",
      "Hidden layers: 10 \t Hidden layer size: 100 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      1.00      0.87        17\n",
      "         1.0       1.00      0.75      0.86        20\n",
      "\n",
      "    accuracy                           0.86        37\n",
      "   macro avg       0.89      0.88      0.86        37\n",
      "weighted avg       0.90      0.86      0.86        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 5, iteration 0\n",
      "Hidden layers: 10 \t Hidden layer size: 1000 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.76      0.81        17\n",
      "         1.0       0.82      0.90      0.86        20\n",
      "\n",
      "    accuracy                           0.84        37\n",
      "   macro avg       0.84      0.83      0.83        37\n",
      "weighted avg       0.84      0.84      0.84        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 5, iteration 1\n",
      "Hidden layers: 10 \t Hidden layer size: 1000 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.94      0.86        17\n",
      "         1.0       0.94      0.80      0.86        20\n",
      "\n",
      "    accuracy                           0.86        37\n",
      "   macro avg       0.87      0.87      0.86        37\n",
      "weighted avg       0.88      0.86      0.86        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 5, iteration 2\n",
      "Hidden layers: 10 \t Hidden layer size: 1000 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.94      0.91        17\n",
      "         1.0       0.95      0.90      0.92        20\n",
      "\n",
      "    accuracy                           0.92        37\n",
      "   macro avg       0.92      0.92      0.92        37\n",
      "weighted avg       0.92      0.92      0.92        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 5, iteration 3\n",
      "Hidden layers: 10 \t Hidden layer size: 1000 \t Activation function: tanh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.82      0.80        17\n",
      "         1.0       0.84      0.80      0.82        20\n",
      "\n",
      "    accuracy                           0.81        37\n",
      "   macro avg       0.81      0.81      0.81        37\n",
      "weighted avg       0.81      0.81      0.81        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 5, iteration 4\n",
      "Hidden layers: 10 \t Hidden layer size: 1000 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.88      0.88        17\n",
      "         1.0       0.90      0.90      0.90        20\n",
      "\n",
      "    accuracy                           0.89        37\n",
      "   macro avg       0.89      0.89      0.89        37\n",
      "weighted avg       0.89      0.89      0.89        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 6, iteration 0\n",
      "Hidden layers: 100 \t Hidden layer size: 10 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.94      0.86        17\n",
      "         1.0       0.94      0.80      0.86        20\n",
      "\n",
      "    accuracy                           0.86        37\n",
      "   macro avg       0.87      0.87      0.86        37\n",
      "weighted avg       0.88      0.86      0.86        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 6, iteration 1\n",
      "Hidden layers: 100 \t Hidden layer size: 10 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      1.00      0.89        17\n",
      "         1.0       1.00      0.80      0.89        20\n",
      "\n",
      "    accuracy                           0.89        37\n",
      "   macro avg       0.90      0.90      0.89        37\n",
      "weighted avg       0.91      0.89      0.89        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 6, iteration 2\n",
      "Hidden layers: 100 \t Hidden layer size: 10 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.65      0.71        17\n",
      "         1.0       0.74      0.85      0.79        20\n",
      "\n",
      "    accuracy                           0.76        37\n",
      "   macro avg       0.76      0.75      0.75        37\n",
      "weighted avg       0.76      0.76      0.75        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 6, iteration 3\n",
      "Hidden layers: 100 \t Hidden layer size: 10 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      1.00      0.63        17\n",
      "         1.0       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.46        37\n",
      "   macro avg       0.23      0.50      0.31        37\n",
      "weighted avg       0.21      0.46      0.29        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 6, iteration 4\n",
      "Hidden layers: 100 \t Hidden layer size: 10 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.59      0.71        17\n",
      "         1.0       0.73      0.95      0.83        20\n",
      "\n",
      "    accuracy                           0.78        37\n",
      "   macro avg       0.82      0.77      0.77        37\n",
      "weighted avg       0.81      0.78      0.77        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 7, iteration 0\n",
      "Hidden layers: 100 \t Hidden layer size: 100 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82        17\n",
      "         1.0       0.85      0.85      0.85        20\n",
      "\n",
      "    accuracy                           0.84        37\n",
      "   macro avg       0.84      0.84      0.84        37\n",
      "weighted avg       0.84      0.84      0.84        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 7, iteration 1\n",
      "Hidden layers: 100 \t Hidden layer size: 100 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      1.00      0.89        17\n",
      "         1.0       1.00      0.80      0.89        20\n",
      "\n",
      "    accuracy                           0.89        37\n",
      "   macro avg       0.90      0.90      0.89        37\n",
      "weighted avg       0.91      0.89      0.89        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 7, iteration 2\n",
      "Hidden layers: 100 \t Hidden layer size: 100 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      1.00      0.89        17\n",
      "         1.0       1.00      0.80      0.89        20\n",
      "\n",
      "    accuracy                           0.89        37\n",
      "   macro avg       0.90      0.90      0.89        37\n",
      "weighted avg       0.91      0.89      0.89        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 7, iteration 3\n",
      "Hidden layers: 100 \t Hidden layer size: 100 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      1.00      0.87        17\n",
      "         1.0       1.00      0.75      0.86        20\n",
      "\n",
      "    accuracy                           0.86        37\n",
      "   macro avg       0.89      0.88      0.86        37\n",
      "weighted avg       0.90      0.86      0.86        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 7, iteration 4\n",
      "Hidden layers: 100 \t Hidden layer size: 100 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      1.00      0.87        17\n",
      "         1.0       1.00      0.75      0.86        20\n",
      "\n",
      "    accuracy                           0.86        37\n",
      "   macro avg       0.89      0.88      0.86        37\n",
      "weighted avg       0.90      0.86      0.86        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 8, iteration 0\n",
      "Hidden layers: 100 \t Hidden layer size: 1000 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.65      0.73        17\n",
      "         1.0       0.75      0.90      0.82        20\n",
      "\n",
      "    accuracy                           0.78        37\n",
      "   macro avg       0.80      0.77      0.78        37\n",
      "weighted avg       0.79      0.78      0.78        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 8, iteration 1\n",
      "Hidden layers: 100 \t Hidden layer size: 1000 \t Activation function: tanh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.94      0.84        17\n",
      "         1.0       0.94      0.75      0.83        20\n",
      "\n",
      "    accuracy                           0.84        37\n",
      "   macro avg       0.85      0.85      0.84        37\n",
      "weighted avg       0.86      0.84      0.84        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 8, iteration 2\n",
      "Hidden layers: 100 \t Hidden layer size: 1000 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.94      0.84        17\n",
      "         1.0       0.94      0.75      0.83        20\n",
      "\n",
      "    accuracy                           0.84        37\n",
      "   macro avg       0.85      0.85      0.84        37\n",
      "weighted avg       0.86      0.84      0.84        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 8, iteration 3\n",
      "Hidden layers: 100 \t Hidden layer size: 1000 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82        17\n",
      "         1.0       0.85      0.85      0.85        20\n",
      "\n",
      "    accuracy                           0.84        37\n",
      "   macro avg       0.84      0.84      0.84        37\n",
      "weighted avg       0.84      0.84      0.84        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 8, iteration 4\n",
      "Hidden layers: 100 \t Hidden layer size: 1000 \t Activation function: tanh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.94      0.86        17\n",
      "         1.0       0.94      0.80      0.86        20\n",
      "\n",
      "    accuracy                           0.86        37\n",
      "   macro avg       0.87      0.87      0.86        37\n",
      "weighted avg       0.88      0.86      0.86        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 9, iteration 0\n",
      "Hidden layers: 1 \t Hidden layer size: 10 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        17\n",
      "         1.0       0.54      1.00      0.70        20\n",
      "\n",
      "    accuracy                           0.54        37\n",
      "   macro avg       0.27      0.50      0.35        37\n",
      "weighted avg       0.29      0.54      0.38        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 9, iteration 1\n",
      "Hidden layers: 1 \t Hidden layer size: 10 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      1.00      0.63        17\n",
      "         1.0       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.46        37\n",
      "   macro avg       0.23      0.50      0.31        37\n",
      "weighted avg       0.21      0.46      0.29        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 9, iteration 2\n",
      "Hidden layers: 1 \t Hidden layer size: 10 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      1.00      0.63        17\n",
      "         1.0       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.46        37\n",
      "   macro avg       0.23      0.50      0.31        37\n",
      "weighted avg       0.21      0.46      0.29        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 9, iteration 3\n",
      "Hidden layers: 1 \t Hidden layer size: 10 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      1.00      0.89        17\n",
      "         1.0       1.00      0.80      0.89        20\n",
      "\n",
      "    accuracy                           0.89        37\n",
      "   macro avg       0.90      0.90      0.89        37\n",
      "weighted avg       0.91      0.89      0.89        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 9, iteration 4\n",
      "Hidden layers: 1 \t Hidden layer size: 10 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      1.00      0.87        17\n",
      "         1.0       1.00      0.75      0.86        20\n",
      "\n",
      "    accuracy                           0.86        37\n",
      "   macro avg       0.89      0.88      0.86        37\n",
      "weighted avg       0.90      0.86      0.86        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 10, iteration 0\n",
      "Hidden layers: 1 \t Hidden layer size: 100 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.94      0.94        17\n",
      "         1.0       0.95      0.95      0.95        20\n",
      "\n",
      "    accuracy                           0.95        37\n",
      "   macro avg       0.95      0.95      0.95        37\n",
      "weighted avg       0.95      0.95      0.95        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 10, iteration 1\n",
      "Hidden layers: 1 \t Hidden layer size: 100 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.94      0.84        17\n",
      "         1.0       0.94      0.75      0.83        20\n",
      "\n",
      "    accuracy                           0.84        37\n",
      "   macro avg       0.85      0.85      0.84        37\n",
      "weighted avg       0.86      0.84      0.84        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 10, iteration 2\n",
      "Hidden layers: 1 \t Hidden layer size: 100 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.59      0.71        17\n",
      "         1.0       0.73      0.95      0.83        20\n",
      "\n",
      "    accuracy                           0.78        37\n",
      "   macro avg       0.82      0.77      0.77        37\n",
      "weighted avg       0.81      0.78      0.77        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 10, iteration 3\n",
      "Hidden layers: 1 \t Hidden layer size: 100 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.94      0.94        17\n",
      "         1.0       0.95      0.95      0.95        20\n",
      "\n",
      "    accuracy                           0.95        37\n",
      "   macro avg       0.95      0.95      0.95        37\n",
      "weighted avg       0.95      0.95      0.95        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 10, iteration 4\n",
      "Hidden layers: 1 \t Hidden layer size: 100 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      1.00      0.63        17\n",
      "         1.0       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.46        37\n",
      "   macro avg       0.23      0.50      0.31        37\n",
      "weighted avg       0.21      0.46      0.29        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 11, iteration 0\n",
      "Hidden layers: 1 \t Hidden layer size: 1000 \t Activation function: relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.94      0.91        17\n",
      "         1.0       0.95      0.90      0.92        20\n",
      "\n",
      "    accuracy                           0.92        37\n",
      "   macro avg       0.92      0.92      0.92        37\n",
      "weighted avg       0.92      0.92      0.92        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 11, iteration 1\n",
      "Hidden layers: 1 \t Hidden layer size: 1000 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.94      0.86        17\n",
      "         1.0       0.94      0.80      0.86        20\n",
      "\n",
      "    accuracy                           0.86        37\n",
      "   macro avg       0.87      0.87      0.86        37\n",
      "weighted avg       0.88      0.86      0.86        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 11, iteration 2\n",
      "Hidden layers: 1 \t Hidden layer size: 1000 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      1.00      0.77        17\n",
      "         1.0       1.00      0.50      0.67        20\n",
      "\n",
      "    accuracy                           0.73        37\n",
      "   macro avg       0.81      0.75      0.72        37\n",
      "weighted avg       0.83      0.73      0.72        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 11, iteration 3\n",
      "Hidden layers: 1 \t Hidden layer size: 1000 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.65      0.69        17\n",
      "         1.0       0.73      0.80      0.76        20\n",
      "\n",
      "    accuracy                           0.73        37\n",
      "   macro avg       0.73      0.72      0.72        37\n",
      "weighted avg       0.73      0.73      0.73        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 11, iteration 4\n",
      "Hidden layers: 1 \t Hidden layer size: 1000 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.71      0.80        17\n",
      "         1.0       0.79      0.95      0.86        20\n",
      "\n",
      "    accuracy                           0.84        37\n",
      "   macro avg       0.86      0.83      0.83        37\n",
      "weighted avg       0.85      0.84      0.83        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 12, iteration 0\n",
      "Hidden layers: 10 \t Hidden layer size: 10 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      1.00      0.63        17\n",
      "         1.0       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.46        37\n",
      "   macro avg       0.23      0.50      0.31        37\n",
      "weighted avg       0.21      0.46      0.29        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 12, iteration 1\n",
      "Hidden layers: 10 \t Hidden layer size: 10 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      1.00      0.63        17\n",
      "         1.0       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.46        37\n",
      "   macro avg       0.23      0.50      0.31        37\n",
      "weighted avg       0.21      0.46      0.29        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 12, iteration 2\n",
      "Hidden layers: 10 \t Hidden layer size: 10 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      1.00      0.63        17\n",
      "         1.0       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.46        37\n",
      "   macro avg       0.23      0.50      0.31        37\n",
      "weighted avg       0.21      0.46      0.29        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 12, iteration 3\n",
      "Hidden layers: 10 \t Hidden layer size: 10 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      1.00      0.63        17\n",
      "         1.0       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.46        37\n",
      "   macro avg       0.23      0.50      0.31        37\n",
      "weighted avg       0.21      0.46      0.29        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 12, iteration 4\n",
      "Hidden layers: 10 \t Hidden layer size: 10 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      1.00      0.87        17\n",
      "         1.0       1.00      0.75      0.86        20\n",
      "\n",
      "    accuracy                           0.86        37\n",
      "   macro avg       0.89      0.88      0.86        37\n",
      "weighted avg       0.90      0.86      0.86        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 13, iteration 0\n",
      "Hidden layers: 10 \t Hidden layer size: 100 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.88      0.86        17\n",
      "         1.0       0.89      0.85      0.87        20\n",
      "\n",
      "    accuracy                           0.86        37\n",
      "   macro avg       0.86      0.87      0.86        37\n",
      "weighted avg       0.87      0.86      0.87        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 13, iteration 1\n",
      "Hidden layers: 10 \t Hidden layer size: 100 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      1.00      0.87        17\n",
      "         1.0       1.00      0.75      0.86        20\n",
      "\n",
      "    accuracy                           0.86        37\n",
      "   macro avg       0.89      0.88      0.86        37\n",
      "weighted avg       0.90      0.86      0.86        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 13, iteration 2\n",
      "Hidden layers: 10 \t Hidden layer size: 100 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      1.00      0.89        17\n",
      "         1.0       1.00      0.80      0.89        20\n",
      "\n",
      "    accuracy                           0.89        37\n",
      "   macro avg       0.90      0.90      0.89        37\n",
      "weighted avg       0.91      0.89      0.89        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 13, iteration 3\n",
      "Hidden layers: 10 \t Hidden layer size: 100 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.94      0.91        17\n",
      "         1.0       0.95      0.90      0.92        20\n",
      "\n",
      "    accuracy                           0.92        37\n",
      "   macro avg       0.92      0.92      0.92        37\n",
      "weighted avg       0.92      0.92      0.92        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 13, iteration 4\n",
      "Hidden layers: 10 \t Hidden layer size: 100 \t Activation function: relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.94      0.84        17\n",
      "         1.0       0.94      0.75      0.83        20\n",
      "\n",
      "    accuracy                           0.84        37\n",
      "   macro avg       0.85      0.85      0.84        37\n",
      "weighted avg       0.86      0.84      0.84        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 14, iteration 0\n",
      "Hidden layers: 10 \t Hidden layer size: 1000 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.76      0.70        17\n",
      "         1.0       0.76      0.65      0.70        20\n",
      "\n",
      "    accuracy                           0.70        37\n",
      "   macro avg       0.71      0.71      0.70        37\n",
      "weighted avg       0.71      0.70      0.70        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 14, iteration 1\n",
      "Hidden layers: 10 \t Hidden layer size: 1000 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82        17\n",
      "         1.0       0.85      0.85      0.85        20\n",
      "\n",
      "    accuracy                           0.84        37\n",
      "   macro avg       0.84      0.84      0.84        37\n",
      "weighted avg       0.84      0.84      0.84        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 14, iteration 2\n",
      "Hidden layers: 10 \t Hidden layer size: 1000 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.94      0.86        17\n",
      "         1.0       0.94      0.80      0.86        20\n",
      "\n",
      "    accuracy                           0.86        37\n",
      "   macro avg       0.87      0.87      0.86        37\n",
      "weighted avg       0.88      0.86      0.86        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 14, iteration 3\n",
      "Hidden layers: 10 \t Hidden layer size: 1000 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.71      0.71        17\n",
      "         1.0       0.75      0.75      0.75        20\n",
      "\n",
      "    accuracy                           0.73        37\n",
      "   macro avg       0.73      0.73      0.73        37\n",
      "weighted avg       0.73      0.73      0.73        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 14, iteration 4\n",
      "Hidden layers: 10 \t Hidden layer size: 1000 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.82      0.85        17\n",
      "         1.0       0.86      0.90      0.88        20\n",
      "\n",
      "    accuracy                           0.86        37\n",
      "   macro avg       0.87      0.86      0.86        37\n",
      "weighted avg       0.87      0.86      0.86        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 15, iteration 0\n",
      "Hidden layers: 100 \t Hidden layer size: 10 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      1.00      0.87        17\n",
      "         1.0       1.00      0.75      0.86        20\n",
      "\n",
      "    accuracy                           0.86        37\n",
      "   macro avg       0.89      0.88      0.86        37\n",
      "weighted avg       0.90      0.86      0.86        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 15, iteration 1\n",
      "Hidden layers: 100 \t Hidden layer size: 10 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      1.00      0.63        17\n",
      "         1.0       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.46        37\n",
      "   macro avg       0.23      0.50      0.31        37\n",
      "weighted avg       0.21      0.46      0.29        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 15, iteration 2\n",
      "Hidden layers: 100 \t Hidden layer size: 10 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      1.00      0.87        17\n",
      "         1.0       1.00      0.75      0.86        20\n",
      "\n",
      "    accuracy                           0.86        37\n",
      "   macro avg       0.89      0.88      0.86        37\n",
      "weighted avg       0.90      0.86      0.86        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 15, iteration 3\n",
      "Hidden layers: 100 \t Hidden layer size: 10 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      1.00      0.63        17\n",
      "         1.0       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.46        37\n",
      "   macro avg       0.23      0.50      0.31        37\n",
      "weighted avg       0.21      0.46      0.29        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 15, iteration 4\n",
      "Hidden layers: 100 \t Hidden layer size: 10 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      1.00      0.63        17\n",
      "         1.0       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.46        37\n",
      "   macro avg       0.23      0.50      0.31        37\n",
      "weighted avg       0.21      0.46      0.29        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 16, iteration 0\n",
      "Hidden layers: 100 \t Hidden layer size: 100 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.88      0.86        17\n",
      "         1.0       0.89      0.85      0.87        20\n",
      "\n",
      "    accuracy                           0.86        37\n",
      "   macro avg       0.86      0.87      0.86        37\n",
      "weighted avg       0.87      0.86      0.87        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 16, iteration 1\n",
      "Hidden layers: 100 \t Hidden layer size: 100 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.94      0.89        17\n",
      "         1.0       0.94      0.85      0.89        20\n",
      "\n",
      "    accuracy                           0.89        37\n",
      "   macro avg       0.89      0.90      0.89        37\n",
      "weighted avg       0.90      0.89      0.89        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 16, iteration 2\n",
      "Hidden layers: 100 \t Hidden layer size: 100 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.88      0.88        17\n",
      "         1.0       0.90      0.90      0.90        20\n",
      "\n",
      "    accuracy                           0.89        37\n",
      "   macro avg       0.89      0.89      0.89        37\n",
      "weighted avg       0.89      0.89      0.89        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 16, iteration 3\n",
      "Hidden layers: 100 \t Hidden layer size: 100 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      1.00      0.87        17\n",
      "         1.0       1.00      0.75      0.86        20\n",
      "\n",
      "    accuracy                           0.86        37\n",
      "   macro avg       0.89      0.88      0.86        37\n",
      "weighted avg       0.90      0.86      0.86        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 16, iteration 4\n",
      "Hidden layers: 100 \t Hidden layer size: 100 \t Activation function: relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.88      0.88        17\n",
      "         1.0       0.90      0.90      0.90        20\n",
      "\n",
      "    accuracy                           0.89        37\n",
      "   macro avg       0.89      0.89      0.89        37\n",
      "weighted avg       0.89      0.89      0.89        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 17, iteration 0\n",
      "Hidden layers: 100 \t Hidden layer size: 1000 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.88      0.88        17\n",
      "         1.0       0.90      0.90      0.90        20\n",
      "\n",
      "    accuracy                           0.89        37\n",
      "   macro avg       0.89      0.89      0.89        37\n",
      "weighted avg       0.89      0.89      0.89        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 17, iteration 1\n",
      "Hidden layers: 100 \t Hidden layer size: 1000 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.88      0.83        17\n",
      "         1.0       0.89      0.80      0.84        20\n",
      "\n",
      "    accuracy                           0.84        37\n",
      "   macro avg       0.84      0.84      0.84        37\n",
      "weighted avg       0.84      0.84      0.84        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 17, iteration 2\n",
      "Hidden layers: 100 \t Hidden layer size: 1000 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.82      0.80        17\n",
      "         1.0       0.84      0.80      0.82        20\n",
      "\n",
      "    accuracy                           0.81        37\n",
      "   macro avg       0.81      0.81      0.81        37\n",
      "weighted avg       0.81      0.81      0.81        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 17, iteration 3\n",
      "Hidden layers: 100 \t Hidden layer size: 1000 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.59      0.69        17\n",
      "         1.0       0.72      0.90      0.80        20\n",
      "\n",
      "    accuracy                           0.76        37\n",
      "   macro avg       0.78      0.74      0.74        37\n",
      "weighted avg       0.77      0.76      0.75        37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 17, iteration 4\n",
      "Hidden layers: 100 \t Hidden layer size: 1000 \t Activation function: relu\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.88      0.91        17\n",
      "         1.0       0.90      0.95      0.93        20\n",
      "\n",
      "    accuracy                           0.92        37\n",
      "   macro avg       0.92      0.92      0.92        37\n",
      "weighted avg       0.92      0.92      0.92        37\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_idx = 0\n",
    "models_predicted = []\n",
    "\n",
    "for k in range(len(activation_function)):\n",
    "    for j in range(len(hidden_layer_size)):\n",
    "        for i in range(len(hidden_layers)):\n",
    "            this_model_predicted = []\n",
    "            for x in range(5):\n",
    "                print(f\"Model {model_idx}, iteration {x}\")\n",
    "                print(f\"Hidden layers: {hidden_layers[j]} \\t Hidden layer size: {hidden_layer_size[i]} \\t Activation function: {activation_function[k]}\")\n",
    "                label_predict = models[model_idx][x].predict(test_songs)\n",
    "                print(metrics.classification_report(test_labels, label_predict))\n",
    "                print('\\n\\n')\n",
    "                this_model_predicted.append(label_predict)\n",
    "            models_predicted.append(this_model_predicted)\n",
    "            model_idx+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_idx = 17\n",
    "# iter_idx = 0\n",
    "\n",
    "# for i in range(len(test_songs)):\n",
    "#     probs = models[model_idx][iter_idx].predict_proba(test_songs)\n",
    "#     pred = models[model_idx][iter_idx].predict(test_songs)\n",
    "#     pred_OST = \"ULTRAKILL\" if pred[i] == 1 else \"Super Mario Galaxy\"\n",
    "#     actual_OST = \"ULTRAKILL\" if test_labels[i] == 1 else \"Super Mario Galaxy\"\n",
    "#     print(f\"{test_titles[i]} ({actual_OST}): {pred_OST} (likelihood: {np.round(max(probs[i]), 3)})\")\n",
    "#     print()\n",
    "\n",
    "# print(metrics.classification_report(test_labels, pred))\n",
    "\n",
    "# print(list(zip(test_labels, test_titles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n",
      "Hidden layers: 1 \t Hidden layer size: 10 \t Activation function: tanh\n",
      "\n",
      "\tPrecision:\n",
      "\t\tULTRAKILL: 0.38\n",
      "\t\tMario Galaxy: 0.8\n",
      "\n",
      "\tRecall:\n",
      "\t\tULTRAKILL: 0.508\n",
      "\t\tMario Galaxy: 0.427\n",
      "\n",
      "\tF1:\n",
      "\t\tULTRAKILL: 0.388\n",
      "\t\tMario Galaxy: 0.554\n",
      "\n",
      "\tAccuracy: 0.573\n",
      "\tMacro F1: 0.388\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 1\n",
      "Hidden layers: 10 \t Hidden layer size: 10 \t Activation function: tanh\n",
      "\n",
      "\tPrecision:\n",
      "\t\tULTRAKILL: 0.71\n",
      "\t\tMario Galaxy: 0.965\n",
      "\n",
      "\tRecall:\n",
      "\t\tULTRAKILL: 0.968\n",
      "\t\tMario Galaxy: 0.751\n",
      "\n",
      "\tF1:\n",
      "\t\tULTRAKILL: 0.809\n",
      "\t\tMario Galaxy: 0.84\n",
      "\n",
      "\tAccuracy: 0.827\n",
      "\tMacro F1: 0.809\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 2\n",
      "Hidden layers: 100 \t Hidden layer size: 10 \t Activation function: tanh\n",
      "\n",
      "\tPrecision:\n",
      "\t\tULTRAKILL: 0.84\n",
      "\t\tMario Galaxy: 0.894\n",
      "\n",
      "\tRecall:\n",
      "\t\tULTRAKILL: 0.911\n",
      "\t\tMario Galaxy: 0.83\n",
      "\n",
      "\tF1:\n",
      "\t\tULTRAKILL: 0.871\n",
      "\t\tMario Galaxy: 0.857\n",
      "\n",
      "\tAccuracy: 0.865\n",
      "\tMacro F1: 0.871\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 3\n",
      "Hidden layers: 1 \t Hidden layer size: 100 \t Activation function: tanh\n",
      "\n",
      "\tPrecision:\n",
      "\t\tULTRAKILL: 0.51\n",
      "\t\tMario Galaxy: 0.882\n",
      "\n",
      "\tRecall:\n",
      "\t\tULTRAKILL: 0.716\n",
      "\t\tMario Galaxy: 0.671\n",
      "\n",
      "\tF1:\n",
      "\t\tULTRAKILL: 0.549\n",
      "\t\tMario Galaxy: 0.727\n",
      "\n",
      "\tAccuracy: 0.681\n",
      "\tMacro F1: 0.549\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 4\n",
      "Hidden layers: 10 \t Hidden layer size: 100 \t Activation function: tanh\n",
      "\n",
      "\tPrecision:\n",
      "\t\tULTRAKILL: 0.81\n",
      "\t\tMario Galaxy: 0.918\n",
      "\n",
      "\tRecall:\n",
      "\t\tULTRAKILL: 0.925\n",
      "\t\tMario Galaxy: 0.807\n",
      "\n",
      "\tF1:\n",
      "\t\tULTRAKILL: 0.861\n",
      "\t\tMario Galaxy: 0.857\n",
      "\n",
      "\tAccuracy: 0.859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMacro F1: 0.861\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 5\n",
      "Hidden layers: 100 \t Hidden layer size: 100 \t Activation function: tanh\n",
      "\n",
      "\tPrecision:\n",
      "\t\tULTRAKILL: 0.86\n",
      "\t\tMario Galaxy: 0.871\n",
      "\n",
      "\tRecall:\n",
      "\t\tULTRAKILL: 0.89\n",
      "\t\tMario Galaxy: 0.843\n",
      "\n",
      "\tF1:\n",
      "\t\tULTRAKILL: 0.873\n",
      "\t\tMario Galaxy: 0.855\n",
      "\n",
      "\tAccuracy: 0.865\n",
      "\tMacro F1: 0.873\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 6\n",
      "Hidden layers: 1 \t Hidden layer size: 1000 \t Activation function: tanh\n",
      "\n",
      "\tPrecision:\n",
      "\t\tULTRAKILL: 0.68\n",
      "\t\tMario Galaxy: 0.835\n",
      "\n",
      "\tRecall:\n",
      "\t\tULTRAKILL: 0.682\n",
      "\t\tMario Galaxy: 0.753\n",
      "\n",
      "\tF1:\n",
      "\t\tULTRAKILL: 0.674\n",
      "\t\tMario Galaxy: 0.763\n",
      "\n",
      "\tAccuracy: 0.751\n",
      "\tMacro F1: 0.674\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 7\n",
      "Hidden layers: 10 \t Hidden layer size: 1000 \t Activation function: tanh\n",
      "\n",
      "\tPrecision:\n",
      "\t\tULTRAKILL: 0.79\n",
      "\t\tMario Galaxy: 0.965\n",
      "\n",
      "\tRecall:\n",
      "\t\tULTRAKILL: 0.97\n",
      "\t\tMario Galaxy: 0.798\n",
      "\n",
      "\tF1:\n",
      "\t\tULTRAKILL: 0.868\n",
      "\t\tMario Galaxy: 0.871\n",
      "\n",
      "\tAccuracy: 0.87\n",
      "\tMacro F1: 0.868\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 8\n",
      "Hidden layers: 100 \t Hidden layer size: 1000 \t Activation function: tanh\n",
      "\n",
      "\tPrecision:\n",
      "\t\tULTRAKILL: 0.81\n",
      "\t\tMario Galaxy: 0.859\n",
      "\n",
      "\tRecall:\n",
      "\t\tULTRAKILL: 0.883\n",
      "\t\tMario Galaxy: 0.799\n",
      "\n",
      "\tF1:\n",
      "\t\tULTRAKILL: 0.84\n",
      "\t\tMario Galaxy: 0.821\n",
      "\n",
      "\tAccuracy: 0.832\n",
      "\tMacro F1: 0.84\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 9\n",
      "Hidden layers: 1 \t Hidden layer size: 10 \t Activation function: relu\n",
      "\n",
      "\tPrecision:\n",
      "\t\tULTRAKILL: 0.51\n",
      "\t\tMario Galaxy: 0.8\n",
      "\n",
      "\tRecall:\n",
      "\t\tULTRAKILL: 0.508\n",
      "\t\tMario Galaxy: 0.5\n",
      "\n",
      "\tF1:\n",
      "\t\tULTRAKILL: 0.49\n",
      "\t\tMario Galaxy: 0.605\n",
      "\n",
      "\tAccuracy: 0.643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMacro F1: 0.49\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 10\n",
      "Hidden layers: 10 \t Hidden layer size: 10 \t Activation function: relu\n",
      "\n",
      "\tPrecision:\n",
      "\t\tULTRAKILL: 0.72\n",
      "\t\tMario Galaxy: 0.882\n",
      "\n",
      "\tRecall:\n",
      "\t\tULTRAKILL: 0.714\n",
      "\t\tMario Galaxy: 0.803\n",
      "\n",
      "\tF1:\n",
      "\t\tULTRAKILL: 0.712\n",
      "\t\tMario Galaxy: 0.814\n",
      "\n",
      "\tAccuracy: 0.795\n",
      "\tMacro F1: 0.712\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 11\n",
      "Hidden layers: 100 \t Hidden layer size: 10 \t Activation function: relu\n",
      "\n",
      "\tPrecision:\n",
      "\t\tULTRAKILL: 0.79\n",
      "\t\tMario Galaxy: 0.847\n",
      "\n",
      "\tRecall:\n",
      "\t\tULTRAKILL: 0.881\n",
      "\t\tMario Galaxy: 0.795\n",
      "\n",
      "\tF1:\n",
      "\t\tULTRAKILL: 0.816\n",
      "\t\tMario Galaxy: 0.808\n",
      "\n",
      "\tAccuracy: 0.816\n",
      "\tMacro F1: 0.816\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 12\n",
      "Hidden layers: 1 \t Hidden layer size: 100 \t Activation function: relu\n",
      "\n",
      "\tPrecision:\n",
      "\t\tULTRAKILL: 0.15\n",
      "\t\tMario Galaxy: 1.0\n",
      "\n",
      "\tRecall:\n",
      "\t\tULTRAKILL: 0.2\n",
      "\t\tMario Galaxy: 0.522\n",
      "\n",
      "\tF1:\n",
      "\t\tULTRAKILL: 0.171\n",
      "\t\tMario Galaxy: 0.678\n",
      "\n",
      "\tAccuracy: 0.541\n",
      "\tMacro F1: 0.171\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 13\n",
      "Hidden layers: 10 \t Hidden layer size: 100 \t Activation function: relu\n",
      "\n",
      "\tPrecision:\n",
      "\t\tULTRAKILL: 0.81\n",
      "\t\tMario Galaxy: 0.953\n",
      "\n",
      "\tRecall:\n",
      "\t\tULTRAKILL: 0.956\n",
      "\t\tMario Galaxy: 0.813\n",
      "\n",
      "\tF1:\n",
      "\t\tULTRAKILL: 0.875\n",
      "\t\tMario Galaxy: 0.876\n",
      "\n",
      "\tAccuracy: 0.876\n",
      "\tMacro F1: 0.875\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 14\n",
      "Hidden layers: 100 \t Hidden layer size: 100 \t Activation function: relu\n",
      "\n",
      "\tPrecision:\n",
      "\t\tULTRAKILL: 0.79\n",
      "\t\tMario Galaxy: 0.812\n",
      "\n",
      "\tRecall:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "i:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tULTRAKILL: 0.833\n",
      "\t\tMario Galaxy: 0.771\n",
      "\n",
      "\tF1:\n",
      "\t\tULTRAKILL: 0.809\n",
      "\t\tMario Galaxy: 0.789\n",
      "\n",
      "\tAccuracy: 0.8\n",
      "\tMacro F1: 0.809\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 15\n",
      "Hidden layers: 1 \t Hidden layer size: 1000 \t Activation function: relu\n",
      "\n",
      "\tPrecision:\n",
      "\t\tULTRAKILL: 0.3\n",
      "\t\tMario Galaxy: 1.0\n",
      "\n",
      "\tRecall:\n",
      "\t\tULTRAKILL: 0.4\n",
      "\t\tMario Galaxy: 0.585\n",
      "\n",
      "\tF1:\n",
      "\t\tULTRAKILL: 0.343\n",
      "\t\tMario Galaxy: 0.726\n",
      "\n",
      "\tAccuracy: 0.622\n",
      "\tMacro F1: 0.343\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 16\n",
      "Hidden layers: 10 \t Hidden layer size: 1000 \t Activation function: relu\n",
      "\n",
      "\tPrecision:\n",
      "\t\tULTRAKILL: 0.85\n",
      "\t\tMario Galaxy: 0.918\n",
      "\n",
      "\tRecall:\n",
      "\t\tULTRAKILL: 0.928\n",
      "\t\tMario Galaxy: 0.843\n",
      "\n",
      "\tF1:\n",
      "\t\tULTRAKILL: 0.885\n",
      "\t\tMario Galaxy: 0.877\n",
      "\n",
      "\tAccuracy: 0.881\n",
      "\tMacro F1: 0.885\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model 17\n",
      "Hidden layers: 100 \t Hidden layer size: 1000 \t Activation function: relu\n",
      "\n",
      "\tPrecision:\n",
      "\t\tULTRAKILL: 0.87\n",
      "\t\tMario Galaxy: 0.812\n",
      "\n",
      "\tRecall:\n",
      "\t\tULTRAKILL: 0.851\n",
      "\t\tMario Galaxy: 0.844\n",
      "\n",
      "\tF1:\n",
      "\t\tULTRAKILL: 0.858\n",
      "\t\tMario Galaxy: 0.823\n",
      "\n",
      "\tAccuracy: 0.843\n",
      "\tMacro F1: 0.858\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_idx = 0\n",
    "macro_f1_scores = []\n",
    "\n",
    "for k in range(len(activation_function)):\n",
    "    for j in range(len(hidden_layer_size)):\n",
    "        for i in range(len(hidden_layers)):\n",
    "            print(f\"Model {model_idx}\")\n",
    "            print(f\"Hidden layers: {hidden_layers[i]} \\t Hidden layer size: {hidden_layer_size[j]} \\t Activation function: {activation_function[k]}\")\n",
    "            print()\n",
    "            print(f\"\\tPrecision:\") \n",
    "            print(f\"\\t\\tULTRAKILL: {np.round(np.average([metrics.precision_score(models_predicted[model_idx][z], test_labels) for z in range(len(models_predicted[model_idx]))]), 3)}\")\n",
    "            print(f\"\\t\\tMario Galaxy: {np.round(np.average([metrics.precision_score(models_predicted[model_idx][z], test_labels, pos_label=0) for z in range(len(models_predicted[model_idx]))]), 3)}\")\n",
    "            print()\n",
    "            print(f\"\\tRecall:\") \n",
    "            print(f\"\\t\\tULTRAKILL: {np.round(np.average([metrics.recall_score(models_predicted[model_idx][z], test_labels) for z in range(len(models_predicted[model_idx]))]), 3)}\")\n",
    "            print(f\"\\t\\tMario Galaxy: {np.round(np.average([metrics.recall_score(models_predicted[model_idx][z], test_labels, pos_label=0) for z in range(len(models_predicted[model_idx]))]), 3)}\")\n",
    "            print()\n",
    "            print(f\"\\tF1:\") \n",
    "            print(f\"\\t\\tULTRAKILL: {np.round(np.average([metrics.f1_score(models_predicted[model_idx][z], test_labels, ) for z in range(len(models_predicted[model_idx]))]), 3)}\")\n",
    "            print(f\"\\t\\tMario Galaxy: {np.round(np.average([metrics.f1_score(models_predicted[model_idx][z], test_labels, pos_label=0) for z in range(len(models_predicted[model_idx]))]), 3)}\")\n",
    "            print()\n",
    "            print(f\"\\tAccuracy: {np.round(np.average([metrics.accuracy_score(models_predicted[model_idx][z], test_labels) for z in range(len(models_predicted[model_idx]))]), 3)}\")\n",
    "            print(f\"\\tMacro F1: {np.round(np.average([metrics.f1_score(models_predicted[model_idx][z], test_labels, ) for z in range(len(models_predicted[model_idx]))]), 3)}\")\n",
    "            print('\\n\\n\\n\\n')\n",
    "\n",
    "            macro_f1_scores.append(np.round(np.average([metrics.f1_score(models_predicted[model_idx][z], test_labels, ) for z in range(len(models_predicted[model_idx]))]), 3))\n",
    "            model_idx+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHzCAYAAADcuTyRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABm4ElEQVR4nO3deXhMZ/8G8HsySWayR8guJLagJUQqYl9CigZtf6UosZfSIuWVVIlSglrSKk2rtVZL31pqV2InthCUiF2ULLbsss7z+8Pr1DRBJpJMcnJ/rmuuyzznOed8z2Rw5znPOUchhBAgIiIikgkDfRdAREREVJIYboiIiEhWGG6IiIhIVhhuiIiISFYYboiIiEhWGG6IiIhIVhhuiIiISFYM9V0Ala78/Hzk5ubquwwiogrJyMgISqVS32WQjhhuZEoIgYSEBCQnJ+u7FCKiCs3a2hoODg5QKBT6LoWKiOFGpp4GGzs7O5iamvIvJRGRjoQQyMzMRFJSEgDA0dFRzxVRUTHcyFB+fr4UbKpWrarvcoiIKiwTExMAQFJSEuzs7HiKqoLghGIZejrHxtTUVM+VEBFVfE//LeX8xYqD4UbGeCqKiOjV8d/SiofhhoiIiGSF4YaIiIhkheGGqIK4efMmFAoFoqOj9V2K5NKlS2jRogXUajWaNGlSaJ/27dtj3LhxL9yOQqHApk2bnru8KMe+f/9+KBSKUr/9QVGOh4j0i+GGqIgGDRoEhUKB2bNna7Vv2rSp0p6TDwkJgZmZGWJjYxEREVHs7cTHx6Nr164lWBkRVWYMN0Q6UKvVmDNnDh49eqTvUkpMTk5Osde9du0aWrdujZo1a77SbQccHBygUqmKvX5l8yo/M6LKgOGGSAe+vr5wcHBAaGjoc/tMmzatwCmasLAwuLq6Su8HDRqEXr16YdasWbC3t4e1tTWmT5+OvLw8TJw4ETY2NqhevTqWL19eYPuXLl1Cy5YtoVar8frrr+PAgQNay//66y907doV5ubmsLe3x4ABA3D//n1pefv27TFmzBiMGzcO1apVg5+fX6HHodFoMH36dFSvXh0qlQpNmjTBzp07peUKhQJRUVGYPn06FAoFpk2b9tzPRKPR4D//+Q9sbGzg4OBQoO+/T0udOHECTZs2hVqthpeXF86cOVNgm9u3b0e9evVgYmKCDh064ObNmwX6HD58GG3atIGJiQlcXFzwySefICMjQ1ru6uqKWbNmYciQIbCwsECNGjXwww8/PPc4CrN69Wp4eXnBwsICDg4O6Nevn3TTNyEE6tSpg3nz5mmtEx0dDYVCgatXrwIAkpOTMWzYMNja2sLS0hIdO3bE2bNnpf5Pv1M//vgj3NzcoFarAQC///47GjVqBBMTE1StWhW+vr5ax0dUWTHcEOlAqVRi1qxZWLRoEf7+++9X2tbevXtx9+5dHDx4EAsWLEBISAjeeustVKlSBcePH8fIkSPx4YcfFtjPxIkT8emnn+LMmTPw8fGBv78/Hjx4AODJf5IdO3ZE06ZNcerUKezcuROJiYno3bu31jZWrlwJY2NjHDlyBOHh4YXW9/XXX2P+/PmYN28ezp07Bz8/P/To0QNXrlwB8ORU0muvvYZPP/0U8fHxmDBhwnOPdeXKlTAzM8Px48cxd+5cTJ8+Hbt37y60b3p6Ot566y00bNgQUVFRmDZtWoFt3759G++88w78/f0RHR2NYcOGISgoSKvPtWvX8Oabb+Ldd9/FuXPnsG7dOhw+fBhjxozR6jd//nwpQH300UcYNWoUYmNjn3ss/5abm4sZM2bg7Nmz2LRpE27evIlBgwYBeBLahgwZUiCkLl++HG3btkWdOnUAAO+99x6SkpKwY8cOREVFwdPTE506dcLDhw+lda5evYr169djw4YNiI6ORnx8PPr27YshQ4YgJiYG+/fvxzvvvAMhRJFrJ5ItQbLz+PFjcfHiRfH48WN9lyIrAQEBomfPnkIIIVq0aCGGDBkihBBi48aN4tm/SiEhIcLDw0Nr3YULF4qaNWtqbatmzZoiPz9fanN3dxdt2rSR3ufl5QkzMzPx66+/CiGEuHHjhgAgZs+eLfXJzc0V1atXF3PmzBFCCDFjxgzRpUsXrX3fvn1bABCxsbFCCCHatWsnmjZt+tLjdXJyEjNnztRqe+ONN8RHH30kvffw8BAhISEv3E67du1E69atC2xn0qRJ0nsAYuPGjUIIIb7//ntRtWpVre/vd999JwCIM2fOCCGECA4OFg0bNtTa5qRJkwQA8ejRIyGEEEOHDhUjRozQ6nPo0CFhYGAgbbtmzZrigw8+kJZrNBphZ2cnvvvuuxcez9ixY5+7/OTJkwKASEtLE0IIcefOHaFUKsXx48eFEELk5OSIatWqiRUrVkg1WVpaiqysLK3t1K5dW3z//fdCiCffKSMjI5GUlCQtj4qKEgDEzZs3n1sLlQz+m1rxcOSGqBjmzJmDlStXIiYmptjbeO2112Bg8M9fQXt7ezRq1Eh6r1QqUbVqVekUx1M+Pj7Snw0NDeHl5SXVcfbsWezbtw/m5ubSq379+gCejGQ81axZsxfWlpqairt376JVq1Za7a1atSrWMTdu3FjrvaOjY4HjeiomJgaNGzeWTr0A2sf8tI+3t7dW27/7nD17FitWrND6LPz8/KDRaHDjxo1Ca1MoFHBwcHhubYWJioqCv78/atSoAQsLC7Rr1w4AEBcXBwBwcnJC9+7dsWzZMgDAli1bkJ2djffee0+qMz09HVWrVtWq9caNG1o/s5o1a8LW1lZ67+HhgU6dOqFRo0Z47733sHTpUlnNBSN6FXy2FFExtG3bFn5+fggODpZOQTxlYGBQ4NRAYbdtNzIy0nqvUCgKbdNoNEWuKz09Hf7+/pgzZ06BZc8+9M/MzKzI2ywJr3pcxZGeno4PP/wQn3zySYFlNWrUKJHaMjIy4OfnBz8/P6xZswa2traIi4uDn5+f1qTfYcOGYcCAAVi4cCGWL1+OPn36SLf0T09Ph6OjI/bv319g+9bW1tKf//0zUyqV2L17N44ePYo///wTixYtwuTJk3H8+HG4ubkVqX4iuWK4ISqm2bNno0mTJnB3d9dqt7W1RUJCAoQQ0iXiJXlvmmPHjqFt27YAgLy8PERFRUnzSDw9PbF+/Xq4urrC0LD4f70tLS3h5OSEI0eOSCMRAHDkyBE0b9781Q7gJRo0aIDVq1cjKytLGr05duxYgT6bN2/Wavt3H09PT1y8eFGa11IaLl26hAcPHmD27NlwcXEBAJw6dapAv27dusHMzAzfffcddu7ciYMHD2rVmZCQAENDQ61J50WhUCjQqlUrtGrVClOnTkXNmjWxceNGBAYGvtJxEVV0PC1FVEyNGjVC//798c0332i1t2/fHvfu3cPcuXNx7do1LF68GDt27Cix/S5evBgbN27EpUuXMHr0aDx69AhDhgwBAIwePRoPHz5E3759cfLkSVy7dg27du3C4MGDkZ+fr9N+Jk6ciDlz5mDdunWIjY1FUFAQoqOjMXbs2BI7lsL069cPCoUCw4cPx8WLF7F9+/YCVxuNHDkSV65cwcSJExEbG4tffvkFK1as0OozadIkHD16FGPGjEF0dDSuXLmCP/74o8CE4ldRo0YNGBsbY9GiRbh+/To2b96MGTNmFOinVCoxaNAgBAcHo27dulqn0Hx9feHj44NevXrhzz//xM2bN3H06FFMnjy50KD01PHjxzFr1iycOnUKcXFx2LBhA+7du4cGDRqU2PERVVQMN0SvYPr06QVOYTRo0ABLlizB4sWL4eHhgRMnTrzwSiJdzZ49G7Nnz4aHhwcOHz6MzZs3o1q1agAgjbbk5+ejS5cuaNSoEcaNGwdra2ut+T1F8cknnyAwMBCffvopGjVqhJ07d2Lz5s2oW7duiR1LYczNzbFlyxacP38eTZs2xeTJkwucZqtRowbWr1+PTZs2wcPDA+Hh4Zg1a5ZWn8aNG+PAgQO4fPky2rRpg6ZNm2Lq1KlwcnIqsVptbW2xYsUK/Pe//0XDhg0xe/bsAkHsqaFDhyInJweDBw/WalcoFNi+fTvatm2LwYMHo169enj//fdx69Yt2NvbP3fflpaWOHjwILp164Z69erh888/x/z583kzRCIACvHvyQFU4WVlZeHGjRta98MgIv06dOgQOnXqhNu3b78wtFD5w39TKx7OuSEiKkXZ2dm4d+8epk2bhvfee4/BhqgM8LQUEVEp+vXXX1GzZk0kJydj7ty5+i6HqFLgaSkZ4hAqEVHJ4b+pFQ9HboiIiEhWGG6IiIhIVhhuiIiISFYYboiIiEhWGG6IiIhIVhhuiIiISFZ4Ez+if5k2bRo2bdpUog+7LCrXoG1lur+bs7uX6f5K3DSrMtxXStntS08arWxUpvs7H3C+TPdHlQdHbqhcUCgUL3xNmzZN3yXS/xw8eBD+/v5wcnKCQqHApk2b9F1SufWyz0oIgalTp8LR0REmJibw9fXFlStX9FNsBVASn+fDhw/Rv39/WFpawtraGkOHDkV6enoZHgWVBYYbKhfi4+OlV1hYGCwtLbXann3wpBACeXl5eqy2csvIyICHhwcWL16s71LKvZd9VnPnzsU333yD8PBwHD9+HGZmZvDz80NWVlYZV1oxlMTn2b9/f1y4cAG7d+/G1q1bcfDgQYwYMaKsDoHKCMNNJaLJzHj+Kzur6H2zHhepry4cHBykl5WVFRQKhfT+0qVLsLCwwI4dO9CsWTOoVCocPnwYGo0GoaGhcHNzg4mJCTw8PPD7779L29y/fz8UCgUiIiLg5eUFU1NTtGzZErGxsVr7nj17Nuzt7WFhYYGhQ4fyP5aX6Nq1K7788ku8/fbb+i6l3HvRZyWEQFhYGD7//HP07NkTjRs3xqpVq3D37l2Ohj3Hq36eMTEx2LlzJ3788Ud4e3ujdevWWLRoEdauXYu7d++W8dFQaeKcm0rkUgvz5y4zb9MNNRb/M98jtr0dRFZmoX1NvdrBddl+6f2Vrq7If3S/QL+G50r2yR5BQUGYN28eatWqhSpVqiA0NBQ///wzwsPDUbduXRw8eBAffPABbG1t0a5dO2m9yZMnY/78+bC1tcXIkSMxZMgQHDlyBADw22+/Ydq0aVi8eDFat26N1atX45tvvkGtWrVKtHaif7tx4wYSEhLg6+srtVlZWcHb2xuRkZF4//339VhdxVOUzzMyMhLW1tbw8vKS+vj6+sLAwADHjx9nYJcRhhuqMKZPn47OnTsDePKk5VmzZmHPnj3w8fEBANSqVQuHDx/G999/rxVuZs6cKb0PCgpC9+7dkZWVBbVajbCwMAwdOhRDhw4FAHz55ZfYs2cPR2+o1CUkJABAgaeE29vbS8uo6IryeSYkJMDOzk5ruaGhIWxsbPiZywzDTSVS/9gLJs0plVpv3fcnPb+vgfbZzLo7br5CVUX37G9bV69eRWZmphR2nsrJyUHTpk212ho3biz92dHREQCQlJSEGjVqICYmBiNHjtTq7+Pjg3379pV0+UREVEYYbioRA1Mzvfd9FWZm/+zn6dUN27Ztg7Ozs1Y/lUql9d7IyEj6s0KhAABoNJrSKpOoSBwcHAAAiYmJUuh++r5JkyZ6qqriKsrn6eDggKQk7V/c8vLy8PDhQ2l9kgdOKKYKqWHDhlCpVIiLi0OdOnW0Xi4uLkXeToMGDXD8+HGttmPHjpV0uUQFuLm5wcHBAREREVJbamoqjh8/Lp1qpaIryufp4+OD5ORkREVFSX327t0LjUYDb2/vMq+ZSg9HbqhCsrCwwIQJEzB+/HhoNBq0bt0aKSkpOHLkCCwtLREQEFCk7YwdOxaDBg2Cl5cXWrVqhTVr1uDChQucUPwC6enpuHr1qvT+xo0biI6Oho2NDWrUqKHHysqfl31W48aNw5dffom6devCzc0NU6ZMgZOTE3r16qW/osuxV/08GzRogDfffBPDhw9HeHg4cnNzMWbMGLz//vtwcnLS01FRqRAkO48fPxYXL14Ujx8/1ncpxbJ8+XJhZWUlvd+3b58AIB49eqTVT6PRiLCwMOHu7i6MjIyEra2t8PPzEwcOHHjuemfOnBEAxI0bN6S2mTNnimrVqglzc3MREBAg/vOf/wgPD4/SO8AK7unn+u9XQECAvksrd172WWk0GjFlyhRhb28vVCqV6NSpk4iNjdVv0eVYSXyeDx48EH379hXm5ubC0tJSDB48WKSlpb1wvxX939TKSCGEKNnrdUnvsrKycOPGDbi5uUGtVuu7HCKiCo3/plY8nHNDREREssJwQ0RERLLCcENERESywnAjY5xORUT06vhvacXDcCNDT29al5lZ+LOhiIio6J7+W/rsDUGpfON9bmRIqVTC2tpauhOnqampdGdeIiIqGiEEMjMzkZSUBGtrayj/9ZgaKr94KbhMCSGQkJCA5ORkfZdCRFShWVtbw8HBgb8kViAMNzKXn5+P3NxcfZdBRFQhGRkZccSmAmK4ISIiIlnhhGIiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhW9hpuDBw/C398fTk5OUCgU2LRp00vX2b9/Pzw9PaFSqVCnTh2sWLGi1OskIiKiikOv4SYjIwMeHh5YvHhxkfrfuHED3bt3R4cOHRAdHY1x48Zh2LBh2LVrVylXSkRERBWFQggh9F0EACgUCmzcuBG9evV6bp9JkyZh27Zt+Ouvv6S2999/H8nJydi5c2cZVElERETlnaG+C9BFZGQkfH19tdr8/Pwwbty4566TnZ2N7Oxs6b1Go8HDhw9RtWpVKBSK0iqViIiISpAQAmlpaXBycoKBwYtPPFWocJOQkAB7e3utNnt7e6SmpuLx48cwMTEpsE5oaCi++OKLsiqRiIiIStHt27dRvXr1F/apUOGmOIKDgxEYGCi9T0lJQY0aNXD79m1YWlrqsTIiIiIqqtTUVLi4uMDCwuKlfStUuHFwcEBiYqJWW2JiIiwtLQsdtQEAlUoFlUpVoN3S0pLhhoiIqIIpypSSCnWfGx8fH0RERGi17d69Gz4+PnqqiIiIiMobvYab9PR0REdHIzo6GsCTS72jo6MRFxcH4MkppYEDB0r9R44cievXr+M///kPLl26hCVLluC3337D+PHj9VE+ERERlUN6DTenTp1C06ZN0bRpUwBAYGAgmjZtiqlTpwIA4uPjpaADAG5ubti2bRt2794NDw8PzJ8/Hz/++CP8/Pz0Uj8RERGVP+XmPjdlJTU1FVZWVkhJSeGcGyIiogpCl/+/K9ScGyIiIqKXYbghIiIiWWG4ISIiIllhuCEiIiJZYbghIiIiWWG4ISIiIllhuCEiIiJZYbghIiIiWWG4ISIiIllhuCEiIiJZYbghIiIiWWG4ISIiIllhuCEiIiJZYbghIiIiWWG4ISIiIllhuCEiIiJZYbghIiIiWWG4ISIiIllhuCEiIiJZYbghIiIiWWG4ISIiIllhuCEiIiJZYbghIiIiWWG4ISIiIllhuCEiojK3ePFiuLq6Qq1Ww9vbGydOnHhh/7CwMLi7u8PExAQuLi4YP348srKypOWurq5QKBQFXqNHj5b6/PDDD2jfvj0sLS2hUCiQnJxc7mr88MMPUbt2bZiYmMDW1hY9e/bEpUuXXrhfKojhhoiIytS6desQGBiIkJAQnD59Gh4eHvDz80NSUlKh/X/55RcEBQUhJCQEFy9exHfTPsOvy5YieNJ/pD4nT55EfHy89Nq9ezcA4L333pP6ZGZm4s0338Rnn31WqjXGxMTgp59+wrp167T2VZQamzVrhuXLlyMmJga7du2CEAJdunRBfn7+S2umZ4hKJiUlRQAQKSkp+i6FiCqgb7/9VtSsWVOoVCrRvHlzcfz48Rf2X7hwoahXr55Qq9WievXqYty4ceLx48daff7++2/Rv39/YWNjI9RqtXj99dfFyZMnpeVpaWli9OjRwtnZWajVatGgQQPx3XfflcrxlYXmzZuL0aNHS+/z8/OFk5OTCA0NLbT/6NGjRYfWrcS9ZXPF1XcaiQuNIAKqQbR4zf25+xg7dqyoXbu20Gg0BZbt27dPABCPHj0q0Ro7duyo1RYYGChatWpVrBqfOnv2rAAgrl69+tw+lYUu/39z5IaIqIhK47f5R48eoVWrVjAyMsKOHTtw8eJFzJ8/H1WqVJH6BAYGYufOnfj5558RExODcePGYcyYMdi8eXOpH3NJy8nJQVRUFHx9faU2AwMD+Pr6IjIyskD/zLORqBdzAKeOHsGemf9B9pXz+FtjhCMaC7zZscNz9/Hzzz9jyJAhUCgUpV4jALRs2RJRUVHSqavr169j+/bt6NatW7FrzMjIwPLly+Hm5gYXFxedj6NSK4OwVa5w5Iao/NLHqMj69etF586dhY2NjQAgzpw589z9lcZv85MmTRKtW7d+4XG+9tprYvr06Vptnp6eYvLkyS9crzy6c+eOACCOHj2q1T5x4kTRvHlzocnLE3npqVJ7+sn94kIjiGBHCCMDhTBUGggAYuTIkc/dx7p164RSqRR37twpdPnLRm5eVuPzfP3118LIyEgYGhq+Uo2LFy8WZmZmAoBwd3fnqM3/cOSGiCocfY2KZGRkoHXr1pgzZ84L6yut3+Y3b94MLy8vvPfee7Czs0PTpk2xdOnSAtvZvHkz7ty5AyEE9u3bh8uXL6NLly4vrLkiyXt0H7kJt3HFrybuLZostZt6tsGVbiOxLK8alnz/A06ficaGDRuwbds2zJgxo9Bt/fTTT+jatSucnJzKqnzs378fs2bNwpIlS3D69OlXqrF///44c+YMDhw4gHr16qF3795aE5OpCEo/a5UvHLkhKp/0NSry1I0bN144clNav82rVCqhUqlEcHCwOH36tPj++++FWq0WK1askPpkZWWJgQMHCgDC0NBQGBsbi5UrVxbpuMqb7OxsoVQqxcaNG0Xu/QRxf9VCca23p+hhDdHBAuJCI4grb9XTmofSunVrMWHCBK3trF69WpiYmIj8/Hyt9ps3bwoDAwOxadOm59bwspGbZ2t81sCBA0WPHj0KXaeka3y2FlNTU/HLL7+8tK/cceSGiCoUfY6KlKai/Dav0Wjg6emJWbNmoWnTphgxYgSGDx+O8PBwqc+iRYtw7NgxbN68GVFRUZg/fz5Gjx6NPXv2lNmxlBRjY2M0a9YMm2dNxmVfZyR+NR6ZF0/jeAbQvFEDVF+wHrXWn9Oah5KZmQkDA+3/rpRKJQBACKHVvnz5ctjZ2aF79+6vXGNERITUptFoEBERAR8fn0LXKa0ahRAQQiA7O1vXw6jcSjlolTscuSEqf/Q5KvLUy0ZuSuu3+Ro1aoihQ4dq9VmyZIlwcnISQgiRmZkpjIyMxNatW7X6DB06VPj5+RW63/JGk58v0k8dFJq8PCGEEGvXrhUqQ6WYWR1iV7dGIqBjK2FtZSUSEhKEEEIMGDBABAUFSeuHhIQICwsL8euvv4rr16+LP//8U9SuXVv07t1baz/5+fmiRo0aYtKkSYXWER8fL86cOSOWLl0qAIiDBw+KM2fOiAcPHhTou3btWqFSqcSKFSvExYsXxYgRI4S1tXWp1njt2jUxa9YscerUKXHr1i1x5MgR4e/vL2xsbERiYmJRPmpZ0+X/b0N9BisiouJ6dlTE29sbV69exdixYzFjxgxMmTIFwJPftr28vDBr1iwAQNOmTfHXX38hPDwcAQEBOu3v2d/me/XqJW0/IiICY8aMKXSdovw236pVK8TGxmr1uXz5MmrWrAkAyM3NRW5ubqHb0Wg0Oh1DWcu+eRkpW1cjZevPyL17EzXCd8G8ZRf06dMHCdeuYP7i75C4JxZNmjTBzl27YG9vDwCIi4vTOt7PP/8cCoUCn3/+Oe7cuQNbW1v4+/tj5syZWvvbs2cP4uLiMGTIkELrCQ8PxxdffCG9b9u2LYAnIymDBg3S6tunTx/cu3cPU6dORUJCwpMad+4s1RrVajUOHTqEsLAwPHr0CPb29mjbti2OHj0KOzu7l33c9KzSz1rlC0duiMoffY2KPOtlIzdClM5v8ydOnBCGhoZi5syZ4sqVK2LNmjXC1NRU/Pzzz1Kfdu3aiddee03s27dPXL9+XSxfvlyo1WqxZMmS59aqL7mP7osHvy4W1/t5iwuNIL1iWliIh78v1Xd5VIFx5IaIKhR9jYroqjR+m3/jjTewceNGBAcHY/r06XBzc0NYWBj69+8v9Vm7di2Cg4PRv39/PHz4EDVr1sTMmTMxcuTIYh1Hacm5fQ1XezYA8nKfNCiVMG/pByv/gbBo3wMGahP9FkiVR+lnrfKFIzdE5ZO+RkUePHggzpw5I7Zt2yYAiLVr14ozZ86I+Pj4sjv4Ckij0YiM6KPi0eZVWm1XejYQ195rIu6vWihy7yfoscLKqzTuF/VUaGioACDGjh1bCpW/mC7/fzPcEFG5sWjRIlGjRg1hbGwsmjdvLo4dOyYta9eunQgICJDe5+bmimnTponatWsLtVotXFxcxEcffVTg8t4tW7aI119/XahUKlG/fn3xww8/aC1fvny5AFDgFRISUopHWnFl374uksKniyvd60inm/IzM6TleSmP9FccibVr1wpjY2OxbNkyceHCBTF8+HBhbW393AnJa9asESqVSqxZs0bcuHFD7Nq1Szg6Oorx48cX6HvixAnh6uoqGjduXO7DjUKIf12jJnOpqamwsrJCSkoKLC0t9V0OEVG5l5+ajNQ//4uUrauRefqQ1K4wMYNlp3dgN34OjGwd9VghPeXt7Y033ngD3377LYAnp3ddXFzw8ccfIygoqED/MWPGICYmRuuy908//RTHjx/H4cOHpbb09HR4enpiyZIl+PLLL9GkSROEhYWV+vE8S5f/v3mfGyIieqGHa75G/PQRT4KNQgGzFr5w+nIl3PclwHnWKgabcqI0n4k1evRodO/eXWvb5RknFBMREYAnE7GzYk4jZfMqmLV6ExZtugIArN76AKm7f4fVWwNg1b0/jOyd9VxpKZpmpe8KXm5aSqHN9+/fR35+vjTB/Sl7e3tcunSp0HX69euH+/fvo3Xr1hBCIC8vDyNHjtR6jMnatWtx+vRpnDx5suSOoZQx3BARVXK5CbeRsm0NkresQs71mCdt8bekcGPsUhu1N5x/5f24Bm175W2UtptqfVdQtl52v6jbt29j7Nix2L17N9TqivPhMNwQEb1Eo5WN9F3CS50P0C18CCGQsnklkresRubJfcD/pl8qVGpYdOgJ6x6DSqFKKk3VqlWDUqlEYmKiVntiYiIcHBwKXWfKlCkYMGAAhg0bBgBo1KgRMjIyMGLECEyePBlRUVFISkqCp6entE5+fj4OHjyIb7/9FtnZ2dItGMoThhsiokpCCCE9s0mhUODh2sXIunAKAGDq1Q5Wbw2AZef/g9KiApyaoQJK435RnTp1wvnz2sF58ODBqF+/PiZNmlQugw3AcENE+lYR5ji41dB3Ba8k6/I5JG9ehbSIDai17jSUltYAgKoDApHz93VYde8PY2dXvdZIJSMwMBABAQHw8vJC8+bNERYWhoyMDAwePBgAMHDgQDg7OyM0NBQA4O/vjwULFqBp06bSaakpU6bA398fSqUSFhYWeP3117X2YWZmhqpVqxZoL08YboiIZCj3XjxSt/+C5K2rkR17VmpP3bMeVd4ZCgCw6tZXX+VRKSmtZ2JVNLzPDRHpVwUYuWlUAUZuns65yb52EQlfBSLj2G7gfw/WVBgZw7ztW7DuMRDmrbtCYWSslxorxoTifvou4eWec7WU3PE+N0RUwOLFi+Hq6gq1Wg1vb2/pvhbPExYWBnd3d5iYmMDFxQXjx49HVlaWTtu8du0a3n77bdja2sLS0hK9e/cuMNmRXo1CCFhk5knvDSyspWBj4uEDh8+/Q7298XBZuB4WHXrqLdgQlSWGG6JKYN26dQgMDERISAhOnz4NDw8P+Pn5ISkpqdD+v/zyC4KCghASEoKYmBj89NNPWLdunda9L162zYyMDHTp0gUKhQJ79+7FkSNHkJOTA39/f2j+N6JAxef8IAvvH0rAt0svY+y221K7kZ0TnL5YhtpbLsNt9VHY9B4JpZWNHislKns8LUVUCZTGLdlfts0///wTXbt2xaNHj6S/aykpKahSpQr+/PPPf+50ytNSRWaRmYdWl1LQ9mIyaic+ltrT1Ep47XsEpZmFHqt7OZ6WKiE8LcXTUkSVXWnckr0o28zOzoZCoYBKpZL6qNVqGBgYaD2zhoqm95FEhH9/CYP3xaN24mPkGQCnallg4VsuGPWhe7kPNkRliVdLEclcadySvSjbbNGiBczMzDBp0iTMmjULQggEBQUhPz8f8fHxpXCkMiIE3O9k4q6NCmmmT/6ZTrIyhqEGuGZvgoMNrXG0vhVSTflPOFFh+DeDiAp42S3Zi8LW1hb//e9/MWrUKHzzzTcwMDBA37594enpWeCmYfSE/aNstL2YjNYxyXBIycXK9g7Y3qwaACCyniWuOJrgTtWKcwt8Kh1yvGN2SWO4IZK50rgle1G32aVLF1y7dg3379+HoaEhrK2t4eDggFq1apXwUVZcZo/z4HM5FW0vPIJ7/D/zaB4bGUCd+8/E62xjJe5ULZ93gyUqb/jrE5HMPXtL9qee3pLdx8en0HVedkt2XbdZrVo1WFtbY+/evUhKSkKPHj1K4tAqPKNcDb798TKG77kL9/jH0CiAaFdzfNOtOj4cVR8bWtjpu0SiCokjN0SVQEnfkr0o2wSA5cuXo0GDBrC1tUVkZCTGjh2L8ePHw93dvew/BH0TArUTHqPh3xnY8oYtACDXyADnaprDITkHhxpa43B9KySbG+m5UKKKj+GGqBIojVuyv2ybABAbG4vg4GA8fPgQrq6umDx5MsaPH192B14OVEvNQZuLyWh7MRlOj3IAAKdqWyLe5slVZIu7VkeOEQfRiUoS73NDRPolw/vcmGTno8XlFLS5mIzX/s6U2rMMFThZ1xLrW9hJ4aak6HsCZ1HwPjclo7zcd+lFSuP7qMv/3xy5ISIqYR430zHyz7sAAA2ACzXMcLChNU7UtUSWMScFE5U2hhsiouISAm5JWWh7MRkJ1sbY1bQqACCqtgViHU0QVdsShxtY4YEln+dEVJYYboiIdGSTlovWMU/m0bg8yAYA3K1ijF1NbACFArmGBpjar7aeqySqvBhuiIiKqEVsCjqde4jX4zKk+2jkKBU4VdsChxpaQwGgUk1iJCqnGG6IiJ7j39dbeF5PQ+O4DABAjLMpDja0xrF6VshUcx4NUXmi9+sPFy9eDFdXV6jVanh7e0sP6nuesLAwuLu7w8TEBC4uLhg/fjyysrLKqFoiqgyyUvKReD4LV3akIzvln7sE7/awwW8t7TBmWD1Me78W9ja2YbAhKof0OnKzbt06BAYGIjw8HN7e3ggLC4Ofnx9iY2NhZ1fwzpy//PILgoKCsGzZMrRs2RKXL1/GoEGDoFAosGDBAj0cAVH5VjEuvdV3BU/kZWmQcjsXKXG5yHom0KTczgWaPvnzFSdTXHEy1VOFRFRUeg03CxYswPDhw6U7moaHh2Pbtm1YtmwZgoKCCvQ/evQoWrVqhX79ntyHwNXVFX379sXx48fLtG4iko+8LA3uRmUhPSnvnwkzCsDCwRBWNYxg7sCz90QVjd5OS+Xk5CAqKgq+vr7/FGNgAF9fX0RGRha6TsuWLREVFSWdurp+/Tq2b9+Obt26PXc/2dnZSE1N1XoRUeUlhEBO5j8jM0pjBR4n5wMCMLFRwsFDjXrdzOHiYwpLZyMYKBV6rJaIikNvv5Lcv38f+fn5WrdqBwB7e3tcunSp0HX69euH+/fvo3Xr1hBCIC8vDyNHjsRnn3323P2Ehobiiy++KNHaiajiyU7LR0pcLlJu50JogLpdzaFQKKAwUMCpmRrGZgZQWXD+DJEc6H1CsS7279+PWbNmYcmSJTh9+jQ2bNiAbdu2YcaMGc9dJzg4GCkpKdLr9u3bZVgxEelTXrYGD6/l4Pq+dFzbnYH7sTnIzRTQ5AnkpP0zemPhYMRgQyQjehu5qVatGpRKJRITE7XaExMT4eDgUOg6U6ZMwYABAzBs2DAAQKNGjZCRkYERI0Zg8uTJWg/+e0qlUkGlKtlnuBBR+ffoeg7iz2ZpzaMxtzOEVU0jWDga8nQTkYzpbeTG2NgYzZo1Q0REhNSm0WgQEREBHx+fQtfJzMwsEGCUyie/bVWy538S0TOEEMh8kIfstHypTWVtAAhAbW0A+8Yq1OtqjhqtTGFVnfNoiOROr5cBBAYGIiAgAF5eXmjevDnCwsKQkZEhXT01cOBAODs7IzQ0FADg7++PBQsWoGnTpvD29sbVq1cxZcoU+Pv7SyGHiCqPnAzNk3k0cbnIydDA2tUITp4mAACTKkrU7mzG001ElZBew02fPn1w7949TJ06FQkJCWjSpAl27twpTTKOi4vTGqn5/PPPoVAo8Pnnn+POnTuwtbWFv78/Zs6cqa9DIKIylp8jkHonF8lxuXj84J+RGoUSWiMyCoWCwYaoktL7DRzGjBmDMWPGFLps//79Wu8NDQ0REhKCkJCQMqiMiMqjGwcytCYDm9kpYVXDCJZORjAw5OkmIioH4YaIqDBCCGQ90iD1Ti7sXlNBYfAkuFg6GSItPg9WNYxg5WIEI5MKddEnEZUBhhsiKldyM588BiE5LlcaoTGtqoSFkxEAwLaBCrYNVVAoOEpDRIVjuCEivdPkCaT+nYvk27nIvPfMPBoDwMLJEIbPjM48HcEhInoehhsi0rucTA3uns6S3ptWU8K6hhEsnI2gNGKYISLdMNwQUZlyS72OTnf2Qiny8X3DDwEAakslLJ0NobJ6MjnY2JTzaIio+BhuiKjU2WQ9QPu7B9Dxzl7USrsJAMg2MMaqugMA9ZM+1b1N9VcgEckKww0RlRqvpFPoefMPNLl/Fko8mRyca2CI43bNEeHcCTlKYz1XSERyxHBDRCVGITRQCAGNwZOb57mm3USz+2cAABerNECEc0cccmyDdCNzfZZJRDLHcENEr8wlLQ6d7uxFh7v7sdw9APudOwAA9jm3h7EmB3udOiDBzFHPVRJRZcFwQ0TFYpWd/L95NPtQN/Wq1N4q4agUbh6oq+GXuv30VSIRVVIMN0SkE6UmD5+fngmve1FQiifzaPIUSpy09cJe5w44YddczxUSUWXHcENEL6QQGrik30acRU0AQL6BIUzzHkMpNIi1qou9zh1xwLEtUlVWeq6UiOgJhhsiKpRTxh10vLMPHe7sg23WfXzQcZUUYH6sPwSPDU3wt7mLnqskIiqI4YaIJOY5aWgbfwid7kSgQXKs1J6pNEGttBuIVjUBAFyxrqenComIXo7hhogAAF5JJzElaiaMRB4AIB8GOG3bFHudO+KYvTeylWo9V0hEVDQMN0SVkRBwT46FUuTjos1rAIDL/xuNuWZZC3udOmC/Uzs8Utvos0oiomJhuCGqROwyE9Hx7j50+nsvnDPv4kKVhpjoMxcAkGpshWHtf8A9Ezs9V0lE9GoYbohkzjQ3A20SDqPjnX1o9PAvqT1LqUKCqT2UmjzkGzz5p4DBhojkgOGGSOY+PbsAPknHAQAaKHC2amNEOHfEUYeWyDI00XN1REQlj+GGSCaEEMiKOY2ULatRdWAgjBxrAAAOOLWFU2Y8Ipw7Yp9TezwwqabnSomIShfDDVEFl5vwN1K2r0HKllXIvnYRAKC0sYPt8M8AAIcc2+CgY1tAodBnmUREZYbhhqgC0mRnIXXXb0jZsgoZJ/YCQgAAFMYqWHTsBdOmraW+QmGgrzKJiPSC4YaoAhJ5uYj/chREViYAwLRZW1j5D4Rl5/+D0oKPQSCiyo2/0lG5t3jxYri6ukKtVsPb2xsnTpx4Yf/k5GSMHj0ajo6OUKlUqFevHrZv3y4td3V1hUKhKPAaPXp0gW0JIdC1a1coFAps2rSppA+tSLIun0figv/g1qiuUpvSzAI2/cfCdvR01Nl+Ha7LD6DKO0MZbIiIwJEbKufWrVuHwMBAhIeHw9vbG2FhYfDz80NsbCzs7ApetpyTk4POnTvDzs4Ov//+O5ydnXHr1i1YW1tLfU6ePIn8/Hzp/V9//YXOnTvjvffeK7C9sLAwKPQwVyXvfgJStv+ClK2rkXUpWmrPij0LtbsHAMB+7Kwyr4uIqCJguKFybcGCBRg+fDgGDx4MAAgPD8e2bduwbNkyBAUFFei/bNkyPHz4EEePHoWRkRGAJyM1z7K1tdV6P3v2bNSuXRvt2rXTao+Ojsb8+fNx6tQpODo6luBRPV/m6cO4/+MspB/dBWg0TxoNjWDR7i1YvTUAxm71y6QOIqKKjKelqNzKyclBVFQUfH19pTYDAwP4+voiMjKy0HU2b94MHx8fjB49Gvb29nj99dcxa9YsrZGaf+/j559/xpAhQ7RGaDIzM9GvXz8sXrwYDg4OJXtgzxAaDTSPM6X3+amPkH54B6DRwKRxCzhMXoJ6e+PhsnADLDu9DQNjVanVQkQkFxy5oXLr/v37yM/Ph729vVa7vb09Ll26VOg6169fx969e9G/f39s374dV69exUcffYTc3FyEhIQU6L9p0yYkJydj0KBBWu3jx49Hy5Yt0bNnzxI7nmdl37iElC2rkbLtZ1h1/wB2n8wEAJi3ehO2Y2bA0q8PVDXrlsq+iYjkjuGGZEWj0cDOzg4//PADlEolmjVrhjt37uCrr74qNNz89NNP6Nq1K5ycnKS2zZs3Y+/evThz5kyJ1pb36D5Sd65F8pZVyPrrpNSedmCLFG4URkawHfF5ie6XiKiyYbihcqtatWpQKpVITEzUak9MTHzuqSJHR0cYGRlBqVRKbQ0aNEBCQgJycnJgbGwstd+6dQt79uzBhg0btLaxd+9eXLt2TWsSMgC8++67aNOmDfbv36/zsdz5fBBStq8B8vKeNCiVMG/1Jqz8B8KifQ+dt0dERM/HOTdUbhkbG6NZs2aIiIiQ2jQaDSIiIuDj41PoOq1atcLVq1eheToZF8Dly5fh6OioFWwAYPny5bCzs0P37t212oOCgnDu3DlER0dLLwBYuHAhli9f/tK6hRDIPHcc4n831gMAA5UJkJcHdQNP2P8nDPX23EWNb7fCyq83DFTql26TiIiKjiM3VK4FBgYiICAAXl5eaN68OcLCwpCRkSFdPTVw4EA4OzsjNDQUADBq1Ch8++23GDt2LD7++GNcuXIFs2bNwieffKK1XY1Gg+XLlyMgIACGhtp/DRwcHAodGapRowbc3NyeW2vO7WtI2fozkreuRu7ta3BddQSmTVoCAKoOngibfh9DVbvhK30eRET0cgw3VK716dMH9+7dw9SpU5GQkIAmTZpg586d0iTjuLg4GBj8MwDp4uKCXbt2Yfz48WjcuDGcnZ0xduxYTJo0SWu7e/bsQVxcHIYMGfJK9eWnPkLqrt+QvHU1Hp85IrUbmJoj59YVKdwYV6/1SvshIqKiY7ihcm/MmDEYM2ZMocsKm//i4+ODY8eOvXCbXbp00Tpt9DKF9c26egE3+nhC5OY8aTAwgFmLzrD2HwCLDr1gYGpW5O0TEVHJYbghKgIhBLL+OonchNuw7PwuAEBVqwEMqznAwMIaVm8NgFX3/jCyLZub/RER0fMx3BC9QM7dW0jZ+jNStqxCzq3LUFaxhUX7HlAYGUFhYAC3X0/B0Mb25RsiIqIyw3BD9C/56alI3f07UrauRubJ/VK7Qm0CM5/OyE9LlgINgw0RUfnDcEP0L/d/+BIPVnz15I1CAdM3OjyZR+P7LpRmFvotjoiIXorhhiotIQSyLkUjZetqWHToBTOvtgAAq7c+QNqBLbD2H/hkHo1jDT1XSkREumC4oUonN/EOUrb/gpQtq5B99S8AQP6je1K4UddrjNqbLmo9SJOIiCoOhhsqlxqtbFSi21NoBFpdSkHbi4/Q6FaGdGvuHKUCUbUtsM/4EM7quM/zAedLtEYiIioZDDckX0IA/xt9EQrgneP34PwwGwAQ42yKgw2tcayeFTLVyhdthYiIKhiGG5Idl/tZaHMxGZ7X0/BZ/9rIMTIAFApsbVYVVTLycKiBNZKsjV++ISIiqpAYbkgWrDLy0OpSMtpeTIZbUpbU7nUtFUfrWwMA9ja20VN1RERUlhhuqEJzuZeFfocS4HEzHcr/PSEhz0CBM27mOPiaNU678dJtIqLK5pXCTVZWFtRqdUnVQvRSCiFgmq1Bxv/myeQYKeB5Ix0AcMXRBAcbWCPS3QpppsztRESVlc7/A2g0GsycORPh4eFITEzE5cuXUatWLUyZMgWurq4YOnRoadRJlZzjw2y0iUlGm4vJuGmnxvyeNQEAidYqLPV1wgUXM8TbqPRcJRERlQc6h5svv/wSK1euxNy5czF8+HCp/fXXX0dYWBjDDZUY88d5aHkpBW1jklE3/rHUrs7VwChPg1zDJxd07/HgXBoiIvqHzuFm1apV+OGHH9CpUyeMHDlSavfw8MClS5dKtDiqvPoeSsBbpx7AUPNkIk2+Ajjrao5DDa1xsralFGyIiIj+Tedwc+fOHdSpU6dAu0ajQW5ubokURZWLEAKPzx2DqvZrUJpbAgAemRnBUCNww06Ngw2tcaS+NVLMOI+GiIheTuf/LRo2bIhDhw6hZs2aWu2///47mjZtWmKFkfzl/H0DKVtXI2XrauTEXYVjyFJUeXcYAOBQQ2tcqGGG29U4YZ2IiHSjc7iZOnUqAgICcOfOHWg0GmzYsAGxsbFYtWoVtm7dWho1kozkpyYj9c//InnLKjw+c1hqV5iYIT/5vvQ+Q62UrogiIiLShc7hpmfPntiyZQumT58OMzMzTJ06FZ6entiyZQs6d+5cGjWSTOSnp+KyrzNEVuaTBoUCZt6dYPXWAFj6vgMDU3P9FkhERLKgU7jJy8vDrFmzMGTIEOzevbu0aiIZEEIg62IUHp87Bpu+YwAASnNLmHq2QV7i37DyHwirbv1g5FBdz5USEZHc6BRuDA0NMXfuXAwcOLC06qEKLjfhNlK2/ozkrauRcz0GUChg0aGXFGKqz/8vDEzNofjfAy2JiIhKms6npTp16oQDBw7A1dW1FMqhiig/Iw1pe9YjectqZJ7c9+Rp3AAUKjUsOvSCyPnnWU9KMz4OgYiISpfO4aZr164ICgrC+fPn0axZM5iZmWkt79GjR4kVRxVD2u7fcXfqEOm9qVc7WPkPhKXvu1BaWOmxMiIiqox0DjcfffQRAGDBggUFlikUCuTn5796VVRuZV0+h+TNq6Cu+zqsew4CAFj4vgv1L9/AwvddWHX/AMbOrnqtkYiIKrdiPVuKKpfce/FI3f4LkresQvblcwAAdQNPKdwozS1R67czeqyQiIjoH7zlKz1X6p//xaMNPyHj2G7gf6FWYWQM87ZvwbrHQAghODGYiIjKnWKFmwMHDmDevHmIiYkB8OSuxRMnTkSbNm1KtDgqW0KjgcLgn2c2pexYi4yjuwAAJh4+Ty7f9usNpRUfVElEROWXzk8f/Pnnn+Hr6wtTU1N88skn+OSTT2BiYoJOnTrhl19+KY0aqZRlX49B4tfBuPKmK7JvXZHaq/T5CNVGhqDO1itwW30UNr1HMtgQEVG5p/PIzcyZMzF37lyMHz9eavvkk0+wYMECzJgxA/369SvRAql05D28h5QdvyJlyypkXYyS2lO3/wLbUSEAAPMWnWDeopO+SiQiIioWnUdurl+/Dn9//wLtPXr0wI0bN3QuYPHixXB1dYVarYa3tzdOnDjxwv7JyckYPXo0HB0doVKpUK9ePWzfvl3n/VZWuQl/I+7jHrjs64TEOWOfBBtDQ5i374Hq8/6LqkMm6btEIiKiV6LzyI2LiwsiIiJQp04drfY9e/bAxcVFp22tW7cOgYGBCA8Ph7e3N8LCwuDn54fY2FjY2dkV6J+Tk4POnTvDzs4Ov//+O5ydnXHr1i1YW1vrehiVhhACeffiYWTnBABQWldF5umDQF4e1K95weqtAbDq2heGNrZ6rpSIiKhk6BxuPv30U3zyySeIjo5Gy5YtAQBHjhzBihUr8PXXX+u0rQULFmD48OEYPHgwACA8PBzbtm3DsmXLEBQUVKD/smXL8PDhQxw9ehRGRkYAwDslP0dO3FUkb1mNlK2roTA0Qu3Nl6BQKGCgNoHTF8ugqtUAqloN9F0mERFRidM53IwaNQoODg6YP38+fvvtNwBAgwYNsG7dOvTs2bPI28nJyUFUVBSCg4OlNgMDA/j6+iIyMrLQdTZv3gwfHx+MHj0af/zxB2xtbdGvXz9MmjQJSqWy0HWys7ORnZ0tvU9NTS1yjRVNfspDpOxch5Stq/H47D+foYGpOXLv3pJurmfp+46eKiQiIip9xboU/O2338bbb7/9Sju+f/8+8vPzYW9vr9Vub2+PS5cuFbrO9evXsXfvXvTv3x/bt2/H1atX8dFHHyE3NxchISGFrhMaGoovvvjilWqtCB7+sggJ8z4F8nKfNBgYwMynC6z9B8KiQ08YmJjqt0AiIqIyonO4OXnyJDQaDby9vbXajx8/DqVSCS8vrxIr7t80Gg3s7Ozwww8/QKlUolmzZrhz5w6++uqr54ab4OBgBAYGSu9TU1N1nhtU3ggh8Pj8CRja2MG4uhsAwLhWAyAvFyp3D1i/NQCW3frByNZRz5USERGVPZ2vlho9ejRu375doP3OnTsYPXp0kbdTrVo1KJVKJCYmarUnJibCwcGh0HUcHR1Rr149rVNQDRo0QEJCAnJycgpdR6VSwdLSUutVUeXcuYl7P3yJaz3q4+YHLfDw10XSMrM3OqDW7+dQ+7/RqBrwaZGDjS5Xq61YsQIKhULrpVarn9t/5MiRUCgUCAsL02o/ffo0OnfuDGtra1StWhUjRoxAenp6keolIiJ6GZ3DzcWLF+Hp6VmgvWnTprh48WKRt2NsbIxmzZohIiJCatNoNIiIiICPj0+h67Rq1QpXr17Ver7V5cuX4ejoCGNjYx2OouLIT0vBow0/4ebgdrja1Q33vp2CnFuXoVCbSI9EAACFUgl1vUY6bfvp1WohISE4ffo0PDw84Ofnh6SkpOeuY2lpifj4eOl169atQvtt3LgRx44dg5OTk1b73bt34evrizp16uD48ePYuXMnLly4gEGDBulUOxER0fPoHG5UKlWB0RYAiI+Ph6Ghbme5AgMDsXTpUqxcuRIxMTEYNWoUMjIypKunBg4cqDXheNSoUXj48CHGjh2Ly5cvY9u2bZg1a5ZOI0YViRAC199rgvhpw5AZdRBQKGDavCOcZqxAvX2JcJgU9krbf/ZqtYYNGyI8PBympqZYtmzZc9dRKBRwcHCQXv+eMwU8GcX7+OOPsWbNGumqtqe2bt0KIyMjLF68GO7u7njjjTcQHh6O9evX4+rVq690PEREREAx5tx06dIFwcHB+OOPP2BlZQXgyY31PvvsM3Tu3FmnbfXp0wf37t3D1KlTkZCQgCZNmmDnzp3Sf5hxcXEweOZZRy4uLti1axfGjx+Pxo0bw9nZGWPHjsWkSfK88ZxCoYBFx15IP7oL1v4DYdW9P4wcSma+UHGuVgOA9PR01KxZExqNBp6enpg1axZee+01ablGo8GAAQMwceJErfansrOzYWxsrPVzNTExAQAcPny4wP2TiIiIdKVzuJk3bx7atm2LmjVromnTpgCA6Oho2NvbY/Xq1ToXMGbMGIwZM6bQZfv37y/Q5uPjg2PHjum8n4rKbmwo7CcuKPGnbxfnajV3d3csW7YMjRs3RkpKCubNm4eWLVviwoULqF69OgBgzpw5MDQ0xCeffFLoNjp27IjAwEB89dVXGDt2LDIyMqR7GsXHx5fgERIRUWWl82kpZ2dnnDt3DnPnzkXDhg3RrFkzfP311zh//nyFvwqpPDJQqUs82BSXj48PBg4ciCZNmqBdu3bYsGEDbG1t8f333wMAoqKi8PXXX0sTjwvz2muvYeXKlZg/fz5MTU3h4OAANzc32Nvba43mEBERFVex7nNjZmaGESNGlHQtVIaKc7XavxkZGaFp06bSXJlDhw4hKSkJNWrUkPrk5+fj008/RVhYGG7evAkA6NevH/r164fExESYmZlBoVBgwYIFqFWrVskcHBERVWpF/lX58uXLBS4TjoiIQIcOHdC8eXPMmjWrxIuj0lOcq9X+LT8/H+fPn4ej45PLzgcMGIBz584hOjpaejk5OWHixInYtWtXgfXt7e1hbm6OdevWQa1W6zxni4iIqDBFHrmZNGkSGjVqhObNmwMAbty4AX9/f7Rp0waNGzdGaGgoTE1NMW7cuNKqlUpYYGAgAgIC4OXlhebNmyMsLKzA1WrOzs4IDQ0FAEyfPh0tWrRAnTp1kJycjK+++gq3bt3CsGHDAABVq1ZF1apVtfZhZGQEBwcHuLu7S23ffvstWrZsCXNzc+zevRsTJ07E7Nmz+QBUIiIqEUUON6dOncJ//vMf6f2aNWtQr1496Tfyxo0bY9GiRQw3FYiuV6s9evQIw4cPR0JCAqpUqYJmzZrh6NGjaNiwoU77PXHiBEJCQpCeno769evj+++/x4ABA0r02IiIqPIqcri5f/++dEUMAOzbtw/+/v7S+/bt2+PTTz8t2eqo1OlytdrChQuxcOFCnbb/dJ7Ns1atWqXTNoiIiHRR5Dk3NjY20qW6Go0Gp06dQosWLaTlOTk5EEKUfIVEREREOihyuGnfvj1mzJiB27dvIywsDBqNBu3bt5eWX7x4Ea6urqVQIhEREVHRFfm01MyZM9G5c2fUrFkTSqUS33zzDczMzKTlq1evRseOHUulSCIiIqKiKnK4cXV1RUxMDC5cuABbW9sCD0T84osvtObkEBEREemDTjfxMzQ0hIeHR6HLntdOREREVJZ4v3siIiKSlWI9foEquGlW+q7g5dxqvLwPERFRIThyQ0RERLLCcENERESyUmLhJiMjAwcPHiypzREREREVS4mFm6tXr6JDhw4ltTkiIiKiYuFpKSIiIpKVIl8tZWNj88Ll+fn5r1wMERER0asqcrjJzs7GqFGj0KhRo0KX37p1C1988UWJFUZERERUHEUON02aNIGLiwsCAgIKXX727FmGGyIiItK7Is+56d69O5KTk5+73MbGBgMHDiyJmoiIiIiKrcgjN5999tkLl7u4uGD58uWvXBARERHRq+DVUkRERCQrRQ43bdu21TottXnzZjx+/Lg0aiIiIiIqtiKHm8OHDyMnJ0d6/8EHHyA+Pr5UiiIiIiIqrmKflhJClGQdRERERCWCc26IiIhIVop8tRQA7Nq1C1ZWVgAAjUaDiIgI/PXXX1p9evToUXLVEREREelIp3Dz7xv4ffjhh1rvFQoFH8NAREREelXkcKPRaEqzDiIiIqISwTk3REREJCsMN0RERCQrDDdEREQkKww3REREJCsMN0RERCQrxQo3ycnJ+PHHHxEcHIyHDx8CAE6fPo07d+6UaHFEREREutLpPjcAcO7cOfj6+sLKygo3b97E8OHDYWNjgw0bNiAuLg6rVq0qjTqJiIiIikTnkZvAwEAMGjQIV65cgVqtltq7deuGgwcPlmhxRERERLrSOdycPHmywJ2JAcDZ2RkJCQklUhQRERFRcekcblQqFVJTUwu0X758Gba2tiVSFBEREVFx6RxuevTogenTpyM3NxfAk+dJxcXFYdKkSXj33XdLvEAiIiIiXegcbubPn4/09HTY2dnh8ePHaNeuHerUqQMLCwvMnDmzNGokIiIiKjKdr5aysrLC7t27cfjwYZw7dw7p6enw9PSEr69vadRHREREpBOdw81TrVu3RuvWrUuyFiIiIqJXpnO4+eabbwptVygUUKvVqFOnDtq2bQulUvnKxRERERHpSudws3DhQty7dw+ZmZmoUqUKAODRo0cwNTWFubk5kpKSUKtWLezbtw8uLi4lXjARERHRi+g8oXjWrFl44403cOXKFTx48AAPHjzA5cuX4e3tja+//hpxcXFwcHDA+PHjS6NeIiIiohfSeeTm888/x/r161G7dm2prU6dOpg3bx7effddXL9+HXPnzuVl4URERKQXOo/cxMfHIy8vr0B7Xl6edIdiJycnpKWlvXp1RERERDrSOdx06NABH374Ic6cOSO1nTlzBqNGjULHjh0BAOfPn4ebm1vJVUlERERURDqHm59++gk2NjZo1qwZVCoVVCoVvLy8YGNjg59++gkAYG5ujvnz55d4sUREREQvo/OcGwcHB+zevRuXLl3C5cuXAQDu7u5wd3eX+nTo0KHkKiQiIiLSQbFv4le/fn3Ur1+/JGshIiIiemXFCjd///03Nm/ejLi4OOTk5GgtW7BgQYkURkRERFQcOs+5iYiIgLu7O7777jvMnz8f+/btw/Lly7Fs2TJER0eXQokV1+LFi+Hq6gq1Wg1vb2+cOHHiuX03bNgALy8vWFtbw8zMDE2aNMHq1au1+qSnp2PMmDGoXr06TExM0LBhQ4SHh2v1SUhIwIABA+Dg4AAzMzN4enpi/fr1pXJ8RERE5ZHO4SY4OBgTJkzA+fPnoVarsX79ety+fRvt2rXDe++9Vxo1Vkjr1q1DYGAgQkJCcPr0aXh4eMDPzw9JSUmF9rexscHkyZMRGRmJc+fOYfDgwRg8eDB27dol9QkMDMTOnTvx888/IyYmBuPGjcOYMWOwefNmqc/AgQMRGxuLzZs34/z583jnnXfQu3dvravbiIiI5EzncBMTE4OBAwcCAAwNDfH48WOYm5tj+vTpmDNnTokXWFEtWLAAw4cPx+DBg6URFlNTUyxbtqzQ/u3bt8fbb7+NBg0aoHbt2hg7diwaN26Mw4cPS32OHj2KgIAAtG/fHq6urhgxYgQ8PDy0RoSOHj2Kjz/+GM2bN0etWrXw+eefw9raGlFRUaV+zEREROWBzuHGzMxMmmfj6OiIa9euScvu379fcpVVYDk5OYiKioKvr6/UZmBgAF9fX0RGRr50fSEEIiIiEBsbi7Zt20rtLVu2xObNm3Hnzh0IIbBv3z5cvnwZXbp00eqzbt06PHz4EBqNBmvXrkVWVhbat29fosdIRERUXuk8obhFixY4fPgwGjRogG7duuHTTz/F+fPnsWHDBrRo0aI0aqxw7t+/j/z8fNjb22u129vb49KlS89dLyUlBc7OzsjOzoZSqcSSJUvQuXNnafmiRYswYsQIVK9eHYaGhjAwMMDSpUu1AtBvv/2GPn36oGrVqjA0NISpqSk2btyIOnXqlPyBEhERlUM6h5sFCxYgPT0dAPDFF18gPT0d69atQ926dXml1CuysLBAdHQ00tPTERERgcDAQNSqVUsadVm0aBGOHTuGzZs3o2bNmjh48CBGjx4NJycnaZRoypQpSE5Oxp49e1CtWjVs2rQJvXv3xqFDh9CoUSM9Hh0REVHZ0Cnc5Ofn4++//0bjxo0BPDlF9e+rdQioVq0alEolEhMTtdoTExPh4ODw3PUMDAykEZYmTZogJiYGoaGhaN++PR4/fozPPvsMGzduRPfu3QEAjRs3RnR0NObNmwdfX19cu3YN3377Lf766y+89tprAAAPDw8cOnQIixcv5s+KiIgqBZ3m3CiVSnTp0gWPHj0qrXpkwdjYGM2aNUNERITUptFoEBERAR8fnyJvR6PRIDs7GwCQm5uL3NxcGBho/8iUSiU0Gg0AIDMzEwBe2IeIiEjudJ5Q/Prrr+P69eslWoQu94N51tq1a6FQKNCrV68SrackBAYGYunSpVi5ciViYmIwatQoZGRkYPDgwQCeXLIdHBws9Q8NDcXu3btx/fp1xMTEYP78+Vi9ejU++OADAIClpSXatWuHiRMnYv/+/bhx4wZWrFiBVatW4e233wbw5K7RderUwYcffogTJ07g2rVrmD9/Pnbv3l0uPyMiIqLSoPOcmy+//BITJkzAjBkz0KxZM5iZmWktt7S01Gl7T+8HEx4eDm9vb4SFhcHPzw+xsbGws7N77no3b97EhAkT0KZNG10PoUz06dMH9+7dw9SpU5GQkIAmTZpg586d0iTjuLg4rRGWjIwMfPTRR/j7779hYmKC+vXr4+eff0afPn2kPmvXrkVwcDD69++Phw8fombNmpg5cyZGjhwJADAyMsL27dsRFBQEf39/pKeno06dOli5ciW6detWth8AERGRniiEEEKXFZ79D1mhUEh/FkJAoVAgPz9fpwK8vb3xxhtv4NtvvwXw5FSMi4sLPv74YwQFBRW6Tn5+Ptq2bYshQ4bg0KFDSE5OxqZNm4q0v9TUVFhZWSElJUXnICYb06z0XcFLNXKroe8SXup8wHl9l/BSrkHb9F3CS91U99N3CS/F72PJ4PexZFTW76Mu/3/rPHKzb9++Yhf2b0/vB/Ps6Zmi3A9m+vTpsLOzw9ChQ3Ho0KEX7iM7O1uatwI8+XCIiIhIvnQON+3atSuxnRfnfjCHDx/GTz/9VOTnWIWGhuKLL7541VKJiIiogtB5QjEAHDp0CB988AFatmyJO3fuAABWr16t9aiA0pCWloYBAwZg6dKlqFatWpHWCQ4ORkpKivS6fft2qdZIRERE+qXzyM369esxYMAA9O/fH6dPn5ZO+aSkpGDWrFnYvn17kbel6/1grl27hps3b8Lf319qe3qJs6GhIWJjY1G7dm2tdVQqFVQqVZFrIiIioopN55GbL7/8EuHh4Vi6dCmMjIyk9latWuH06dM6bUvX+8HUr18f58+fR3R0tPTq0aMHOnTogOjoaLi4uOh6OERERCQzOo/c/Pthjk9ZWVkhOTlZ5wICAwMREBAALy8vNG/eHGFhYQXuB+Ps7IzQ0FCo1Wq8/vrrWutbW1sDQIF2fakYVwPouwIiIqLSo3O4cXBwwNWrV+Hq6qrVfvjwYdSqVUvnAnS9HwwRERHRi+gcboYPH46xY8di2bJlUCgUuHv3LiIjIzFhwgRMmTKlWEWMGTMGY8aMKXTZ/v37X7juihUrirVPIiIikiedw01QUBA0Gg06deqEzMxMtG3bFiqVChMmTMDHH39cGjUSERERFZnO4UahUGDy5MmYOHEirl69ivT0dDRs2BDm5ualUR8RERGRTnSezPLzzz8jMzMTxsbGaNiwIZo3b85gQ0REROWGzuFm/PjxsLOzQ79+/bB9+3adnyVFREREVJp0Djfx8fFYu3YtFAoFevfuDUdHR4wePRpHjx4tjfqIiIiIdKJzuDE0NMRbb72FNWvWICkpCQsXLsTNmzfRoUOHAncHJiIiIiprOk8ofpapqSn8/Pzw6NEj3Lp1CzExMSVVFxEREVGxFOvueJmZmVizZg26desGZ2dnhIWF4e2338aFCxdKuj4iIiIineg8cvP+++9j69atMDU1Re/evTFlypRCnwNFREREpA86hxulUonffvsNfn5+UCqVWsv++uuvcvOMJyIiIqqcdA43a9as0XqflpaGX3/9FT/++COioqJ4aTgRERHpVbGfSHnw4EEEBATA0dER8+bNQ8eOHXHs2LGSrI2IiIhIZzqN3CQkJGDFihX46aefkJqait69eyM7OxubNm1Cw4YNS6tGIiIioiIr8siNv78/3N3dce7cOYSFheHu3btYtGhRadZGREREpLMij9zs2LEDn3zyCUaNGoW6deuWZk1ERERExVbkkZvDhw8jLS0NzZo1g7e3N7799lvcv3+/NGsjIiIi0lmRw02LFi2wdOlSxMfH48MPP8TatWvh5OQEjUaD3bt3Iy0trTTrJCIiIioSna+WMjMzw5AhQ3D48GGcP38en376KWbPng07Ozv06NGjNGokIiIiKrJiXwoOAO7u7pg7dy7+/vtv/PrrryVVExEREVGxvVK4eUqpVKJXr17YvHlzSWyOiIiIqNhKJNwQERERlRcMN0RERCQrDDdEREQkKww3REREJCsMN0RERCQrDDdEREQkKww3REREJCsMN0RERCQrDDdEREQkKww3REREJCsMN0RERCQrDDdEREQkKww3REREJCsMN0RERCQrDDdEREQkKww3REREJCsMN0RERCQrDDdEREQkKww3REREJCsMN0RERCQrDDdEREQkKww3REREJCsMN0RERCQrDDdEREQkKww3REREJCsMN0RERCQrDDdEREQkKww3REREJCsMN0RERCQrDDdEREQkKww3REREJCsMN0RERCQrDDdEREQkKww3REREJCsMN0RERCQrDDdEREQkKww3REREJCsMN0RERCQrDDdEREQkKww3REREJCsMN0RERCQrDDdEREQkKww3REREJCvlItwsXrwYrq6uUKvV8Pb2xokTJ57bd+nSpWjTpg2qVKmCKlWqwNfX94X9iYiIqHLRe7hZt24dAgMDERISgtOnT8PDwwN+fn5ISkoqtP/+/fvRt29f7Nu3D5GRkXBxcUGXLl1w586dMq6ciIiIyiO9h5sFCxZg+PDhGDx4MBo2bIjw8HCYmppi2bJlhfZfs2YNPvroIzRp0gT169fHjz/+CI1Gg4iIiDKunIiIiMojvYabnJwcREVFwdfXV2ozMDCAr68vIiMji7SNzMxM5ObmwsbGptDl2dnZSE1N1XoRERGRfOk13Ny/fx/5+fmwt7fXare3t0dCQkKRtjFp0iQ4OTlpBaRnhYaGwsrKSnq5uLi8ct1ERERUfun9tNSrmD17NtauXYuNGzdCrVYX2ic4OBgpKSnS6/bt22VcJREREZUlQ33uvFq1alAqlUhMTNRqT0xMhIODwwvXnTdvHmbPno09e/agcePGz+2nUqmgUqlKpF4iIiIq//Q6cmNsbIxmzZppTQZ+OjnYx8fnuevNnTsXM2bMwM6dO+Hl5VUWpRIREVEFodeRGwAIDAxEQEAAvLy80Lx5c4SFhSEjIwODBw8GAAwcOBDOzs4IDQ0FAMyZMwdTp07FL7/8AldXV2lujrm5OczNzfV2HERERFQ+6D3c9OnTB/fu3cPUqVORkJCAJk2aYOfOndIk47i4OBgY/DPA9N133yEnJwf/93//p7WdkJAQTJs2rSxLJyIionJI7+EGAMaMGYMxY8YUumz//v1a72/evFn6BREREVGFVaGvliIiIiL6N4YbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpKVchFuFi9eDFdXV6jVanh7e+PEiRMv7P/f//4X9evXh1qtRqNGjbB9+/YyqpSIiIjKO72Hm3Xr1iEwMBAhISE4ffo0PDw84Ofnh6SkpEL7Hz16FH379sXQoUNx5swZ9OrVC7169cJff/1VxpUTERFReaT3cLNgwQIMHz4cgwcPRsOGDREeHg5TU1MsW7as0P5ff/013nzzTUycOBENGjTAjBkz4OnpiW+//baMKyciIqLyyFCfO8/JyUFUVBSCg4OlNgMDA/j6+iIyMrLQdSIjIxEYGKjV5ufnh02bNhXaPzs7G9nZ2dL7lJQUAEBqauorVl84TXZmqWy3JKUqhL5LeKn8x/n6LuGlSus7VJL4fSwZ/D6WDH4fS0Zl/T4+3aYQL/8Z6TXc3L9/H/n5+bC3t9dqt7e3x6VLlwpdJyEhodD+CQkJhfYPDQ3FF198UaDdxcWlmFVXfFb6LqBIYvRdwEtZjaoYn2R5VzE+RX4fK4uK8SlW7u9jWloarKxevH29hpuyEBwcrDXSo9Fo8PDhQ1StWhUKhUKPlclDamoqXFxccPv2bVhaWuq7HKrk+H2k8oTfx5IlhEBaWhqcnJxe2lev4aZatWpQKpVITEzUak9MTISDg0Oh6zg4OOjUX6VSQaVSabVZW1sXv2gqlKWlJf/yUrnB7yOVJ/w+lpyXjdg8pdcJxcbGxmjWrBkiIiKkNo1Gg4iICPj4+BS6jo+Pj1Z/ANi9e/dz+xMREVHlovfTUoGBgQgICICXlxeaN2+OsLAwZGRkYPDgwQCAgQMHwtnZGaGhoQCAsWPHol27dpg/fz66d++OtWvX4tSpU/jhhx/0eRhERERUTug93PTp0wf37t3D1KlTkZCQgCZNmmDnzp3SpOG4uDgYGPwzwNSyZUv88ssv+Pzzz/HZZ5+hbt262LRpE15//XV9HUKlplKpEBISUuDUH5E+8PtI5Qm/j/qjEEW5poqIiIiogtD7TfyIiIiIShLDDREREckKww0RERHJCsMNERERyQrDDRXJwYMH4e/vDycnJygUigLP8hJCYOrUqXB0dISJiQl8fX1x5coV/RRLslMS37+HDx+if//+sLS0hLW1NYYOHYr09PQyPAqqiMrqu3fu3Dm0adMGarUaLi4umDt3bmkfmqwx3FCRZGRkwMPDA4sXLy50+dy5c/HNN98gPDwcx48fh5mZGfz8/JCVlVXGlZIclcT3r3///rhw4QJ2796NrVu34uDBgxgxYkRZHQJVUGXx3UtNTUWXLl1Qs2ZNREVF4auvvsK0adN4/7ZXIYh0BEBs3LhReq/RaISDg4P46quvpLbk5GShUqnEr7/+qocKSc6K8/27ePGiACBOnjwp9dmxY4dQKBTizp07ZVY7VWyl9d1bsmSJqFKlisjOzpb6TJo0Sbi7u5fyEckXR27old24cQMJCQnw9fWV2qysrODt7Y3IyEg9VkaVQVG+f5GRkbC2toaXl5fUx9fXFwYGBjh+/HiZ10zyUFLfvcjISLRt2xbGxsZSHz8/P8TGxuLRo0dldDTywnBDrywhIQEApLtKP2Vvby8tIyotRfn+JSQkwM7OTmu5oaEhbGxs+B2lYiup715CQkKh23h2H6QbhhsiIiKSFYYbemUODg4AgMTERK32xMREaRlRaSnK98/BwQFJSUlay/Py8vDw4UN+R6nYSuq75+DgUOg2nt0H6Ybhhl6Zm5sbHBwcEBERIbWlpqbi+PHj8PHx0WNlVBkU5fvn4+OD5ORkREVFSX327t0LjUYDb2/vMq+Z5KGkvns+Pj44ePAgcnNzpT67d++Gu7s7qlSpUkZHIzP6ntFMFUNaWpo4c+aMOHPmjAAgFixYIM6cOSNu3bolhBBi9uzZwtraWvzxxx/i3LlzomfPnsLNzU08fvxYz5WTHJTE9+/NN98UTZs2FcePHxeHDx8WdevWFX379tXXIVEFURbfveTkZGFvby8GDBgg/vrrL7F27Vphamoqvv/++zI/XrlguKEi2bdvnwBQ4BUQECCEeHJJ5JQpU4S9vb1QqVSiU6dOIjY2Vr9Fk2yUxPfvwYMHom/fvsLc3FxYWlqKwYMHi7S0ND0cDVUkZfXdO3v2rGjdurVQqVTC2dlZzJ49u6wOUZYUQghR1qNFRERERKWFc26IiIhIVhhuiIiISFYYboiIiEhWGG6IiIhIVhhuiIiISFYYboiIiEhWGG6IiIhIVhhuiIiISFYYbohkRKFQYNOmTXqtYdOmTahTpw6USiXGjRv3StsaNGgQevXq9cI+7du3f+l+XF1dERYW9kq1EFHFwXBDVEHcu3cPo0aNQo0aNaBSqeDg4AA/Pz8cOXJE6hMfH4+uXbvqsUrgww8/xP/93//h9u3bmDFjRqF9nhc2pk2bhiZNmkjvv/76a6xYsaJ0Ci1h+/fvh0KhQHJysr5LIar0DPVdABEVzbvvvoucnBysXLkStWrVQmJiIiIiIvDgwQOpj4ODgx4rBNLT05GUlAQ/Pz84OTm98vasrKxKoKrKSwiB/Px8GBryn3qqXDhyQ1QBJCcn49ChQ5gzZw46dOiAmjVronnz5ggODkaPHj2kfs+elpo2bRoUCkWB19OREI1Gg9DQULi5ucHExAQeHh74/fffX1jHo0ePMHDgQFSpUgWmpqbo2rUrrly5AuDJyIWFhQUAoGPHjlAoFNi/f/8rHfe/T0tlZGRg4MCBMDc3h6OjI+bPn19gnaSkJPj7+8PExARubm5Ys2ZNgT7JyckYNmwYbG1tYWlpiY4dO+Ls2bPS8qcjSKtXr4arqyusrKzw/vvvIy0trdjHcvLkSXTu3BnVqlWDlZUV2rVrh9OnT0vLhwwZgrfeektrndzcXNjZ2eGnn34C8PKf2dPRox07dqBZs2ZQqVQ4fPgwzp49iw4dOsDCwgKWlpZo1qwZTp06VexjISrvGG6IKgBzc3OYm5tj06ZNyM7OLtI6EyZMQHx8vPSaN28eTE1N4eXlBQAIDQ3FqlWrEB4ejgsXLmD8+PH44IMPcODAgeduc9CgQTh16hQ2b96MyMhICCHQrVs35ObmomXLloiNjQUArF+/HvHx8WjZsuWrH/wzJk6ciAMHDuCPP/7An3/+if3792sFhKc13r59G/v27cPvv/+OJUuWICkpSavPe++9h6SkJOzYsQNRUVHw9PREp06d8PDhQ6nPtWvXsGnTJmzduhVbt27FgQMHMHv27GLXnpaWhoCAABw+fBjHjh1D3bp10a1bNykwDRs2DDt37kR8fLy0ztatW5GZmYk+ffoAKPrPLCgoCLNnz0ZMTAwaN26M/v37o3r16jh58iSioqIQFBQEIyOjYh8LUbmn34eSE1FR/f7776JKlSpCrVaLli1biuDgYHH27FmtPgDExo0bC6wbGRkp1Gq1WLdunRBCiKysLGFqaiqOHj2q1W/o0KGib9++he7/8uXLAoA4cuSI1Hb//n1hYmIifvvtNyGEEI8ePRIAxL59+154LDVr1hTGxsbCzMxM62VkZCQ8PDykfgEBAaJnz55CCCHS0tKEsbGxtC8hhHjw4IEwMTERY8eOFUIIERsbKwCIEydOSH1iYmIEALFw4UIhhBCHDh0SlpaWIisrS6um2rVri++//14IIURISIgwNTUVqamp0vKJEycKb2/v5x7Tvn37BADx6NGjFx77U/n5+cLCwkJs2bJFamvYsKGYM2eO9N7f318MGjRICFG0n9nTGjZt2qTVx8LCQqxYsaJIdRHJAUduiCqId999F3fv3sXmzZvx5ptvYv/+/fD09HzphNu4uDj06tULEyZMQO/evQEAV69eRWZmJjp37iyNCpmbm2PVqlW4du1aoduJiYmBoaEhvL29pbaqVavC3d0dMTExOh/PxIkTER0drfUaOXLkc/tfu3YNOTk5Wvu3sbGBu7t7gRqbNWsmtdWvXx/W1tbS+7NnzyI9PR1Vq1bVOvYbN25oHburq6t0mg0AHB0dC4wA6SIxMRHDhw9H3bp1YWVlBUtLS6SnpyMuLk7qM2zYMCxfvlzqv2PHDgwZMgSAbj+zp6NzTwUGBmLYsGHw9fXF7Nmzn/szJpILzjIjqkDUajU6d+6Mzp07Y8qUKRg2bBhCQkIwaNCgQvtnZGSgR48e8PHxwfTp06X29PR0AMC2bdvg7OystY5KpSq1+p9VrVo11KlTR6vNxsam1Pebnp4OR0fHQucDPRuC/n3aRqFQQKPRFHu/AQEBePDgAb7++mvUrFkTKpUKPj4+yMnJkfoMHDgQQUFBiIyMxNGjR+Hm5oY2bdpIdQNF+5mZmZlpvZ82bRr69euHbdu2YceOHQgJCcHatWvx9ttvF/t4iMozhhuiCqxhw4bPva+NEAIffPABNBoNVq9eDYVCobWeSqVCXFwc2rVrV6R9NWjQAHl5eTh+/Lg0l+bBgweIjY1Fw4YNX/lYXqZ27dowMjLC8ePHUaNGDQBPJjhfvnxZOob69esjLy8PUVFReOONNwAAsbGxWpdne3p6IiEhAYaGhnB1dS31up86cuQIlixZgm7dugEAbt++jfv372v1qVq1Knr16oXly5cjMjISgwcPlpYV52f2rHr16qFevXoYP348+vbti+XLlzPckGwx3BBVAA8ePMB7772HIUOGoHHjxrCwsMCpU6cwd+5c9OzZs9B1pk2bhj179uDPP/9Eenq69Ju/lZUVLCwsMGHCBIwfPx4ajQatW7dGSkoKjhw5AktLSwQEBBTYXt26ddGzZ08MHz4c33//PSwsLBAUFARnZ+fn1lCSzM3NMXToUEycOBFVq1aFnZ0dJk+eDAODf86uu7u7480338SHH36I7777DoaGhhg3bhxMTEykPr6+vvDx8UGvXr0wd+5c1KtXD3fv3sW2bdvw9ttvFzilo6vz589rnc5SKBTw8PBA3bp1sXr1anh5eSE1NRUTJ07UquupYcOG4a233kJ+fr7Wz6E4PzMAePz4MSZOnIj/+7//g5ubG/7++2+cPHkS77777isdJ1F5xnBDVAGYm5vD29sbCxcuxLVr15CbmwsXFxcMHz4cn332WaHrHDhwAOnp6QWuWFq+fDkGDRqEGTNmwNbWFqGhobh+/Tqsra3h6en53O09XXfs2LF46623kJOTg7Zt22L79u1lduXNV199hfT0dPj7+8PCwgKffvopUlJSCtQ4bNgwtGvXDvb29vjyyy8xZcoUablCocD27dsxefJkDB48GPfu3YODgwPatm0Le3v7V66xbdu2Wu+VSiXy8vLw008/YcSIEfD09ISLiwtmzZqFCRMmFFjf19cXjo6OeO211wrcK6g4PzOlUokHDx5g4MCBSExMRLVq1fDOO+/giy++eOVjJSqvFEIIoe8iiIjoifT0dDg7O2P58uV455139F0OUYXEkRsionJAo9Hg/v37mD9/PqytrbVuzkhEumG4ISIqB+Li4uDm5obq1atjxYoVfGQC0SvgaSkiIiKSFd7Ej4iIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhk5f8BSmWaw+CbXeAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plt.xscale(\"log\")\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "\n",
    "x = np.arange(len(hidden_layers))\n",
    "w = 0.25\n",
    "multiplier = 0\n",
    "\n",
    "f1s = [macro_f1_scores[:3], macro_f1_scores[3:6], macro_f1_scores[6:9]]\n",
    "f1s_arr = np.array(f1s)\n",
    "\n",
    "trend_line = np.polyfit(np.tile(x, (len(f1s), 1)).flatten(), f1s_arr.flatten(), 1)\n",
    "trend_x = np.linspace(min(x), max(x), 100)\n",
    "trend_y = np.polyval(trend_line, trend_x)\n",
    "ax.plot(trend_x, trend_y, color='#D52D00', linestyle='--', label='Trend')\n",
    "\n",
    "for i in range(len(f1s)):\n",
    "    offset = w * multiplier\n",
    "    rects = ax.bar(x + offset, f1s[i], w, label=hidden_layers[i])\n",
    "    ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "    ax.set_xticks(x + w, hidden_layer_size)\n",
    "    ax.legend(loc='upper left', ncols=4, title=\"Number of hidden layers\", bbox_to_anchor=(0.1,1.2))\n",
    "    ax.set_xlabel(\"Size of Hidden Layers\")\n",
    "    ax.set_ylabel(\"Average F1 Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plt.xscale(\"log\")\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "\n",
    "x = np.arange(len(hidden_layers))\n",
    "w = 0.25\n",
    "multiplier = 0\n",
    "\n",
    "f1s = [macro_f1_scores[9:12], macro_f1_scores[12:15], macro_f1_scores[15:18]]\n",
    "f1s_arr = np.array(f1s)\n",
    "\n",
    "trend_line = np.polyfit(np.tile(x, (len(f1s), 1)).flatten(), f1s_arr.flatten(), 1)\n",
    "trend_x = np.linspace(min(x), max(x), 100)\n",
    "trend_y = np.polyval(trend_line, trend_x)\n",
    "ax.plot(trend_x, trend_y, color='#D52D00', linestyle='--', label='Trend')\n",
    "\n",
    "for i in range(len(f1s)):\n",
    "    offset = w * multiplier\n",
    "    rects = ax.bar(x + offset, f1s[i], w, label=hidden_layers[i])\n",
    "    ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "    ax.set_xticks(x + w, hidden_layer_size)\n",
    "    ax.legend(loc='upper left', ncols=4, title=\"Number of hidden layers\", bbox_to_anchor=(0.1,1.2))\n",
    "    ax.set_xlabel(\"Size of Hidden Layers\")\n",
    "    ax.set_ylabel(\"Average F1 Score\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
